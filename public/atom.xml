<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>在下小宋</title>
  
  <subtitle>Never settle!</subtitle>
  <link href="https://www.songzj.com/atom.xml" rel="self"/>
  
  <link href="https://www.songzj.com/"/>
  <updated>2022-04-18T10:06:21.000Z</updated>
  <id>https://www.songzj.com/</id>
  
  <author>
    <name>在下小宋</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2022第十四周实盘</title>
    <link href="https://www.songzj.com/posts/d9e0e633/"/>
    <id>https://www.songzj.com/posts/d9e0e633/</id>
    <published>2022-04-18T10:06:21.000Z</published>
    <updated>2022-04-18T10:06:21.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src= "/img/loading.gif" data-lazy-src="/media/202214price_pct_chg_everyweek_analyse.png" alt="price_pct_chg_everyweek_analyse"></p><p>如图所示，这周市场比上周跌的更狠，A股平均跌幅3.9个点。<br>目前持仓洲明科技、拉卡拉、润欣科技、蔚蓝生物、爱朋医疗、<em>ST绿庭、华宇软件、高鸿股份、测绘股份。<br>卖出了芒果超媒、维科科技，按照上周的分析，需要科学合理地配置每只股票的仓位，所以这周买入了拉卡拉、润欣科技、蔚蓝生物、爱朋医疗、</em>ST绿庭等多只股票，当然这不是为买而买，买入的股票全都符合我的标准，都是具有小周期的股票且处在周期的底部。洲明科技的仓位有点多，暂时还没法减少到安全的水平线，希望后面幸运一点市场不要这么疯狂的杀跌了。<br>这周做了正确的事情，把股票的配置合理化分散化了，还投资了几只很棒的股票，但是没有收到好的效果，市场的大环境就是这样，一边倒的下跌，投资者的努力仿佛就是螳臂当车一样无力。即使是这样，投资者依然应该有自己的信念和坚持。<br>投资真的不能以短期的成败论输赢，投资本质上是个无限游戏，一时的得失根本不算什么，投资者真正要看重和计较的是在长期的投资活动中，什么样的策略是赚钱的，而这正是投资最难的部分，你可能无法从当下的市场获得恰当的反馈，没有一种公平的激励机制，我们的思维和行动就很容易混乱。所以投资者需要有战略思维，要用长远的眼光看问题，这是投资者拨开迷雾冲破现实的阻碍必须的一种思维方式。<br>上周我们谈到了凯利公式，我用自己的整体成功率和盈亏比来估算的买入合理仓位，这只是一个例子，如果在实际操作中，这样来计算是不合理的，为什么呢？每只股票的涨跌没有这种联动效应，各个如果以个人整体的<br>上周我们谈到了凯利公式，我用自己的整体成功率和盈亏比来估算的买入合理仓位，这只是一个例子，如果在实际操作中，这样来计算是不合理的，为什么呢？<br>第一，每只股票的涨跌没有这种联动效应，每只股票的运行轨迹都不相同；第二，买股票买的是未来的增长潜力，在风云变幻的股市上，如果以以前的记录作为行动依据就像刻舟求剑一样愚蠢。<br>所以，我们不应该笼统的套用凯利公式用在每一次的操作中，而应该每次都具体判断买入标的的涨跌概率和幅度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src= &quot;/img/loading.gif&quot; data-lazy-src=&quot;/media/202214price_pct_chg_everyweek_analyse.png&quot; alt=&quot;price_pct_chg_everyweek_analyse&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="实盘" scheme="https://www.songzj.com/tags/%E5%AE%9E%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>决策树详解</title>
    <link href="https://www.songzj.com/posts/80020160/"/>
    <id>https://www.songzj.com/posts/80020160/</id>
    <published>2022-02-09T10:09:49.000Z</published>
    <updated>2022-01-09T10:09:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>决策树是一种以树状结构表示的分类和回归模型，从根节点开始，根据最优属性从上往下层层划分，最终输出叶子节点为分类结果值。</p><p>决策树代表对象属性和对象值之间的一种映射关系。它由节点（node）和有向边（directed edge）组成，其节点有两种类型：内节点（internal node）和叶节点（leaf node），内部节点表示一个特征或属性，叶节点表示一个类。<strong>根节点是决策树最开始的结点，内部结点是可以继续分类的结点。</strong></p><p>决策树的学习本质上是从训练集中归纳出一组分类规则，得到与数据集矛盾较小的决策树，同时具有很好的泛化能力。决策树学习的损失函数通常是正则化的极大似然函数，通常采用启发式方法，近似求解这一最优化问题。<br>决策树学习算法包含特征选择、决策树生成与决策树的剪枝。决策树表示的是一个条件概率分布，所以深浅不同的决策树对应着不同复杂程度的概率模型。决策树的生成对应着模型的局部选择（局部最优），决策树的剪枝对应着全局选择（全局最优）。决策树常用的算法有ID3，C4.5，CART，下面通过一个简单的例子来分别介绍这几种算法。</p><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>宗旨是在每一个决策点，选择能够使得样本的熵降低得最快，样本纯度提升最大的那个特征作为该决策点的划分特征。</p><p>特征选择就是选择对训练数据具有分类能力的特征，也就是我们在背景里面提到的对工作好坏评判起作用的指标，这样就可以提高决策树学习的效率。如果一个特征的分类能力与随机分类的结果没什么差异，则称这个特征是没有分类能力的。一般我们把这类特征是直接去掉的，我们优先那些能够对我们的分类起到决定作用的特征，我们在具体选取的时候会用到三个准则：信息增益、信息增益比和基尼系数。</p><p>在信息论和概率统计中，熵表示随机变量不确定性的度量。</p><p>信息增益表示在得知特征X的信息而使得类Y的不确定性减少的程度。</p><p>以信息增益作为划分训练数据集的特征，存在偏向于选取取值较多的特征的问题使用信息增益比可以对这一问题进行校正。</p><h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><p>决策树的生成主要分以下两步，这两步通常通过学习已经知道分类结果的样本来实现。</p><ol><li><p>节点的分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成2个子节点（如不是二叉树的情况会分成n个子节点）</p></li><li><p>阈值的确定：选择适当的阈值使得分类错误率最小 （Training Error）。</p></li></ol><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><p>ID3算法的核心是在决策树各个结点的上应用信息增益准则选择特征，递归地构建决策树。</p><p>具体方法就是从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子节点递归地调用以上方法，构建决策树；直至所有特征的信息增益均很小或没有特征可以选择为止，最后得到一个决策树。</p><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>输入：训练数据集D，特征集A，阈值z。</p><p>输出：决策树T。</p><ul><li>若D中所有的实例都属于同一类Ck(k表示样本D本身按照结果分成k个类别),则T为单节点树，并将类Ck作为该节点的类标记，返回T。</li><li>若特征A集合为空，则T为单节点树，并将D中实例数最大的类Ck作为该节点的类标记，返回T。</li><li>如果不符合上面两种情况，则按照信息增益算法公式计算A中每个特征对D的信息增益，选择信息增益最大的特征Ag。</li><li>如果Ag的信息增益小于阈值z,则置T为单节点树，并将D中的实例数最大的类Ck作为该节点的类标记，返回T。</li><li>如果Ag的信息增益大于阈值z,则对Ag的每一个取值ai,依据Ag&#x3D;ai将D分割为若干非空子集Di,将Di中实例数最大的类作为标记，构建子节点，由结点及其子节点构成树T，返回T。</li><li>对第i个子节点，以Di为训练集，以A-Ag为特征集，递归地调用上面5个步骤，得到子树Ti,返回Ti。</li></ul><h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>C4.5和ID3算法相似，C4.5是在ID3的基础上进行了改进，从ID3用信息增益来选取特征改成了用信息增益比来选取特征，其他步骤均与ID3算法一致</p><h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>选择Gini系数最小的作为划分特征，Gini系数越小，纯度越高。<br>CART（Classification And Regression Tree）算法主要有两部分组成：<br>(1) 决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量打。这与ID3算法类似，不同之处也是特征选取的方式；<br>(2) 决策树的剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，此时用损失函数最小作为剪枝的标准。<br>CART算法可以用于回归，即建立回归树。在终于分类时，其算法流程与ID3较为类似，不同的是特征选取，选择的是最小基尼指数。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src= "/img/loading.gif" data-lazy-src="https://img2020.cnblogs.com/blog/1925148/202004/1925148-20200426115511078-28064221.png"></p><h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><p>目的是防止过拟合，增强其泛化能力。</p><p>决策树生成算法是通过递归的方法产生决策树，直到不能继续下去为止，这样产生的树往往对训练数据的分类很准确，但对未知数据的分类却没那么准确，即出现过拟合的现象。过拟合的原因在于学习时过度考虑如何提高训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的方法是考虑决策树的复杂度，对已生成的决策树进行简化，我们把这种对已生成的树进行简化的过程称为剪枝。</p><p>剪枝是从已生成的树上裁掉一些子树或叶节点，并将其根结点或父节点作为新的叶节点，从而简化分类树模型。</p><p>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。</p><p>剪枝是应该决策树过拟合的一种重要方法，主要分为以下两种：</p><ul><li>预剪枝：该策略就是在对一个节点进行划分前进行估计，如果不能提升决策树泛化精度，就停止划分，将当前节点设置为叶节点。那么怎么测量泛化精度，就是留出一部分训练数据当做测试集，每次划分前比较划分前后的测试集预测精度。<ul><li>优点：降低了过拟合风险，降低了训练所需的时间。</li><li>缺点：预剪枝是一种贪心操作，可能有些划分暂时无法提升精度，但是后续划分可以提升精度。故产生了欠拟合的风险。</li></ul></li><li>后剪枝：该策略是首先正常建立一个决策树，然后对整个决策树进行剪枝。按照决策树的广度优先搜索的反序，依次对内部节点进行剪枝，如果将某以内部节点为根的子树换成一个叶节点，可以提高泛化性能，就进行剪枝。<ul><li>优点：降低过拟合风险，降低欠拟合风险，决策树效果提升比预剪枝强</li><li>缺点：时间开销大得多</li></ul></li></ul><h2 id="决策树的优缺点"><a href="#决策树的优缺点" class="headerlink" title="决策树的优缺点"></a>决策树的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li><p>可解释性强：易于理解和解释，可以可视化分析，容易提取出规则。</p></li><li><p>数据预处理：只需很少的数据准备 ，以同时处理类别型和数值型数据，允许缺失值，能够处理不相关的特征</p></li><li><p>大规模数据处理：决策树可以很好的扩展到大型数据库中，处理大规模数据</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3></li><li><p>过拟合：容易出现过拟合问题，导致无法很好的预测训练集之外的数据</p></li><li><p>忽略特征关联：忽略数据集中属性的相互关联。当数值型变量之间存在许多错综复杂的关系，如金融数据分析，不是一个好选择</p></li><li><p>模型敏感：模型不够稳健，某一个节点的小小变化可能导致整个树会有很大的不同。</p></li></ul><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://blog.csdn.net/zx10212029/article/details/49529843">决策树（上）原理</a></li><li><a href="https://blog.csdn.net/zx10212029/article/details/49617179">决策树（下）算法实现</a></li><li><a href="https://cloud.tencent.com/developer/article/1092087">决策树详解</a></li><li><a href="https://blog.csdn.net/bravery_again/article/details/81104914">braveryCHR<br>：决策树详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/197476119">王改改：通俗易懂的讲解决策树（Decision Tree）</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;决策树是一种以树状结构表示的分类和回归模型，从根节点开始，根据最优属性从上往下层层划分，最终输出叶子节点为分类结果值。&lt;/p&gt;
&lt;p&gt;决策树代表对象属性和对象值之间的一种映射关系。它由节点（node）和有向边（directed edge）组成，其节点有两种类型：内节点（in</summary>
      
    
    
    
    
    <category term="决策树" scheme="https://www.songzj.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归详解</title>
    <link href="https://www.songzj.com/posts/ab903812/"/>
    <id>https://www.songzj.com/posts/ab903812/</id>
    <published>2022-02-09T06:20:03.000Z</published>
    <updated>2022-02-02T06:20:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是逻辑回归？"><a href="#什么是逻辑回归？" class="headerlink" title="什么是逻辑回归？"></a>什么是逻辑回归？</h2><p>逻辑回归（Logistic Regression）主要解决二分类问题，用来表示某件事情发生的可能性。</p><p>比如：</p><ul><li>一封邮件是垃圾邮件的肯能性（是、不是）</li><li>你购买一件商品的可能性（买、不买）</li><li>广告被点击的可能性（点、不点）</li></ul><p>一句话总结，逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</p><h2 id="逻辑回归公式"><a href="#逻辑回归公式" class="headerlink" title="逻辑回归公式"></a>逻辑回归公式</h2><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/logistic.jpg"></p><p>Sigmoid函数<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sigmoid.jpg"></p><p>图像是<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sigmoidpic.jpg"><br>Sigmoid函数之所以叫Sigmoid，是因为函数的图像很想一个字母S。这个函数是一个很有意思的函数，从图像上我们可以观察到一些直观的特性：函数的取值在0-1之间，且在0.5处为中心对称，并且越靠近x&#x3D;0的取值斜率越大。</p><h2 id="逻辑回归的假设"><a href="#逻辑回归的假设" class="headerlink" title="逻辑回归的假设"></a>逻辑回归的假设</h2><h3 id="数据服从伯努利分布"><a href="#数据服从伯努利分布" class="headerlink" title="数据服从伯努利分布"></a>数据服从伯努利分布</h3><blockquote><p>伯努利分布：是一个离散型概率分布，若成功，则随机变量取值1；若失败，随机变量取值为0。成功概率记为p，失败为q &#x3D; 1-p。</p></blockquote><p>在逻辑回归中，既然假设了数据分布服从伯努利分布，那就存在一个成功和失败，对应二分类问题就是正类和负类，那么就应该有一个样本为正类的概率，和样本为负类的概率。</p><h3 id="正类的概率由sigmoid的函数计算"><a href="#正类的概率由sigmoid的函数计算" class="headerlink" title="正类的概率由sigmoid的函数计算"></a>正类的概率由sigmoid的函数计算</h3><h2 id="逻辑回归的损失函数"><a href="#逻辑回归的损失函数" class="headerlink" title="逻辑回归的损失函数"></a>逻辑回归的损失函数</h2><p>都说逻辑回归的损失函数是它的极大似然函数，但是为啥呢？</p><blockquote><p>极大似然估计：利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值（模型已定，参数未知）</p></blockquote><p>一般和平方损失函数（最小二乘法）拿来比较，因为线性回归用的就是平方损失函数，原因就是平方损失函数加上sigmoid的函数将会是一个非凸的函数，不易求解，会得到局部解，用对数似然函数得到高阶连续可导凸函数，可以得到最优解。</p><p>其次，是因为对数损失函数更新起来很快，因为只和x，y有关，和sigmoid本身的梯度无关。</p><h2 id="逻辑回归的求解方法"><a href="#逻辑回归的求解方法" class="headerlink" title="逻辑回归的求解方法"></a>逻辑回归的求解方法</h2><p>一般都是用梯度下降法来求解，梯度下降又有随机梯度下降，批梯度下降，small batch 梯度下降三种方式：</p><ul><li><p>简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</p></li><li><p>随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。</p></li><li><p>小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</p></li></ul><h2 id="逻辑回归的如何分类"><a href="#逻辑回归的如何分类" class="headerlink" title="逻辑回归的如何分类"></a>逻辑回归的如何分类</h2><p>设定一个阈值，判断正类概率是否大于该阈值。比如阈值是0.5，只用判断正类概率是否大于0.5即可。</p><h2 id="逻辑回归的优缺点"><a href="#逻辑回归的优缺点" class="headerlink" title="逻辑回归的优缺点"></a>逻辑回归的优缺点</h2><p><strong>优点：</strong></p><ul><li>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</li><li>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</li><li>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</li><li>资源占用小,尤其是内存。因为只需要存储各个维度的特征值。</li><li>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cut off，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</li></ul><p><strong>缺点：</strong></p><ul><li>当特征空间很大时，逻辑回归的性能不是很好，容易欠拟合；因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</li><li>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</li><li>不能很好地处理大量多类特征或变量；</li><li>只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须<strong>线性可分</strong>；</li><li>对于非线性特征，需要进行转换；</li><li>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</li></ul><h2 id="逻辑回归-VS-线性回归"><a href="#逻辑回归-VS-线性回归" class="headerlink" title="逻辑回归 VS 线性回归"></a>逻辑回归 VS 线性回归</h2><p>线性回归和逻辑回归是 2 种经典的算法。经常被拿来做比较，下面整理了一些两者的区别：</p><ul><li>逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个Sigmoid函数，使样本映射到[0,1]之间的数值，从而来处理分类问题。</li><li>线性回归只能用于回归问题，逻辑回归虽然名字叫回归，但是更多用于分类问题（关于回归和分类的区别可以看看这篇文章《<a href="https://easyai.tech/ai-definition/supervised-learning/">一文看懂监督学习（基本概念+4步流程+9个典型算法）</a>》）</li><li>逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。</li><li>线性回归要求因变量是连续性数值变量，而逻辑回归要求因变量是离散的变量</li><li>逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。</li><li>线性回归要求自变量和因变量呈线性关系，而逻辑回归不要求自变量和因变量呈线性关系</li><li>线性回归可以直观的表达自变量和因变量之间的关系，逻辑回归则无法表达变量之间的关系</li><li>逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量量关系进行拟合。</li></ul><blockquote><p>自变量：主动操作的变量，可以看做「因变量」的原因<br>因变量：因为「自变量」的变化而变化，可以看做「自变量」的结果。也是我们想要预测的结果。</p></blockquote><h2 id="在什么情况下将连续的特征离散化之后可以获得更好的效果？"><a href="#在什么情况下将连续的特征离散化之后可以获得更好的效果？" class="headerlink" title="在什么情况下将连续的特征离散化之后可以获得更好的效果？"></a>在什么情况下将连续的特征离散化之后可以获得更好的效果？</h2><p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p><ul><li><p>离散特征的增加和减少都很容易，易于模型的快速迭代，容易扩展；</p></li><li><p>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</p></li><li><p>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。具体来说，离散化后可以进行特征交叉，由M+N个变量变为M*N个变量；</p></li><li><p>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。</p></li></ul><h2 id="逻辑回归在训练的过程当中，如果有很多的特征高度相关，或者说有一个特征重复了100遍，会造成怎样的影响？"><a href="#逻辑回归在训练的过程当中，如果有很多的特征高度相关，或者说有一个特征重复了100遍，会造成怎样的影响？" class="headerlink" title="逻辑回归在训练的过程当中，如果有很多的特征高度相关，或者说有一个特征重复了100遍，会造成怎样的影响？"></a>逻辑回归在训练的过程当中，如果有很多的特征高度相关，或者说有一个特征重复了100遍，会造成怎样的影响？</h2><p>先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</p><p>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</p><ul><li><p>去掉高度相关的特征会让模型的可解释性更好</p></li><li><p>可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。</p></li></ul><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/121759976">关于逻辑回归，面试官都怎么问</a></li><li><a href="https://blog.csdn.net/fengdu78/article/details/105384574">关于逻辑回归，面试官们都怎么问</a></li><li><a href="https://blog.csdn.net/pakko/article/details/37878837">逻辑回归 - 理论篇</a></li><li><a href="https://blog.csdn.net/zx10212029/article/details/49889319">从线性回归到逻辑斯特回归</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是逻辑回归？&quot;&gt;&lt;a href=&quot;#什么是逻辑回归？&quot; class=&quot;headerlink&quot; title=&quot;什么是逻辑回归？&quot;&gt;&lt;/a&gt;什么是逻辑回归？&lt;/h2&gt;&lt;p&gt;逻辑回归（Logistic Regression）主要解决二分类问题，用来表示某件事情发生的</summary>
      
    
    
    
    
    <category term="逻辑回归" scheme="https://www.songzj.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>TextCNN详解</title>
    <link href="https://www.songzj.com/posts/6bc1f905/"/>
    <id>https://www.songzj.com/posts/6bc1f905/</id>
    <published>2022-02-07T07:28:45.000Z</published>
    <updated>2022-02-07T07:28:45.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是卷积？"><a href="#什么是卷积？" class="headerlink" title="什么是卷积？"></a>什么是卷积？</h2><p>最好理解的方式就是，一个小框在矩阵上滑动，并通过一定的计算来得到一个新的矩阵。看图吧，这样更好理解！</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/convolution.gif"></p><p>卷积神经网络的核心思想是捕捉局部特征，对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。</p><h2 id="TextCNN原理"><a href="#TextCNN原理" class="headerlink" title="TextCNN原理"></a>TextCNN原理</h2><p>Yoon Kim在论文(2014 EMNLP) <a href="https://link.zhihu.com/?target=https://pan.baidu.com/disk/pdfview?path=%252Fpaper%252Fnlp%252FConvolutional%2520Neural%2520Networks%2520for%2520Sentence%2520Classification.pdf">Convolutional Neural Networks for Sentence Classification</a>提出TextCNN。将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性。</p><p>每一个单词的embedding固定，所以kernel size的宽度不变，只能改变高度。kernel的通道可以理解为用不同的词向量表示。</p><p>输入句子的长度不一样，但卷积核的个数一样。每个卷积核抽取单词的个数不一样，高度低的形成feature maps长度就较长，高度高的形成feature maps的长度就较短，但feature maps的长度不影响后面的输入，因为通过Max-over-time pooling层后，每一个feature maps都只取一个最大值，所以最终形成的向量与卷积核的个数一致。</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/textcnn2.png"></p><h2 id="TextCNN模型结构"><a href="#TextCNN模型结构" class="headerlink" title="TextCNN模型结构"></a>TextCNN模型结构</h2><p>TextCNN只能输入文本上纵向滑动，因为每个单词的embedding长度固定，不能截断。Filter的宽度要与输入向量一致，不同的Filter高度不一样。</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/textcnn.png"></p><p>TextCNN与image-CNN的差别： </p><ul><li>最大的不同便是在输入数据的不同； </li><li>图像是二维数据, 图像的卷积核是从左到右, 从上到下进行滑动来进行特征抽取；自然语言是一维数据, 虽然经过word-embedding 生成了二维向量，但是对词向量只能做从上到下，做从左到右滑动来进行卷积没有意义； </li><li>文本卷积宽度的固定的，宽度的就embedding的维度。文本中卷积核的设计和图像中的不同。</li><li>在文本分类中，主要是要注意一下和CV场景中不同的情况，卷积核不是一个正方形，是一个宽和word embedding相同、长表示n-gram的窗口。一个卷积层会使用多个不同大小的卷积核，往往是(3, 4, 5)这种类型。每一种大小的卷积核也会使用很多个。</li></ul><h2 id="TextCNN训练详细过程"><a href="#TextCNN训练详细过程" class="headerlink" title="TextCNN训练详细过程"></a>TextCNN训练详细过程</h2><h3 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h3><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/textcnn1.png"></p><ul><li>Embedding：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度&#x3D;5，这个可以类比为图像中的原始像素点。</li><li>Convolution：然后经过 kernel_sizes&#x3D;(2,3,4) 的一维卷积层，每个kernel_size 有两个输出 channel。我们也可以有多个不同的卷积核，分别代表不同特征的提取，它们的维度也可以不同，分别代表unigram, bigram，trigram, 4-gram等的提取。</li><li>MaxPolling：第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li><li>FullConnection and Softmax：最后接一层全连接的 softmax 层，输出每个类别的概率。</li></ul><p>通道： </p><ul><li>图像中可以利用 (R, G, B) 作为不同channel； </li><li>文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。</li></ul><p>一维卷积（conv-1d）： </p><ul><li>图像是二维数据； </li><li>文本是一维数据，因此在TextCNN卷积用的是一维卷积（在word-level上是一维卷积；虽然文本经过词向量表达后是二维数据，但是在embedding-level上的二维卷积没有意义）。一维卷积带来的问题是需要通过设计不同 kernel_size 的 filter 获取不同宽度的视野。</li></ul><h3 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h3><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/textcnn2.png"></p><p>上图的输入是一个用预训练好的词向量（Word2Vector或者glove）方法得到的一个Embedding层。每一个词向量都是通过无监督的方法训练得到的。</p><p>词向量的维度是固定的，相对于原来的One-Hot编码要小，同时在新的词向量空间语义上相近或者语法相近的单词会更加接近。</p><p>所以你可以看到这里的词向量有wait、for、the等，把这些词向量拼接起来就得到一个Embedding layer。两个维度，0轴是单词、1轴是词向量的维度（固定的）。当然实际的Embedding layer维度要比这里图像表示的大的多。至于细节，看后面代码。</p><p>到此，我们已经得到了一张二维的图（矩阵）了，利用我们用CNN处理图像的思想，后面就可以用卷积池化那一套CNN的套路来搞定了，实际上也确实这样，但是这里还有有些区别的，下面我们就看看有哪些不一样的地方。</p><h3 id="卷积-convolution"><a href="#卷积-convolution" class="headerlink" title="卷积(convolution)"></a>卷积(convolution)</h3><p>相比于一般CNN中的卷积核，这里的卷积核的宽度一般需要个词向量的维度一样，图上的维度是6 。卷积核的高度则是一个超参数可以设置，比如设置为2、3等如图。然后剩下的就是正常的卷积过程了。</p><h3 id="池化-pooling"><a href="#池化-pooling" class="headerlink" title="池化(pooling)"></a>池化(pooling)</h3><p>这里的池化操作是max-overtime-pooling，其实就是在对应的feature map求一个最大值。最后把得到的值做concate。</p><h4 id="添加池化层的作用："><a href="#添加池化层的作用：" class="headerlink" title="添加池化层的作用："></a>添加池化层的作用：</h4><ul><li>降维。这点很好理解，就是经过池化操作后，图像&quot;变小&quot;了。在图像处理中，把图像缩小就称为下采样或降采样，由此可窥见池化操作的降维性质。</li><li>不变性(invariance)。包括平移不变性(translation invariance)，旋转不变性(rotation invariance)，尺度不变性(scale invariance)。简单来说，池化操作能将卷积后得到的特征图中的特征进行统一化。另外，平移不变性，是指一个特征，无论出现在图片的哪一个位置，都会识别出来（也有人说平移不变性是权值共享带来的？）。</li><li>定长输出。比如我们的文本分类的例子中就是使用到了这个特性。无论经过卷积后得到的特征图有多大，使用池化操作后总能得到一个scalar，再将这些scalar拼接在一起，就能得到一个定长的向量；</li><li>参数减少, 进一步加速计算；</li><li>降低了过拟合的风险；</li></ul><h3 id="优化、正则化"><a href="#优化、正则化" class="headerlink" title="优化、正则化"></a>优化、正则化</h3><p>池化层后面加上全连接层和SoftMax层做分类任务，同时防止过拟合，一般会添加L2和Dropout正则化方法。最后整体使用梯度法进行参数的更新模型的优化。</p><h2 id="基于pytorch实现TextCNN模型"><a href="#基于pytorch实现TextCNN模型" class="headerlink" title="基于pytorch实现TextCNN模型"></a>基于pytorch实现TextCNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;配置参数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset</span>):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Bert的输出词向量的维度</span></span><br><span class="line">        self.hidden_size = <span class="number">768</span></span><br><span class="line">        <span class="comment"># 卷积核尺寸</span></span><br><span class="line">        self.filter_sizes = (<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 卷积核数量</span></span><br><span class="line">        self.num_filters = <span class="number">256</span></span><br><span class="line">        <span class="comment"># droptout</span></span><br><span class="line">        self.dropout = <span class="number">0.5</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(config.bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">             [nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=config.num_filters, kernel_size=(k, config.hidden_size)) <span class="keyword">for</span> k <span class="keyword">in</span> config.filter_sizes]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.droptout = nn.Dropout(config.dropout)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(config.num_filters * <span class="built_in">len</span>(config.filter_sizes), config.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">conv_and_pool</span>(<span class="params">self, x, conv</span>):</span><br><span class="line">        x = conv(x)<span class="comment">#  最后一个维度为1   ：(input_height-kenl_size+padding*2)/stride[0]</span></span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = x.squeeze(<span class="number">3</span>)<span class="comment">#去掉最后一个维度</span></span><br><span class="line">        size = x.size(<span class="number">2</span>)</span><br><span class="line">        x = F.max_pool1d(x, size)</span><br><span class="line">        x = x.squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x [ids, seq_len, mask]</span></span><br><span class="line">        context = x[<span class="number">0</span>] <span class="comment">#对应输入的句子 shape[128,32]</span></span><br><span class="line">        mask = x[<span class="number">2</span>] <span class="comment">#对padding部分进行mask shape[128,32]</span></span><br><span class="line">        encoder_out, pooled = self.bert(context, attention_mask = mask, output_all_encoded_layers = <span class="literal">False</span>) </span><br><span class="line">        out = encoder_out.unsqueeze(<span class="number">1</span>) <span class="comment">#输入卷积需要四维的数据</span></span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv)<span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.droptout(out)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://www.cnblogs.com/ModifyRong/p/11319301.html">文本分类算法TextCNN原理详解</a></li><li><a href="https://www.cnblogs.com/ModifyRong/p/11442661.html">意图识别:TextCNN的优化经验Tricks汇总</a></li><li><a href="https://www.cnblogs.com/ModifyRong/p/11442595.html">TextCNN 代码详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/40276005">自然语言中的CNN--TextCNN（基础篇）</a></li><li><a href="https://juejin.cn/post/6844904185608011789">TextCNN原理解析与代码实现</a></li><li><a href="https://zhuanlan.zhihu.com/p/77634533">深入TextCNN（一）详述CNN及TextCNN原理</a></li><li><a href="https://zhuanlan.zhihu.com/p/77634533">卷积神经网络（CNN）入门讲解</a></li><li><a href="https://zhuanlan.zhihu.com/p/68333187">CS224N笔记(十一):NLP中的CNN</a></li><li><a href="https://zhuanlan.zhihu.com/p/46531725">在NLP中理解CNN</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是卷积？&quot;&gt;&lt;a href=&quot;#什么是卷积？&quot; class=&quot;headerlink&quot; title=&quot;什么是卷积？&quot;&gt;&lt;/a&gt;什么是卷积？&lt;/h2&gt;&lt;p&gt;最好理解的方式就是，一个小框在矩阵上滑动，并通过一定的计算来得到一个新的矩阵。看图吧，这样更好理解！&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="TextCNN" scheme="https://www.songzj.com/tags/TextCNN/"/>
    
  </entry>
  
  <entry>
    <title>LSTM详解</title>
    <link href="https://www.songzj.com/posts/b16b957b/"/>
    <id>https://www.songzj.com/posts/b16b957b/</id>
    <published>2022-02-07T07:18:51.000Z</published>
    <updated>2022-02-07T07:18:51.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="长期依赖-Long-Term-Dependencies"><a href="#长期依赖-Long-Term-Dependencies" class="headerlink" title="长期依赖(Long Term Dependencies)"></a>长期依赖(Long Term Dependencies)</h2><p>传统的RNN节点输出仅由权值，偏置以及激活函数决定（图3）。RNN是一个链式结构，每个时间片使用的是相同的参数。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/rnn.png"></p><p>在深度学习领域中（尤其是RNN），“长期依赖“问题是普遍存在的。长期依赖产生的原因是当神经网络的节点经过许多阶段的计算后，之前比较长的时间片的特征已经被覆盖，例如下面例子</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eg1: The cat, which already ate a bunch of food, was full.</span><br><span class="line">      |   |     |      |     |  |   |   |   |     |   |</span><br><span class="line">     t0  t1    t2      t3    t4 t5  t6  t7  t8    t9 t10</span><br><span class="line">eg2: The cats, which already ate a bunch of food, were full.</span><br><span class="line">      |   |      |      |     |  |   |   |   |     |    |</span><br><span class="line">     t0  t1     t2     t3    t4 t5  t6  t7  t8    t9   t10</span><br></pre></td></tr></table></figure><p>我们想预测&#39;full&#39;之前系动词的单复数情况，显然full是取决于第二个单词’cat‘的单复数情况，而非其前面的单词food。随着数据时间片的增加，RNN丧失了学习连接如此远的信息的能力。</p><h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><p>梯度消失和梯度爆炸是困扰RNN模型训练的关键原因之一，产生梯度消失和梯度爆炸是由于RNN的权值矩阵循环相乘导致的，相同函数的多次组合会导致极端的非线性行为。梯度消失和梯度爆炸主要存在RNN中，因为RNN中每个时间片使用相同的权值矩阵。对于一个DNN，虽然也涉及多个矩阵的相乘，但是通过精心设计权值的比例可以避免梯度消失和梯度爆炸的问题。</p><p>处理梯度爆炸可以采用梯度截断的方法。所谓梯度截断是指将梯度值超过阈值 $\theta$ 的梯度手动降到 $\theta$ 。虽然梯度截断会一定程度上改变梯度的方向，但梯度截断的方向依旧是朝向损失函数减小的方向。</p><p>对比梯度爆炸，梯度消失不能简单的通过类似梯度截断的阈值式方法来解决，因为长期依赖的现象也会产生很小的梯度。在上面例子中，我们希望 $t_9$ 时刻能够读到 $t_1$ 时刻的特征，在这期间内我们自然不希望隐层节点状态发生很大的变化，所以 [$t_2, t_8$] 时刻的梯度要尽可能的小才能保证梯度变化小。很明显，如果我们刻意提高小梯度的值将会使模型失去捕捉长期依赖的能力。</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM的全称是Long Short Term Memory，顾名思义，它具有记忆长短期信息的能力的神经网络。<br>LSTM提出的动机是为了解决上面我们提到的长期依赖问题。<br>LSTM之所以能够解决RNN的长期依赖问题，是因为LSTM引入了门（gate）机制用于控制特征的流通和损失。</p><p>原始的 RNN 只有一个隐藏层的状态，即$h$，它对于短期的输入非常敏感。<br>再增加一个状态，即$c$，让它来保存长期的状态，称为单元状态(cell state)。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/rnn2lstm.png"><br>把上图按照时间维度展开：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/rnn2lstm2.png"></p><p>在 $t$ 时刻，LSTM 的输入有三个：当前时刻网络的输入值 $x_t$、上一时刻 LSTM 的输出值 $h_t-1$、以及上一时刻的单元状态 $c_t-1$；<br>LSTM 的输出有两个：当前时刻 LSTM 输出值 $h_t$、和当前时刻的单元状态 $c_t$</p><p>关键问题是：怎样控制长期状态 c ？</p><p>方法是：使用三个控制开关<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_gates.png"></p><p>第一个开关，负责控制继续保存长期状态c；<br>第二个开关，负责控制把即时状态输入到长期状态c；<br>第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。</p><p>如何在算法中实现这三个开关？<br>方法：用 门（gate）</p><p>定义：gate 实际上就是一层全连接层，输入是一个向量，输出是一个 0到1 之间的实数向量。<br>公式为：<br><img src= "/img/loading.gif" data-lazy-src="https://bbsmax.ikafan.com/static/L3Byb3h5L2h0dHBzL2ltZzIwMTguY25ibG9ncy5jb20vYmxvZy82OTc2ODcvMjAxOTAzLzY5NzY4Ny0yMDE5MDMyNjIwNDIxMTY0My04NDM1MjQxODAucG5n.jpg"><br>回忆一下它的样子：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_gate_math.png"></p><p>gate 如何进行控制？<br>方法：用门的输出向量按元素乘以我们需要控制的那个向量<br>原理：门的输出是 0到1 之间的实数向量，<br>当门输出为 0 时，任何向量与之相乘都会得到 0 向量，这就相当于什么都不能通过；<br>输出为 1 时，任何向量与之相乘都不会有任何改变，这就相当于什么都可以通过。</p><p>LSTM 的前向计算:<br>遗忘门（forget gate）<br>它决定了上一时刻的单元状态 $c_t-1$ 有多少保留到当前时刻 $c_t$</p><p>输入门（input gate）<br>它决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$</p><p>输出门（output gate）<br>控制单元状态 $c_t$ 有多少输出到 LSTM 的当前输出值 $h_t$</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm.png"></p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm1.png"></p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm2.png"></p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm3.png"></p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm4.png"></p><p>（１）遗忘门（forget gate）：<br>它决定了上一时刻的单元状态 $c_t-1$ 有多少保留到当前时刻 $c_t$</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_forget_gate.png"><br>（２）输入门（input gate）：<br>它决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_input_gate.png"></p><p>（３）输出门（output gate）：<br>控制单元状态 $c_t$ 有多少输出到 LSTM 的当前输出值 $h_t$</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_output_gate.png"></p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_update_state.png"></p><p>LSTM 的反向传播训练算法</p><p>主要有三步：</p><ol><li><p>前向计算每个神经元的输出值，一共有 5 个变量，计算方法就是前一部分：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_bp.png"></p></li><li><p>反向计算每个神经元的误差项值。与 RNN 一样，LSTM 误差项的反向传播也是包括两个方向：<br>一个是沿时间的反向传播，即从当前 t 时刻开始，计算每个时刻的误差项；<br>一个是将误差项向上一层传播。</p></li><li><p>根据相应的误差项，计算每个权重的梯度。</p></li></ol><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_state.png"></p><p>目标是要学习 8 组参数，如下图所示：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_bp2.png"><br>又权重矩阵 $W$ 都是由两个矩阵拼接而成，这两部分在反向传播中使用不同的公式，因此在后续的推导中，权重矩阵也要被写为分开的两个矩阵。</p><p>接着就来求两个方向的误差，和一个梯度计算。</p><ul><li><p>误差项沿时间的反向传递：<br>定义 $t$ 时刻的误差项：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_bp3.png"><br>目的是要计算出 $t-1$ 时刻的误差项：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_bp4.png"></p></li><li><p>利用 $h_t$ $c_t$ 的定义，和全导数公式，可以得到 将误差项向前传递到任意$k$时刻的公式：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_bp5.png"></p></li><li><p>权重梯度的计算：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/lstm_bp6.png"></p></li></ul><h2 id="LSTM优缺点"><a href="#LSTM优缺点" class="headerlink" title="LSTM优缺点"></a>LSTM优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>有助于缓解梯度消失现象；</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>$t$时刻的计算需要依赖于$t-1$时刻的值所以无法并行计算；</li></ul><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://www.bbsmax.com/A/nAJv8B1mdr/">详解 LSTM</a></li><li><a href="https://www.sohu.com/a/128669963_642762">详解LSTM的使用方法及其不同变体的结构特征</a></li><li><a href="https://blog.csdn.net/qq_31278903/article/details/88690959">LSTM原理详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/42717426">大师兄：详解LSTM</a></li><li><a href="https://baijiahao.baidu.com/s?id=1573792228593933&wfr=spider&for=pc">LSTM入门必读：从基础知识到工作方式详解</a></li><li><a href="https://www.zhihu.com/zvideo/1323002838411423744">视频：LSTM架构讲解</a></li><li><a href="https://www.jiqizhixin.com/articles/2018-12-14-4">RNN 结构详解</a></li><li><a href="https://easyai.tech/ai-definition/rnn/">easyai：循环神经网络 – Recurrent Neural Network | RNN</a></li><li><a href="https://zhuanlan.zhihu.com/p/103182683">RNN&#x2F;LSTM&#x2F;GRU 详解+tensorflow使用</a></li><li><a href="https://zhuanlan.zhihu.com/p/337700483">LSTM架构详解</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;长期依赖-Long-Term-Dependencies&quot;&gt;&lt;a href=&quot;#长期依赖-Long-Term-Dependencies&quot; class=&quot;headerlink&quot; title=&quot;长期依赖(Long Term Dependencies)&quot;&gt;&lt;/a&gt;长期依赖</summary>
      
    
    
    
    
    <category term="LSTM" scheme="https://www.songzj.com/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>TinyBert详解</title>
    <link href="https://www.songzj.com/posts/3197c121/"/>
    <id>https://www.songzj.com/posts/3197c121/</id>
    <published>2022-01-30T03:28:39.000Z</published>
    <updated>2022-01-30T03:28:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>BERT 等大模型性能强大，但很难部署到算力、内存有限的设备中。为此，来自华中科技大学、华为诺亚方舟实验室的研究者提出了 TinyBert，这是为基于 transformer 的模型专门设计的知识蒸馏（knowledge distillation，KD）方法。通过这种新的 KD 方法，大型 teacherBERT 模型中编码的大量知识可以很好地迁移到小型 student TinyBert模型中。模型大小还不到 BERT 的 1&#x2F;7，但速度是 BERT 的 9 倍还要多，而且性能没有出现明显下降。</p><p>TinyBert 的结构如下图：<br><img src= "/img/loading.gif" data-lazy-src="https://pics1.baidu.com/feed/91529822720e0cf3056ddb2f4c667a1abf09aa76.jpeg"><br>在TinyBert中，student 和 teacher 网络都是通过 Transformer 层构建的。<br>此外，研究者还提出了一种专门用于 TinyBERT 的两段式学习框架，从而分别在预训练和针对特定任务的学习阶段执行 transformer 蒸馏。这一框架确保 TinyBert 可以获取 teacherBERT 的通用知识和针对特定任务的知识。</p><p>除了提出新的 transformer 蒸馏法之外，研究者还提出了一种专门用于 TinyBERT 的两段式学习框架，从而分别在预训练和针对特定任务的具体学习阶段执行 transformer 蒸馏。这一框架确保 TinyBERT 可以获取 teacherBERT 的通用和针对特定任务的知识。</p><h2 id="BERT模型的瘦身方法"><a href="#BERT模型的瘦身方法" class="headerlink" title="BERT模型的瘦身方法"></a>BERT模型的瘦身方法</h2><h3 id="1-网络剪枝："><a href="#1-网络剪枝：" class="headerlink" title="1) 网络剪枝："></a>1) 网络剪枝：</h3><p>网络剪枝包括从模型中删除一部分不太重要的权重从而产生稀疏的权重矩阵，或者直接去掉与注意力头相对应的矩阵等方法来实现模型的剪枝，还有一些模型通过正则化方法实现剪枝。</p><h3 id="2-低秩分解："><a href="#2-低秩分解：" class="headerlink" title="2) 低秩分解："></a>2) 低秩分解：</h3><p>即将原来大的权重矩阵分解多个低秩的小矩阵从而减少了运算量。这种方法既可以用于词向量以节省磁盘内存，也可以用到前馈层或自注意力层的参数矩阵中以加快模型训练速度。</p><h3 id="3-知识蒸馏"><a href="#3-知识蒸馏" class="headerlink" title="3) 知识蒸馏"></a>3) 知识蒸馏</h3><p>通过引入教师网络用以诱导学生网络的训练，实现知识迁移。教师网络拥有复杂的结构用以训练出推理性能优越的概率分布，是把概率分布这部分精华从复杂结构中“蒸馏”出来，再用其指导精简的学生网络的训练，从而实现模型压缩，即所谓知识蒸馏。蒸馏简单的说是将大模型（teacher）的学习结果，作为小模型（student）的学习目标，这样将大模型学到的知识迁移到另一个轻量级单模型上。<br>teacher和student模型原理甚至可以毫不相关，它的work原理，一方面student模型的loss构造学习了本身的true-label，也学到了teacher model的soft label， soft label本身也相对精确的模型(teacher)是数据泛化的一种结果，例如在二分类中，true label是【伤痛欲绝】，teacher大模型学到的是【一点点忧郁】，那【一点点忧郁】作为soft-label 也是student的学习目标，对于student只用泛化到不是【开心】就足够了。</p><h3 id="4-参数共享"><a href="#4-参数共享" class="headerlink" title="4) 参数共享"></a>4) 参数共享</h3><p>ALBERT模型是BERT模型的改进版，其改进之一就是参数共享。全连接层与自注意力层都实现参数共享，即共享了编码器中的所有参数，这样不仅减少了参数量还提升了训练速度。</p><h3 id="5-量化"><a href="#5-量化" class="headerlink" title="5) 量化"></a>5) 量化</h3><p>通过减少每个参数所需的比特数来压缩原始网络，可以显著降低内存需求。</p><h3 id="6-预训练和Downstream"><a href="#6-预训练和Downstream" class="headerlink" title="6) 预训练和Downstream"></a>6) 预训练和Downstream</h3><p>模型压缩可以在模型训练时进行也可以在模型训练好之后进行。后期压缩使得训练更快，通常不需要训练数据，而训练期间压缩可以保持更高的准确性并导致更高的压缩率。</p><h2 id="TinyBert瘦身方法"><a href="#TinyBert瘦身方法" class="headerlink" title="TinyBert瘦身方法"></a>TinyBert瘦身方法</h2><p>Tinybert主要用到的方法是模型的蒸馏。<br>知识蒸馏使用的是老师-学生（Teacher-Student）[1]模型，其中老师模型是“知识”的输出者，学生模型是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li>原始模型训练</li></ol><p>老师模型（Net-T）的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对老师模型不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是对于输入X, 其都能输出Y。其中Y经过Softmax函数的映射，输出值对应相应类别的概率值。</p><ol start="2"><li>模型蒸馏</li></ol><p>学生模型（Net-S）的特点是参数量较小、模型结构相对简单的单模型。同样地，对于输入X，其都能输出Y，Y经过Softmax函数映射输出对应相应类别的概率值。</p><p>在知识蒸馏的论文中，作者将问题限定在分类问题下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个Softmax函数，其输出值对应了相应类别的概率值。</p><p>回到机器学习最基础的理论，机器学习最根本的目的是训练出在某个问题上泛化能力强的模型。即在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。</p><p>而现实中，由于我们不可能收集到某问题的所有数据作为训练数据，并且新数据总是在源源不断的产生，因此我们只能退而求其次，训练目标变成在已有的训练数据集上建模输入和输出之间的关系。由于训练数据集是对真实数据分布情况的采样，训练数据集上的最优解往往会多少偏离真正的最优解(这里的讨论不考虑模型容量)。</p><p>而在知识蒸馏时，由于我们已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p><p>一个很直白且高效的迁移泛化能力的方法就是：使用Softmax层输出的类别的概率来作为“soft target”（软标签）。</p><p>如下图11.18所示，传统机器学习模型在训练过程中拟合的标签为硬标签(Hard targets)，即对真实类别的标签取独热编码并求极大似然，而知识蒸馏的训练过程则是使用了软标签，用大模型的的各个类别预测的概率作为软标签（Soft targets）。</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-3f8b447fc51892903d93efe17e11d531_r.jpg"><br>图11.18 硬标签与软标签</p><p>这是由于大模型softmax层的输出，除了正例之外，负标签也带有大量的信息，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，KD的训练方式使得每个样本给Net-S带来的信息量远远大于传统的训练方式，通过软标签的学习可以让大模型教会小模型如何去学习。</p><p>而这个构造软标签的过程，涉及到知识蒸馏一个非常经典的概念，蒸馏温度。在介绍蒸馏温度之前，我们回顾一下Softmax公式。</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-9c902bfee70a0a751741d85a7ea553ac_r.jpg"></p><p>但如果直接使用Softmax层的输出值作为软标签, 这又会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此&quot;蒸馏温度&quot;这个变量就派上了用场，如下式11.50所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-acb140a1c44c8230455c7693de5713c6_r.jpg"></p><p>T即蒸馏温度。当T&#x3D;1时，该式即是正常的Softmax公司。随着T越变高，softmax的输出概率结果也会越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将能关注到负标签的信息。其中红色柱为真实标签的类别概率，蓝色柱为负标签的类别概率。</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-e713b6791dde3788901e7a9cb4242b59_r.jpg"><br>图11.19引入蒸馏温度软标签的变化</p><p>通用的知识蒸馏框架图如图11.20所示。训练Net-T的过程即我们正常任务使用大模型完成当前的任务，下面详细讲讲第二步:高温蒸馏的过程。高温蒸馏过程的目标函数由distill loss(对应soft target)和student loss(对应hard target)加权得到，其表达式如下式11.51所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-c8a64afad135316e58527e5fba2cca91_r.jpg"></p><p>图11.20 知识蒸馏通用框架</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-1b4b2c87781ec80e6be5b60025e42b93_r.jpg"></p><p>为何第二部分Loss 仍引入硬标签呢？这是因为教师网络也有一定的错误率，使用真实标签的one hot编码可以有效降低错误被传播给学生网络的可能。打个比方，老师虽然学识远远超过学生，但是他仍然有出错的可能，而如果学生可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</p><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><p><a href="https://zhuanlan.zhihu.com/p/94359189">比 Bert 体积更小速度更快的 TinyBERT</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/384521670">模型蒸馏与剪枝</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/282777488">Bert模型压缩(蒸馏|剪枝|量化)小记</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/444629182">BERT 模型的知识蒸馏： DistilBERT 方法的理论和机制研究</a></p></li><li><p><a href="https://baijiahao.baidu.com/s?id=1646220794556507928&wfr=spider&for=pc">TinyBERT：模型小7倍，速度快8倍，华中科大、华为出品</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;BERT 等大模型性能强大，但很难部署到算力、内存有限的设备中。为此，来自华中科技大学、华为诺亚方舟实验室的研究者提出了 TinyBert，这是为基于 transformer 的模型专门设计的知识蒸馏（knowledge distillation，KD）方法。通过这种新的 </summary>
      
    
    
    
    
    <category term="TinyBert" scheme="https://www.songzj.com/tags/TinyBert/"/>
    
  </entry>
  
  <entry>
    <title>Normalization方法总结</title>
    <link href="https://www.songzj.com/posts/cf4456cd/"/>
    <id>https://www.songzj.com/posts/cf4456cd/</id>
    <published>2022-01-29T07:26:03.000Z</published>
    <updated>2022-01-29T07:26:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h2><p>描述：</p><p>将数据映射到指定的范围，如：把数据映射到0～1或-1~1的范围之内处理。</p><p>作用：</p><ul><li>数据映射到指定的范围内进行处理，更加便捷快速。</li><li>把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。经过归一化后，将有量纲的数据集变成纯量，还可以达到简化计算的作用。</li></ul><p>常见做法：Min-Max归一化</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-768d71b5ced77365e42b7b9e8108c903_720w.jpg"></p><h2 id="标准化（Normalization）"><a href="#标准化（Normalization）" class="headerlink" title="标准化（Normalization）"></a>标准化（Normalization）</h2><p>注：在英文翻译中，归一化和标准化的翻译是一致的，而在实际使用中，我们需要根据实际的公式（或用途）去理解~</p><p>数据标准化方法有多种，如：直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。不同的标准化方法，对系统的评价结果会产生不同的影响。其中，最常用的是Z-Score 标准化。</p><p>Z-Score 标准化<br><img src= "/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-5e77b78462a8e2d91d19ffb12545f2e2_720w.jpg"><br>其中，$\mu$为数据均值（mean），$\sigma$为标准差（std）。</p><p>描述：<br>将原数据转换为符合均值为0，标准差为1的标准正态分布的新数据。</p><p>作用：</p><ul><li>提升模型的收敛速度（加快梯度下降的求解速度）</li><li>提升模型的精度（消除量级和量纲的影响）</li><li>简化计算（与归一化的简化原理相同）</li></ul><h2 id="归一化和标准化的作用和区别"><a href="#归一化和标准化的作用和区别" class="headerlink" title="归一化和标准化的作用和区别"></a>归一化和标准化的作用和区别</h2><p>在很多情况下，归一化和标准化的效果区别不是很大。他们两者首先都是线性变化，基本维持了原始数据的分部特征。</p><p>1.显然归一化和标准化往往会压缩且平移了数据，这样做两大好处：避免分布数据偏移和远离导数饱和区。</p><p>2.归一化和标准化可以将每一层的网络输入都变成固定的分部。</p><p>3.从公式不难看出，归一化的缩放仅跟最大值和最小值有关，当数据具有离散值的时候，这种方式会造成一定的误差。但是标准化确发挥了每个数据的作用，对原有的数据特征保持的更好。标准化缩放不会有范围的限制，综上，如果你的场景要求不是一定要使数据维持在（0，1）范围内的话，通常用标准化是更稳妥的，而对于将数据需要压缩到1个范围内时，用归一化。</p><p>4.此外，归一化和标准化还可以将多组有量纲数据转成无量纲数据，消除量级和量纲的影响。</p><p>5.数据分部更集中，计算更加的简单快捷。</p><h2 id="使用归一化-x2F-标准化会改变数据原来的规律吗？"><a href="#使用归一化-x2F-标准化会改变数据原来的规律吗？" class="headerlink" title="使用归一化&#x2F;标准化会改变数据原来的规律吗？"></a>使用归一化&#x2F;标准化会改变数据原来的规律吗？</h2><p>归一化&#x2F;标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化&#x2F;标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。</p><h2 id="如果是单纯想实现消除量级和量纲的影响，用Min-Max还是用Z-Score？"><a href="#如果是单纯想实现消除量级和量纲的影响，用Min-Max还是用Z-Score？" class="headerlink" title="如果是单纯想实现消除量级和量纲的影响，用Min-Max还是用Z-Score？"></a>如果是单纯想实现消除量级和量纲的影响，用Min-Max还是用Z-Score？</h2><p>1、数据的分布本身就服从正态分布，使用Z-Score。</p><p>2、有离群值的情况：使用Z-Score。</p><p>这里不是说有离群值时使用Z-Score不受影响，而是，Min-Max对于离群值十分敏感，因为离群值的出现，会影响数据中max或min值，从而使Min-Max的效果很差。相比之下，虽然使用Z-Score计算方差和均值的时候仍然会受到离群值的影响，但是相比于Min-Max法，影响会小一点。</p><h2 id="当数据出现离群点时，用什么方法？"><a href="#当数据出现离群点时，用什么方法？" class="headerlink" title="当数据出现离群点时，用什么方法？"></a>当数据出现离群点时，用什么方法？</h2><p>当数据中有离群点时，我们可以使用Z-Score进行标准化，但是标准化后的数据并不理想，因为异常点的特征往往在标准化后容易失去离群特征，此时就可以用RobustScaler 针对离群点做标准化处理。</p><h2 id="Robust标准化（RobustScaler）"><a href="#Robust标准化（RobustScaler）" class="headerlink" title="Robust标准化（RobustScaler）"></a>Robust标准化（RobustScaler）</h2><p>很多时候我们在机器学习中，或是其他模型都会经常见到一个词：鲁棒性。也就是Robust的音译。</p><p>计算机科学中，健壮性（英语：Robustness）是指一个计算机系统在执行过程中处理错误，以及算法在遭遇输入、运算等异常时继续正常运行的能力。 诸如模糊测试之类的形式化方法中，必须通过制造错误的或不可预期的输入来验证程序的健壮性。很多商业产品都可用来测试软件系统的健壮性。健壮性也是失效评定分析中的一个方面。</p><p>关于Robust,是这么描述的：</p><p>This Scaler removes the median（中位数） and scales the data according to the quantile range(四分位距离，也就是说排除了outliers).</p><p>Huber从稳健统计的角度系统地给出了鲁棒性3个层面的概念：</p><p>一是模型具有较高的精度或有效性，这也是对于机器学习中所有学习模型的基本要求；</p><p>二是对于模型假设出现的较小偏差，只能对算法性能产生较小的影响；</p><p>主要是：噪声（noise）</p><p>三是对于模型假设出现的较大偏差，不可对算法性能产生“灾难性”的影响。</p><p>主要是：离群点（outlier）</p><p>在机器学习，训练模型时，工程师可能会向算法内添加噪声（如对抗训练），以便测试算法的「鲁棒性」。可以将此处的鲁棒性理解述算法对数据变化的容忍度有多高。鲁棒性并不同于稳定性，稳定性通常意味着「特性随时间不变化的能力」，鲁棒性则常被用来描述可以面对复杂适应系统的能力，需要更全面的对系统进行考虑。</p><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><p><a href="https://zhuanlan.zhihu.com/p/269744099">深度学习Normalization方法们的总结</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/135473375">数据预处理——标准化&#x2F;归一化</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;归一化（Normalization）&quot;&gt;&lt;a href=&quot;#归一化（Normalization）&quot; class=&quot;headerlink&quot; title=&quot;归一化（Normalization）&quot;&gt;&lt;/a&gt;归一化（Normalization）&lt;/h2&gt;&lt;p&gt;描述：&lt;/p</summary>
      
    
    
    
    
    <category term="Normalization" scheme="https://www.songzj.com/tags/Normalization/"/>
    
  </entry>
  
  <entry>
    <title>Markdown数学公式语法</title>
    <link href="https://www.songzj.com/posts/378cfac4/"/>
    <id>https://www.songzj.com/posts/378cfac4/</id>
    <published>2022-01-25T02:09:15.000Z</published>
    <updated>2022-01-25T02:09:15.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="行内与独行"><a href="#行内与独行" class="headerlink" title="行内与独行"></a>行内与独行</h2><ol><li><p>行内公式：将公式插入到本行内，符号：<code>$公式内容$</code>，如：$xyz$</p></li><li><p>独行公式：将公式插入到新的一行内，并且居中，符号：<code>$$公式内容$$</code>，如：</p><p>$$xyz$$</p></li></ol><h2 id="上标、下标与组合"><a href="#上标、下标与组合" class="headerlink" title="上标、下标与组合"></a>上标、下标与组合</h2><ol><li>上标符号，符号：<code>^</code>，如：$x^4$</li><li>下标符号，符号：<code>_</code>，如：$x_1$</li><li>组合符号，符号：<code>&#123;&#125;</code>，如：${16}<em>{8}O{2+}</em>{2}$</li></ol><h2 id="汉字、字体与格式"><a href="#汉字、字体与格式" class="headerlink" title="汉字、字体与格式"></a>汉字、字体与格式</h2><ol><li>汉字形式，符号：<code>\mbox&#123;&#125;</code>，如：$V_{\mbox{初始}}$</li><li>字体控制，符号：<code>\displaystyle</code>，如：$\displaystyle \frac{x+y}{y+z}$</li><li>下划线符号，符号：<code>\underline</code>，如：$\underline{x+y}$</li><li>标签，符号<code>\tag&#123;数字&#125;</code>，如：$\tag{11}$</li><li>上大括号，符号：<code>\overbrace&#123;算式&#125;</code>，如：$\overbrace{a+b+c+d}^{2.0}$</li><li>下大括号，符号：<code>\underbrace&#123;算式&#125;</code>，如：$a+\underbrace{b+c}_{1.0}+d$</li><li>上位符号，符号：<code>\stacrel&#123;上位符号&#125;&#123;基位符号&#125;</code>，如：$\vec{x}\stackrel{\mathrm{def}}{&#x3D;}{x_1,\dots,x_n}$</li></ol><h2 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h2><ol><li>两个quad空格，符号：<code>\qquad</code>，如：$x \qquad y$</li><li>quad空格，符号：<code>\quad</code>，如：$x \quad y$</li><li>大空格，符号<code>\</code>，如：$x \  y$</li><li>中空格，符号<code>\:</code>，如：$x : y$</li><li>小空格，符号<code>\,</code>，如：$x , y$</li><li>没有空格，符号&#96;&#96;，如：$xy$</li><li>紧贴，符号<code>\!</code>，如：$x ! y$</li></ol><h2 id="定界符与组合"><a href="#定界符与组合" class="headerlink" title="定界符与组合"></a>定界符与组合</h2><ol><li>括号，符号：<code>()\big(\big) \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)</code>，如：$()\big(\big) \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)$</li><li>中括号，符号：<code>[]</code>，如：$[x+y]$</li><li>大括号，符号：<code>\&#123; \&#125;</code>，如：${x+y}$</li><li>自适应括号，符号：<code>\left \right</code>，如：$\left(x\right)$，$\left(x{yz}\right)$</li><li>组合公式，符号：<code>&#123;上位公式 \choose 下位公式&#125;</code>，如：${n+1 \choose k}&#x3D;{n \choose k}+{n \choose k-1}$</li><li>组合公式，符号：<code>&#123;上位公式 \atop 下位公式&#125;</code>，如：$\sum_{k_0,k_1,\ldots&gt;0 \atop k_0+k_1+\cdots&#x3D;n}A_{k_0}A_{k_1}\cdots$</li></ol><h2 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h2><ol><li>加法运算，符号：<code>+</code>，如：$x+y&#x3D;z$</li><li>减法运算，符号：<code>-</code>，如：$x-y&#x3D;z$</li><li>加减运算，符号：<code>\pm</code>，如：$x \pm y&#x3D;z$</li><li>减甲运算，符号：<code>\mp</code>，如：$x \mp y&#x3D;z$</li><li>乘法运算，符号：<code>\times</code>，如：$x \times y&#x3D;z$</li><li>点乘运算，符号：<code>\cdot</code>，如：$x \cdot y&#x3D;z$</li><li>星乘运算，符号：<code>\ast</code>，如：$x \ast y&#x3D;z$</li><li>除法运算，符号：<code>\div</code>，如：$x \div y&#x3D;z$</li><li>斜法运算，符号：<code>/</code>，如：$x&#x2F;y&#x3D;z$</li><li>分式表示，符号：<code>\frac&#123;分子&#125;&#123;分母&#125;</code>，如：$\frac{x+y}{y+z}$</li><li>分式表示，符号：<code>&#123;分子&#125; \voer &#123;分母&#125;</code>，如：${x+y} \over {y+z}$</li><li>绝对值表示，符号：<code>||</code>，如：$|x+y|$</li></ol><h2 id="高级运算"><a href="#高级运算" class="headerlink" title="高级运算"></a>高级运算</h2><ol><li>平均数运算，符号：<code>\overline&#123;算式&#125;</code>，如：$\overline{xyz}$</li><li>开二次方运算，符号：<code>\sqrt</code>，如：$\sqrt x$</li><li>开方运算，符号：<code>\sqrt[开方数]&#123;被开方数&#125;</code>，如：$\sqrt[3]{x+y}$</li><li>对数运算，符号：<code>\log</code>，如：$\log(x)$</li><li>极限运算，符号：<code>\lim</code>，如：$\lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</li><li>极限运算，符号：<code>\displaystyle \lim</code>，如：$\displaystyle \lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</li><li>求和运算，符号：<code>\sum</code>，如：$\sum^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</li><li>求和运算，符号：<code>\displaystyle \sum</code>，如：$\displaystyle \sum^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</li><li>积分运算，符号：<code>\int</code>，如：$\int^{\infty}_{0}{xdx}$</li><li>积分运算，符号：<code>\displaystyle \int</code>，如：$\displaystyle \int^{\infty}_{0}{xdx}$</li><li>微分运算，符号：<code>\partial</code>，如：$\frac{\partial x}{\partial y}$</li><li>矩阵表示，符号：<code>\begin&#123;matrix&#125; \end&#123;matrix&#125;</code>，如：$A&#x3D; \left{ \begin{matrix} a &amp; b &amp; \cdots &amp; e\ f &amp; g &amp; \cdots &amp; j \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ p &amp; q &amp; \cdots &amp; t \end{matrix} \right}$<br>或者<br>$A&#x3D; \left{ \begin{array}{cccc|c} a &amp; b &amp; c &amp; d &amp; e\ f &amp; g &amp; h &amp; i &amp; j \ k &amp; l &amp; m &amp; n &amp; o \ p &amp; q &amp; r &amp; s &amp; t \end{array} \right}$</li></ol><h2 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h2><ol><li>等于运算，符号：<code>=</code>，如：$x+y&#x3D;z$</li><li>大于运算，符号：<code>&gt;</code>，如：$x+y&gt;z$</li><li>小于运算，符号：<code>&lt;</code>，如：$x+y&lt;z$</li><li>大于等于运算，符号：<code>\geq</code>，如：$x+y \geq z$</li><li>小于等于运算，符号：<code>\leq</code>，如：$x+y \leq z$</li><li>不等于运算，符号：<code>\neq</code>，如：$x+y \neq z$</li><li>不大于等于运算，符号：<code>\ngeq</code>，如：$x+y \ngeq z$</li><li>不大于等于运算，符号：<code>\not\geq</code>，如：$x+y \not\geq z$</li><li>不小于等于运算，符号：<code>\nleq</code>，如：$x+y \nleq z$</li><li>不小于等于运算，符号：<code>\not\leq</code>，如：$x+y \not\leq z$</li><li>约等于运算，符号：<code>\approx</code>，如：$x+y \approx z$</li><li>恒定等于运算，符号：<code>\equiv</code>，如：$x+y \equiv z$</li></ol><h2 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h2><ol><li>属于运算，符号：<code>\in</code>，如：$x \in y$</li><li>不属于运算，符号：<code>\notin</code>，如：$x \notin y$</li><li>不属于运算，符号：<code>\not\in</code>，如：$x \not\in y$</li><li>子集运算，符号：<code>\subset</code>，如：$x \subset y$</li><li>子集运算，符号：<code>\supset</code>，如：$x \supset y$</li><li>真子集运算，符号：<code>\subseteq</code>，如：$x \subseteq y$</li><li>非真子集运算，符号：<code>\subsetneq</code>，如：$x \subsetneq y$</li><li>真子集运算，符号：<code>\supseteq</code>，如：$x \supseteq y$</li><li>非真子集运算，符号：<code>\supsetneq</code>，如：$x \supsetneq y$</li><li>非子集运算，符号：<code>\not\subset</code>，如：$x \not\subset y$</li><li>非子集运算，符号：<code>\not\supset</code>，如：$x \not\supset y$</li><li>并集运算，符号：<code>\cup</code>，如：$x \cup y$</li><li>交集运算，符号：<code>\cap</code>，如：$x \cap y$</li><li>差集运算，符号：<code>\setminus</code>，如：$x \setminus y$</li><li>同或运算，符号：<code>\bigodot</code>，如：$x \bigodot y$</li><li>同与运算，符号：<code>\bigotimes</code>，如：$x \bigotimes y$</li><li>实数集合，符号：<code>\mathbb&#123;R&#125;</code>，如：$\mathbb{R}$</li><li>自然数集合，符号：<code>\mathbb&#123;Z&#125;</code>，如：$\mathbb{Z}$</li><li>空集，符号：<code>\emptyset</code>，如：$\emptyset$</li></ol><h2 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h2><ol><li>无穷，符号：<code>\infty</code>，如：$\infty$</li><li>虚数，符号：<code>\imath</code>，如：$\imath$</li><li>虚数，符号：<code>\jmath</code>，如：$\jmath$</li><li>数学符号，符号<code>\hat&#123;a&#125;</code>，如：$\hat{a}$</li><li>数学符号，符号<code>\check&#123;a&#125;</code>，如：$\check{a}$</li><li>数学符号，符号<code>\breve&#123;a&#125;</code>，如：$\breve{a}$</li><li>数学符号，符号<code>\tilde&#123;a&#125;</code>，如：$\tilde{a}$</li><li>数学符号，符号<code>\bar&#123;a&#125;</code>，如：$\bar{a}$</li><li>矢量符号，符号<code>\vec&#123;a&#125;</code>，如：$\vec{a}$</li><li>数学符号，符号<code>\acute&#123;a&#125;</code>，如：$\acute{a}$</li><li>数学符号，符号<code>\grave&#123;a&#125;</code>，如：$\grave{a}$</li><li>数学符号，符号<code>\mathring&#123;a&#125;</code>，如：$\mathring{a}$</li><li>一阶导数符号，符号<code>\dot&#123;a&#125;</code>，如：$\dot{a}$</li><li>二阶导数符号，符号<code>\ddot&#123;a&#125;</code>，如：$\ddot{a}$</li><li>上箭头，符号：<code>\uparrow</code>，如：$\uparrow$</li><li>上箭头，符号：<code>\Uparrow</code>，如：$\Uparrow$</li><li>下箭头，符号：<code>\downarrow</code>，如：$\downarrow$</li><li>下箭头，符号：<code>\Downarrow</code>，如：$\Downarrow$</li><li>左箭头，符号：<code>\leftarrow</code>，如：$\leftarrow$</li><li>左箭头，符号：<code>\Leftarrow</code>，如：$\Leftarrow$</li><li>右箭头，符号：<code>\rightarrow</code>，如：$\rightarrow$</li><li>右箭头，符号：<code>\Rightarrow</code>，如：$\Rightarrow$</li><li>底端对齐的省略号，符号：<code>\ldots</code>，如：$1,2,\ldots,n$</li><li>中线对齐的省略号，符号：<code>\cdots</code>，如：$x_1^2 + x_2^2 + \cdots + x_n^2$</li><li>竖直对齐的省略号，符号：<code>\vdots</code>，如：$\vdots$</li><li>斜对齐的省略号，符号：<code>\ddots</code>，如：$\ddots$</li></ol><h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2><table><thead><tr><th>字母</th><th>实现</th><th>字母</th><th>实现</th></tr></thead><tbody><tr><td>A</td><td><code>A</code></td><td>α</td><td><code>\alhpa</code></td></tr><tr><td>B</td><td><code>B</code></td><td>β</td><td><code>\beta</code></td></tr><tr><td>Γ</td><td><code>\Gamma</code></td><td>γ</td><td><code>\gamma</code></td></tr><tr><td>Δ</td><td><code>\Delta</code></td><td>δ</td><td><code>\delta</code></td></tr><tr><td>E</td><td><code>E</code></td><td>ϵ</td><td><code>\epsilon</code></td></tr><tr><td>Z</td><td><code>Z</code></td><td>ζ</td><td><code>\zeta</code></td></tr><tr><td>H</td><td><code>H</code></td><td>η</td><td><code>\eta</code></td></tr><tr><td>Θ</td><td><code>\Theta</code></td><td>θ</td><td><code>\theta</code></td></tr><tr><td>I</td><td><code>I</code></td><td>ι</td><td><code>\iota</code></td></tr><tr><td>K</td><td><code>K</code></td><td>κ</td><td><code>\kappa</code></td></tr><tr><td>Λ</td><td><code>\Lambda</code></td><td>λ</td><td><code>\lambda</code></td></tr><tr><td>M</td><td><code>M</code></td><td>μ</td><td><code>\mu</code></td></tr><tr><td>N</td><td><code>N</code></td><td>ν</td><td><code>\nu</code></td></tr><tr><td>Ξ</td><td><code>\Xi</code></td><td>ξ</td><td><code>\xi</code></td></tr><tr><td>O</td><td><code>O</code></td><td>ο</td><td><code>\omicron</code></td></tr><tr><td>Π</td><td><code>\Pi</code></td><td>π</td><td><code>\pi</code></td></tr><tr><td>P</td><td><code>P</code></td><td>ρ</td><td><code>\rho</code></td></tr><tr><td>Σ</td><td><code>\Sigma</code></td><td>σ</td><td><code>\sigma</code></td></tr><tr><td>T</td><td><code>T</code></td><td>τ</td><td><code>\tau</code></td></tr><tr><td>Υ</td><td><code>\Upsilon</code></td><td>υ</td><td><code>\upsilon</code></td></tr><tr><td>Φ</td><td><code>\Phi</code></td><td>ϕ</td><td><code>\phi</code></td></tr><tr><td>X</td><td><code>X</code></td><td>χ</td><td><code>\chi</code></td></tr><tr><td>Ψ</td><td><code>\Psi</code></td><td>ψ</td><td><code>\psi</code></td></tr><tr><td>Ω</td><td><code>\v</code></td><td>ω</td><td><code>\omega</code></td></tr></tbody></table><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://blog.csdn.net/weixin_39814454/article/details/111270176">markdown纯手撸复杂数学公式</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;行内与独行&quot;&gt;&lt;a href=&quot;#行内与独行&quot; class=&quot;headerlink&quot; title=&quot;行内与独行&quot;&gt;&lt;/a&gt;行内与独行&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;行内公式：将公式插入到本行内，符号：&lt;code&gt;$公式内容$&lt;/code&gt;，如：$xyz$&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="教程" scheme="https://www.songzj.com/tags/%E6%95%99%E7%A8%8B/"/>
    
    <category term="Markdown" scheme="https://www.songzj.com/tags/Markdown/"/>
    
    <category term="数学公式" scheme="https://www.songzj.com/tags/%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Roberta详解</title>
    <link href="https://www.songzj.com/posts/b1946a71/"/>
    <id>https://www.songzj.com/posts/b1946a71/</id>
    <published>2022-01-24T12:55:08.000Z</published>
    <updated>2022-01-24T12:55:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="roberta三个训练改进："><a href="#roberta三个训练改进：" class="headerlink" title="roberta三个训练改进："></a>roberta三个训练改进：</h2><ul><li><p>去掉下一句预测(NSP)任务</p></li><li><p>动态掩码。BERT 依赖随机掩码和预测 token。原版的 BERT 实现在数据预处理期间执行一次掩码，得到一个静态掩码。 而 RoBERTa<br>使用了动态掩码：每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。 </p></li><li><p>文本编码。不管是GPT还是Bert，都是用的BPE的编码方式，BPE是Byte-Pair Encoding的简称，是介于字符和词语之间的一个表达方式，比如hello，可能会被拆成“he”, “ll”, “o”, 其中BPE的字典是从语料中统计学习到的。是用以解决OOV（out of vocab）问题的算法。<br>原始Bert中，采用的BPE字典是30k， Roberta中增大到了50K，相对于Bertbase和Bertlarge会增加15M&#x2F;20M的参数。</p></li><li><p>大语料与更长的训练步数</p></li><li><p>Large-Batch：现在越来越多的实验表明增大batch_size会使得收敛更快，最后的效果更好。原始的Bert中，batch_size&#x3D;256，同时训练1M steps。<br>在Roberta中，实验了两个设置：<br>batch_size&#x3D;2k, 训练125k steps。<br>batch_size&#x3D;8k, 训练31k steps。<br>从结果中看，batch_size&#x3D;2k时结果最好。</p></li></ul><h2 id="为什么动态掩码比静态掩码更加好？"><a href="#为什么动态掩码比静态掩码更加好？" class="headerlink" title="为什么动态掩码比静态掩码更加好？"></a>为什么动态掩码比静态掩码更加好？</h2><p>原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p><p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N&#x2F;10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。</p><h2 id="batch-size对模型效果的影响"><a href="#batch-size对模型效果的影响" class="headerlink" title="batch_size对模型效果的影响"></a>batch_size对模型效果的影响</h2><ol><li><p>batch_size设的大一些，收敛得块，也就是需要训练的次数少，准确率上升的也很稳定，但是实际使用起来精度不高。</p></li><li><p>batch_size设的小一些，收敛得慢，可能准确率来回震荡，因此需要把基础学习速率降低一些，但是实际使用起来精度较高。</p></li></ol><p>结论：<br>batch_size太小导致网络收敛不稳定，最后结果比较差。而batch_size太大会影响随机性的引入</p><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/108883085?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.queryctrv2&spm=1001.2101.3001.4242.1&utm_relevant_index=3">Roberta: Bert调优</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/101610592">BERT与其他预训练模型</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;roberta三个训练改进：&quot;&gt;&lt;a href=&quot;#roberta三个训练改进：&quot; class=&quot;headerlink&quot; title=&quot;roberta三个训练改进：&quot;&gt;&lt;/a&gt;roberta三个训练改进：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;去掉下一句预测(NSP)任</summary>
      
    
    
    
    
    <category term="Roberta" scheme="https://www.songzj.com/tags/Roberta/"/>
    
  </entry>
  
  <entry>
    <title>Albert详解</title>
    <link href="https://www.songzj.com/posts/d661c569/"/>
    <id>https://www.songzj.com/posts/d661c569/</id>
    <published>2022-01-24T12:54:41.000Z</published>
    <updated>2022-01-24T12:54:41.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Bert超参数的分布"><a href="#Bert超参数的分布" class="headerlink" title="Bert超参数的分布"></a>Bert超参数的分布</h3><p>红色部分是embedding层，蓝色部分是encoder，注意蓝色部分总共有12个。</p><p>这两部分的参数分别是，其中每个encoder参数是0.85亿&#x2F;12&#x3D;700万<br><img src= "/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-a44344b6154cdeabe84908d5851ad42d_r.jpg"><br><img src= "/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-cd3028f90168598baa167681624d8dcd_r.jpg"></p><p>讲了BERT的参数分布，再说一下ALBERT对BERT的参数减少是从哪个方向。</p><p>一，12个encoder换成一个encoder，但是这个encoder会encode12次，这样encoder参数直接从84M变成7M，少了77M参数。</p><p>二，从embedding层参数最大的word embedding下手，原来word embedding的参数是</p><p>$$word_{num}*embedding_{dim}$$</p><p>记为$V<em>H$，这里用$H$因为encoder要保持向量为$H$的大小，下面进行参数减少改造，仅需通过一个低维空间$E$作为中转，由$V</em>H$变为$V<em>E+E</em>H$</p><p>比如语料库单词数为3W，$embedding_dim&#x3D;768$，没改造的参数量为$30000*768&#x3D;23M$</p><p>改造后假设$E$为128，$30000<em>128 + 128</em>768 &#x3D;4M$，少了19M参数。</p><p>由此ALBERT由BERT的110M减少96M变成14M参数。</p><h2 id="相比于BERT的改进"><a href="#相比于BERT的改进" class="headerlink" title="相比于BERT的改进"></a>相比于BERT的改进</h2><h3 id="参数缩减方法"><a href="#参数缩减方法" class="headerlink" title="参数缩减方法"></a>参数缩减方法</h3><p>论文提出了2种模型参数压缩的方法，即embedding矩阵分解法以及层参数共享法，具体如下：</p><ul><li><p>从模型角度来讲，wordPiece embedding是学习上下文独立的表征维度为$E$，而隐藏层embedding是 学习上下文相关的表征维度为$H$。为了应用的方便，原始的bert的向量维度$E&#x3D;H$，这样一旦增加了$H$，$E$也就增大了。 ALBert提出向量参数分解法，将一个非常大的词汇向量矩阵分解为两个小矩阵，例如词汇量大小是$V$，向量维度是$E$，隐藏层向量为$H$，则 原始词汇向量参数大小为$V<em>H$，ALBert想将原始embedding映射到$V</em>E$（低纬度的向量），然后映射到隐藏空间$H$，这样参数量从 $V<em>H$下降到 $V</em>E+E*H$，参数量大大下降。但是要注意这样做的损失确保矩阵分解后的两个小矩阵的乘积损失，是一个有损的操作。</p></li><li><p>层之间参数共享。base的bert总共由12层的transformer的encoder部分组成，层参数共享方法避免了随着深度的加深带来的参数量的增大。 具体的共享参数有这几种，attention参数共享、ffn残差网络参数共享。</p></li></ul><h4 id="对Embedding因式分解（Factorized-embedding-parameterization）"><a href="#对Embedding因式分解（Factorized-embedding-parameterization）" class="headerlink" title="对Embedding因式分解（Factorized embedding parameterization）"></a>对Embedding因式分解（Factorized embedding parameterization）</h4><p>在BERT中，词embedding与encoder输出的embedding维度是一样的都是768。但是ALBERT认为，词级别的embedding是没有上下文依赖的表述，而隐藏层的输出值不仅包括了词本生的意思还包括一些上下文信息，理论上来说隐藏层的表述包含的信息应该更多一些，因此应该让H&gt;&gt;E，所以ALBERT的词向量的维度是小于encoder输出值维度的。</p><p>在NLP任务中，通常词典都会很大，embedding matrix的大小是E×V，如果和BERT一样让H&#x3D;E，那么embedding matrix的参数量会很大，并且反向传播的过程中，更新的内容也比较稀疏。</p><p>结合上述说的两个点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，说白了就是先经过一个维度很低的embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从$O(V×H)O(V×H)O(V×H)O(V×H)O(V×E+E×H)$，当E&lt;&lt;H时参数量减少的很明显。</p><p>下图是E选择不同值的一个实验结果，尴尬的是，在不采用参数共享优化方案时E设置为768效果反而好一些，在采用了参数共享优化方案时E取128效果更好一些。</p><h4 id="跨层的参数共享（Cross-layer-parameter-sharing）"><a href="#跨层的参数共享（Cross-layer-parameter-sharing）" class="headerlink" title="跨层的参数共享（Cross-layer parameter sharing）"></a>跨层的参数共享（Cross-layer parameter sharing）</h4><p>在ALBERT还提出了一种参数共享的方法，Transformer中共享参数有多种方案，只共享全连接层，只共享attention层，ALBERT结合了上述两种方案，全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。</p><p>下图是BERT与ALBERT的一个对比，以base为例，BERT的参数是108M，而ALBERT仅有12M，但是效果的确相比BERT降低了两个点。由于其速度快的原因，我们再以BERT xlarge为参照标准其参数是1280M，假设其训练速度是1，ALBERT的xxlarge版本的训练速度是其1.2倍，并且参数也才223M，评判标准的平均值也达到了最高的88.7<br>除了上述说了训练速度快之外，ALBERT每一层的输出的embedding相比于BERT来说震荡幅度更小一些。下图是不同的层的输出值的L2距离与cosine相似度，可见参数共享其实是有稳定网络参数的作用的。<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191002155617541.png"></p><h3 id="句间连贯（Inter-sentence-coherence-loss）"><a href="#句间连贯（Inter-sentence-coherence-loss）" class="headerlink" title="句间连贯（Inter-sentence coherence loss）"></a>句间连贯（Inter-sentence coherence loss）</h3><p>我们知道原始的Bert预训练的loss由两个任务组成，maskLM和NSP(Next Sentence Prediction)，maskLM通过预测mask掉的词语来实现真正的双向transformer， NSP类似于语义匹配的任务，预测句子A和句子B是否匹配，是一个二分类的任务，其中正样本从原始语料获得，负样本随机负采样。NSP任务可以 提高下游任务的性能，比如句子对的关系预测。但是也有论文指出NSP任务其实可以去掉，反而可以提高性能，比如RoBert。</p><p>论文以为NSP任务相对于MLM任务太简单了，学习到的东西也有限，因此论文提出了一个新的loss，sentence-order prediction(SOP)， SOP关注于句子间的连贯性，而非句子间的匹配性。SOP正样本也是从原始语料中获得，负样本是原始语料的句子A和句子B交换顺序。 举个例子说明NSP和SOP的区别，原始语料句子 A和B， NSP任务正样本是 AB，负样本是AC；SOP任务正样本是AB，负样本是BA。 可以看出SOP任务更加难，学习到的东西更多了（句子内部排序），可以学到句子中的内部顺序。<br>SOP任务也很简单，它的正例和NSP任务一致（判断两句话是否有顺序关系），反例则是判断两句话是否为反序关系。</p><p>BERT的NSP任务实际上是一个二分类，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。该任务主要是希望能提高下游任务的效果，例如NLI自然语言推理任务。但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了，并且在MLM任务中其实也有类型的效果。</p><p>这里提一下为啥包含了主题预测，因为正样本是在同一个文档中选取的，负样本是在不同的文档选取的，假如我们有2个文档，一个是娱乐相关的，一个是新中国成立70周年相关的，那么负样本选择的内容就是不同的主题，而正样都在娱乐文档中选择的话预测出来的主题就是娱乐，在新中国成立70周年的文档中选择的话就是后者这个主题了。</p><p>在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP），SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务，该任务的添加给最终的结果提升了一个点。</p><h3 id="n-gram-masking"><a href="#n-gram-masking" class="headerlink" title="n-gram masking"></a>n-gram masking</h3><p>BERT 的mask language model是直接对字进行masking，ALBERT使用n-gram masking，这其实和后面有人改进word masking一样，对中文进行分词，对词的masking比对字的masking性能会有一定的提升，所以ALBERT使用n-gram masking，其中n取值为1-3。</p><h3 id="移除dropout"><a href="#移除dropout" class="headerlink" title="移除dropout"></a>移除dropout</h3><p>除了上面提到的三个主要优化点，ALBERT的作者还发现一个很有意思的点，ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在初闻ALBERT时，以为其减少了总的运算量，但实际上是通过参数共享的方式降低了内存，预测阶段还是需要和BERT一样的时间，如果采用了xxlarge版本的ALBERT，那实际上预测速度会更慢。</p><p>ALBERT解决的是训练时候的速度提升，如果要真的做到总体运算量的减少，的确是一个复杂且艰巨的任务，毕竟鱼与熊掌不可兼得。不过话说回来，ALBERT也更加适合采用feature base或者模型蒸馏等方式来提升最终效果。</p><p>ALBERT作者最后也简单提了下后续可能的优化方案，例如采用sparse attention或者block attention，这些方案的确是能真正降低运算量。其次，作者认为还有更多维度的特征需要去采用其他的自监督任务来捕获。</p><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><p><a href="https://zhuanlan.zhihu.com/p/242253766">BERT你关注不到的点</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/88152893">ALBert论文阅读笔记-缩减版的bert，模型参数更少，性能更好</a></p></li><li><p><a href="https://blog.csdn.net/kyle1314608/article/details/106546529">albert速度</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/102470776">ALBERT真的瘦身成功了吗</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Bert超参数的分布&quot;&gt;&lt;a href=&quot;#Bert超参数的分布&quot; class=&quot;headerlink&quot; title=&quot;Bert超参数的分布&quot;&gt;&lt;/a&gt;Bert超参数的分布&lt;/h3&gt;&lt;p&gt;红色部分是embedding层，蓝色部分是encoder，注意蓝色部分总共有</summary>
      
    
    
    
    
    <category term="Albert" scheme="https://www.songzj.com/tags/Albert/"/>
    
  </entry>
  
  <entry>
    <title>Bert详解</title>
    <link href="https://www.songzj.com/posts/f06b97c0/"/>
    <id>https://www.songzj.com/posts/f06b97c0/</id>
    <published>2022-01-24T12:50:06.000Z</published>
    <updated>2022-01-24T12:50:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Bert含义"><a href="#Bert含义" class="headerlink" title="Bert含义"></a>Bert含义</h2><p>BERT模型的全称是：BidirectionalEncoder Representations from Transformer。双向Transformer编码表达，其中双向指的是attention矩阵中，每个字都包含前后所有字的信息。</p><p>BERT模型的目标是利用大规模无标注语料训练、获得文本的包含丰富语义信息的Representation，即：文本的语义表示，然后将文本的语义表示在特定NLP任务中作微调，最终应用于该NLP任务。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Bert依然是依赖Transformer模型结构，我们知道GPT采用的是Transformer中的Decoder部分的模型结构，当前位置只能attend到之前的位置。而Bert中则没有这样的限制，因此它是用的Transformer的Encoder部分。</p><p>而Transformer是由一个一个的block组成的，其主要参数如下：</p><p>L: 多少个block<br>H: 隐含状态尺寸，不同block上的隐含状态尺寸一般相等，这个尺寸单指多头注意力层的尺寸，有一个惯例就是在Transformer Block中全连接层的尺寸是多头注意力层的4倍。所以指定了H相当于是把Transformer Block里的两层隐含状态尺寸都指定了。<br>A: 多头注意力的头的个数<br>有了这几个参数后，就可以定义不同配置的模型了，Bert中定义了两个模型，<br>BertBase和BertLarge。其中：</p><p>BertBase: L&#x3D;12, H&#x3D;768, A&#x3D;12, 参数量110M。<br>BertLarge: L&#x3D;24, H&#x3D;1024, A&#x3D;16, 参数量340M。</p><p>输入输出<br>为了让Bert能够处理下游任务，Bert的输入是两个句子，中间用分隔符分开，在开头加一个特殊的用于分类的字符。即Bert的输入是: [CLS] sentence1 [SEP] sentence2。</p><p>其中，两个句子对应的词语对应的embedding还要加上位置embedding和标明token属于哪个句子的embedding。如下图所示：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_embedding.jpg"><br>在[CLS]上的输出我们认为是输入句子的编码。<br>输入最长是512。</p><p>position embedding的结构为<code>[max sequence leghth, embedding dimension]</code><br>论文给出如下公示求解位置信息：</p><ul><li>$PE(pos,2i)&#x3D;sin(pos&#x2F;10000^{2i&#x2F;d})$</li><li>$PE(pos,2i+1)&#x3D;cos(pos&#x2F;10000^{2i&#x2F;d})$</li></ul><p>这里的pos是字的位置，取值为<code>[0, max sequence length]</code>，i是维度的位置，设embedding dimension &#x3D; 256，则$i \in [0, 255)$。d为维度的总数，即256。</p><h2 id="Bert的训练数据生成和解读"><a href="#Bert的训练数据生成和解读" class="headerlink" title="Bert的训练数据生成和解读"></a>Bert的训练数据生成和解读</h2><p>用于训练的文本材料是以行排列的句子。</p><p>首先读取一行句子，以：“工时填写。”为例，该句子会被认为是一个document和一个chunk，认定只有一个句子后，会随机从其他行的句子中挑一个出来，与该句组合成如下的结构：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[‘[CLS]’, ‘工’, ‘时’, ‘填’, ‘写’, ‘[SEP]’, ‘平’, ‘安’, ‘好’, ‘医’, ‘生’, ‘非, ‘常’, ‘优’, ‘秀’, ‘[SEP]’]</span><br></pre></td></tr></table></figure><p>当然，在组合数据的过程中，会随机有如下的调整：</p><p>在mask的个数范围之内，此处举例有3个mask，其中有80%的概率某字会变成’[MASK]’，有10%的概率不会变化，有10%的概率会随机用另一个字代替它，接下来可能会变化成如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[‘[CLS]’, ‘工’, ‘时’, ‘[MASK]’, ‘写’, ‘[SEP]’, ‘平’, ‘安’, ‘好’, ‘特’, ‘生’, ‘非, ‘常’, ‘优’, ‘秀’, ‘[SEP]’]。</span><br></pre></td></tr></table></figure><p>同时我们也会得到相应的mask信息：</p><ul><li><p>segment_ids：这是一个list，用于区分句子之间的覆盖的范围，对应上述内容，其值为[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]；</p></li><li><p>is_random_next：这是一个重点，它表示两句话之间是否有上下文关系，注意，这里的random，这表示该变量如果是True，则两句没有上下文关系，这是一个随机生成的句子对。当每个document中只有一个句子时一定会随机找一个对子与之配对，如果是两个有关的句子，则有50%的概率随机找一个句子与前一个句子配对；</p></li><li><p>masked_lm_positions：这是一个list，表示本句中出现掩码的位置，其值为[3, 6, 9]；</p></li><li><p>masked_lm_labels：这是一个list，是与position对应的字，其值为[‘填’, ‘平’, ‘医’]；</p></li></ul><p>上述所有信息会封装在一个instance对象中。</p><h2 id="Bert的输入输出"><a href="#Bert的输入输出" class="headerlink" title="Bert的输入输出"></a>Bert的输入输出</h2><p>在基于深度神经网络的NLP方法中，文本中的字&#x2F;词通常都用一维向量来表示（一般称之为“词向量”）；在此基础上，神经网络会将文本中各个字或词的一维词向量作为输入，经过一系列复杂的转换后，输出一个一维词向量作为文本的语义表示。特别地，我们通常希望语义相近的字&#x2F;词在特征向量空间上的距离也比较接近，如此一来，由字&#x2F;词向量转换而来的文本向量也能够包含更为准确的语义信息。因此，BERT模型的主要输入是文本中各个字&#x2F;词的原始词向量，该向量既可以随机初始化，也可以利用Word2Vector等算法进行预训练以作为初始值；输出是文本中各个字&#x2F;词融合了全文语义信息后的向量表示，如下图所示<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_input.png"><br>从上图中可以看出，BERT模型通过查询字向量表将文本中的每个字转换为一维向量，作为模型输入；模型输出则是输入各字对应的融合全文语义信息后的向量表示。此外，模型输入除了字向量，还包含另外两个部分：</p><ol><li><p>文本向量：该向量的取值在模型训练过程中自动学习，用于刻画文本的全局语义信息，并与单字&#x2F;词的语义信息相融合</p></li><li><p>位置向量：由于出现在文本不同位置的字&#x2F;词所携带的语义信息存在差异（比如：“我爱你”和“你爱我”），因此，BERT模型对不同位置的字&#x2F;词分别附加一个不同的向量以作区分</p></li></ol><p>最后，BERT模型将字向量、文本向量和位置向量的加和作为模型输入。特别地，在目前的BERT模型中，文章作者还将英文词汇作进一步切割，划分为更细粒度的语义单位（WordPiece），例如：将playing分割为play和ing；此外，对于中文，目前作者尚未对输入文本进行分词，而是直接将单字作为构成文本的基本单位。</p><p>对于不同的NLP任务，模型输入会有微调，对模型输出的利用也有差异，例如：</p><p>单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：与文本中已有的其它字&#x2F;词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字&#x2F;词的语义信息。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_cls.png"><br>语句对分类任务：该任务的实际应用场景包括：问答（判断一个问题与一个答案是否匹配）、语句匹配（两句话是否表达同一个意思）等。对于该任务，BERT模型除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分，如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_pair.png"></p><p>序列标注任务：该任务的实际应用场景包括：中文分词&amp;新词发现（标注每个字是词的首字、中间字或末字）、答案抽取（答案的起止位置）等。对于该任务，BERT模型利用文本中每个字对应的输出向量对该字进行标注（分类），如下图所示(B、I、E分别表示一个词的第一个字、中间字和最后一个字)。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_seq_label.png"></p><p>根据具体任务的不同，在实际应用中我们可以脑洞大开，通过调整模型的输入、输出将模型适配到真实业务场景中。</p><h2 id="Bert的预训练任务"><a href="#Bert的预训练任务" class="headerlink" title="Bert的预训练任务"></a>Bert的预训练任务</h2><p>BERT实际上是一个语言模型。语言模型通常采用大规模、与特定NLP任务无关的文本语料进行训练，其目标是学习语言本身应该是什么样的，这就好比我们学习语文、英语等语言课程时，都需要学习如何选择并组合我们已经掌握的词汇来生成一篇通顺的文本。回到BERT模型上，其预训练过程就是逐渐调整模型参数，使得模型输出的文本语义表示能够刻画语言的本质，便于后续针对具体NLP任务作微调。为了达到这个目的，BERT文章作者提出了两个预训练任务：Masked LM和Next Sentence Prediction。</p><h3 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h3><p>一般语言模型建模的方式是从左到右或者从右到左，这样的损失函数都很直观，即预测下一个词的概率。</p><p>而Bert这种双向的网络，使得下一个词这个概念消失了，没有了目标，如何做训练呢？</p><p>答案就是完形填空，在输入中，把一些词语遮挡住，遮挡的方法就是用[Mask]这个特殊词语代替。而在预测的时候，就预测这些被遮挡住的词语。其中遮挡词语占所有词语的15%，且是每次随机Mask。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_masked_lm.png"></p><p>但这有一个问题：在预训练中会[Mask]这个词语，但是在下游任务中，是没有这个词语的，这会导致预训练和下游任务的不匹配。</p><blockquote><p>不匹配的意思我理解就是在预训练阶段任务中，模型会学到句子中有被遮挡的词语，模型要去学习它，而在下游任务中没有，但是模型会按照预训练的习惯去做，会导致任务的不匹配。</p></blockquote><p>解决的办法就是不让模型意识到有这个任务的存在，具体做法就是在所有Mask的词语中，有80%的词语继续用[Mask]特殊词语，有10%用其他词语随机替换，有10%的概率保持不变。这样，模型就不知道当前句子中有没[Mask]的词语了。</p><p>这么做的主要原因是：</p><ul><li>在后续微调任务中语句中并不会出现[MASK]标记；</li><li>这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（10%概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。</li></ul><p>我们知道80%[mask]是为了让bert获得双向语言的语义表征，但是为什么还有另外20%的情况被另外处理，为啥？<br>原论文说如果100%mask会导致training见不到某些token而对fine-tuning阶段也有影响，所以肯定不能全都做mask。</p><p>为什么有10%保持不变，就是为了解决我们前面那个问题，让模型可以看到这些词，从而降低出现没看过情况。</p><p>为什么又10%用随机token，因为如果不用随机token替换一下，模型很可能已经记住那个位置应该填什么，使得模型的词汇多样性减少了，比如本来可以填写水果一类的词，但是只填apple。</p><h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p>当年大学考英语四六级的时候，大家应该都做过段落重排序，即：将一篇文章的各段打乱，让我们通过重新排序把原文还原出来，这其实需要我们对全文大意有充分、准确的理解。Next Sentence Prediction任务实际上就是段落重排序的简化版：只考虑两句话，判断是否是一篇文章中的前后句。在实际预训练过程中，文章作者从文本语料库中随机选择50%正确语句对和50%错误语句对进行训练，与Masked LM任务相结合，让模型能够更准确地刻画语句乃至篇章层面的语义信息。<br>在很多下游任务中，需要判断两个句子之间的关系，比如QA问题，需要判断一个句子是不是另一个句子的答案，比如NLI(Natural Language Inference)问题，直接就是两个句子之间的三种关系判断。</p><p>因此，为了能更好的捕捉句子之间的关系，在预训练的时候，就做了一个句子级别的损失函数，这个损失函数的目的很简单，就是判断第二个句子是不是第一个句子的下一句。训练时，会随机选择生成训练语料，50%的时下一句，50%的不是。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>BERT模型通过对Masked LM任务和Next Sentence Prediction任务进行联合训练，使模型输出的每个字&#x2F;词的向量表示都能尽可能全面、准确地刻画输入文本（单句或语句对）的整体信息，为后续的微调任务提供更好的模型参数初始值。</p><h2 id="Bert模型结构"><a href="#Bert模型结构" class="headerlink" title="Bert模型结构"></a>Bert模型结构</h2><p>BERT模型的全称是：BidirectionalEncoder Representations from Transformer，也就是说，Transformer是组成BERT的核心模块，而Attention机制又是Transformer中最关键的部分，因此，下面我们从Attention机制开始，介绍如何利用Attention机制构建Transformer模块，在此基础上，用多层Transformer组装BERT模型。</p><h3 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h3><p>Attention: Attention机制的中文名叫“注意力机制”，顾名思义，它的主要作用是让神经网络把“注意力”放在一部分输入上，即：区分输入的不同部分对输出的影响。这里，我们从增强字&#x2F;词的语义表示这一角度来理解一下Attention机制。</p><p>我们知道，一个字&#x2F;词在一篇文本中表达的意思通常与它的上下文有关。比如：光看“鹄”字，我们可能会觉得很陌生（甚至连读音是什么都不记得吧），而看到它的上下文“鸿鹄之志”后，就对它立马熟悉了起来。因此，字&#x2F;词的上下文信息有助于增强其语义表示。同时，上下文中的不同字&#x2F;词对增强语义表示所起的作用往往不同。比如在上面这个例子中，“鸿”字对理解“鹄”字的作用最大，而“之”字的作用则相对较小。为了有区分地利用上下文字信息增强目标字的语义表示，就可以用到Attention机制。</p><p>Attention机制主要涉及到三个概念：Query、Key和Value。在上面增强字的语义表示这个应用场景中，目标字及其上下文的字都有各自的原始Value，Attention机制将目标字作为Query、其上下文的各个字作为Key，并将Query与各个Key的相似性作为权重，把上下文各个字的Value融入目标字的原始Value中。如下图所示，Attention机制将目标字和上下文各个字的语义向量表示作为输入，首先通过线性变换获得目标字的Query向量表示、上下文各个字的Key向量表示以及目标字与上下文各个字的原始Value表示，然后计算Query向量与各个Key向量的相似度作为权重，加权融合目标字的Value向量和各个上下文字的Value向量，作为Attention的输出，即：目标字的增强语义向量表示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_attention.png"></p><p>Self-Attention:对于输入文本，我们需要对其中的每个字分别增强语义向量表示，因此，我们分别将每个字作为Query，加权融合文本中所有字的语义信息，得到各个字的增强语义向量，如下图所示。在这种情况下，Query、Key和Value的向量表示均来自于同一输入文本，因此，该Attention机制也叫Self-Attention。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert_self_attention.png"></p><p>Multi-head Self-Attention:为了增强Attention的多样性，文章作者进一步利用不同的Self-Attention模块获得文本中每个字在不同语义空间下的增强语义向量，并将每个字的多个增强语义向量进行线性组合，从而获得一个最终的与原始字向量长度相同的增强语义向量，如下图所示。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bet_multi_head_attention.png"></p><p>这里，我们再给出一个例子来帮助理解Multi-head Self-Attention（注：这个例子仅用于帮助理解，并非严格正确）。看下面这句话：“南京市长江大桥”，在不同语义场景下对这句话可以有不同的理解：“南京市&#x2F;长江大桥”，或“南京市长&#x2F;江大桥”。对于这句话中的“长”字，在前一种语义场景下需要和“江”字组合才能形成一个正确的语义单元；而在后一种语义场景下，它则需要和“市”字组合才能形成一个正确的语义单元。我们前面提到，Self-Attention旨在用文本中的其它字来增强目标字的语义表示。在不同的语义场景下，Attention所重点关注的字应有所不同。因此，Multi-head Self-Attention可以理解为考虑多种语义场景下目标字与文本中其它字的语义向量的不同融合方式。可以看到，Multi-head Self-Attention的输入和输出在形式上完全相同，输入为文本中各个字的原始向量表示，输出为各个字融合了全文语义信息后的增强向量表示。因此，Multi-head Self-Attention可以看作是对文本中每个字分别增强其语义向量表示的黑盒。</p><h3 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h3><p>在Multi-head Self-Attention的基础上再添加一些“佐料”，就构成了大名鼎鼎的Transformer Encoder。实际上，Transformer模型还包含一个Decoder模块用于生成文本，但由于BERT模型中并未使用到Decoder模块，因此这里对其不作详述。下图展示了Transformer Encoder的内部结构，可以看到，Transformer Encoder在Multi-head Self-Attention之上又添加了三种关键操作：</p><ul><li>残差连接（ResidualConnection）：将模块的输入与输出直接相加，作为最后的输出。这种操作背后的一个基本考虑是：修改输入比重构整个输出更容易（“锦上添花”比“雪中送炭”容易多了！）。这样一来，可以使网络更容易训练。</li><li>Layer Normalization：对某一层神经网络节点作0均值1方差的标准化。</li><li>线性转换：对每个字的增强语义向量再做两次线性变换，以增强整个模型的表达能力。这里，变换后的向量与原向量保持长度相同。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/transformer_encoder.png"></li></ul><p>可以看到，Transformer Encoder的输入和输出在形式上还是完全相同，因此，Transformer Encoder同样可以表示为将输入文本中各个字的语义向量转换为相同长度的增强语义向量的一个黑盒。</p><h3 id="BERT-Model"><a href="#BERT-Model" class="headerlink" title="BERT Model"></a>BERT Model</h3><p>组装好TransformerEncoder之后，再把多个Transformer Encoder一层一层地堆叠起来，BERT模型就大功告成了！<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/bert.png"></p><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><p><a href="https://zhuanlan.zhihu.com/p/157806409">Bert的训练数据生成和解读</a></p></li><li><p><a href="https://blog.csdn.net/BmwGaara/article/details/107557205?spm=1001.2101.3001.6650.5&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-5.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-5.pc_relevant_aa&utm_relevant_index=8">原生Bert的训练和使用总结</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/364142934">神经网络权重初始化为0</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/242253766">BERT你关注不到的点</a></p></li><li><p><a href="https://www.cnblogs.com/huangyc/p/10223075.html">一文读懂BERT中的WordPiece</a></p></li><li><p><a href="https://blog.csdn.net/gg7894125/article/details/106884858/">超详细BERT介绍（一）BERT主模型的结构及其组件</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/103226488">BERT 详解</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/225180249?utm_source=wechat_session">BERT详解：BERT一下，你就知道</a></p></li><li><p><a href="https://blog.csdn.net/jiaowoshouzi/article/details/89073944">一文读懂BERT(原理篇)</a></p></li><li><p><a href="https://www.cnblogs.com/kouin/p/13427243.html">BERT详解--慢慢来</a></p></li><li><p><a href="https://www.jiqizhixin.com/articles/2018-12-03">谷歌BERT模型深度解析</a></p></li><li><p><a href="https://baijiahao.baidu.com/s?id=1651912822853865814&wfr=spider&for=pc">彻底理解 Google BERT 模型</a></p></li><li><p><a href="http://fancyerii.github.io/2019/03/09/bert-theory/">BERT模型详解</a></p></li><li><p><a href="https://cloud.tencent.com/developer/article/1389555">图解BERT模型：从零开始构建BERT</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Bert含义&quot;&gt;&lt;a href=&quot;#Bert含义&quot; class=&quot;headerlink&quot; title=&quot;Bert含义&quot;&gt;&lt;/a&gt;Bert含义&lt;/h2&gt;&lt;p&gt;BERT模型的全称是：BidirectionalEncoder Representations from T</summary>
      
    
    
    
    
    <category term="Bert" scheme="https://www.songzj.com/tags/Bert/"/>
    
  </entry>
  
  <entry>
    <title>Transformer详解</title>
    <link href="https://www.songzj.com/posts/c2ac91e1/"/>
    <id>https://www.songzj.com/posts/c2ac91e1/</id>
    <published>2022-01-24T12:40:46.000Z</published>
    <updated>2022-01-24T12:40:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/transformer.jpg"></p><p>左边处理源语言，称之为Encoder，右边处理目标语言，被称为Decoder，分别由N个Block组成。然后每个block都有这么几个模块：</p><ul><li>Multi-Head Attention</li><li>Masked Multi-Head Attention</li><li>Add &amp; Norm</li><li>Feed Forward</li><li>Positional Encoding</li><li>Linear</li></ul><p>其中， Feed Forward和Linear是神经网络的基本操作全连接层，Add &amp; Norm以及延伸出来的一条侧边也是一个常见的神经网络结构残差连接</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>attention说白了就是权重计算和加权求和。图上的循环神经网络中的每一步都会输出一个向量，在预测目标语言到某一步时，用当前步的向量去和源语言中的每一步的向量去做内积，然后经过softmax得到归一化后的权重，再用权重去把源语言上的每一步的向量去做加权平均。然后做预测的时候也作为输入进入全连接层</p><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/transformer_multihead_attention.jpg"></p><p>Multi-Head Attention是由多个Scaled Dot-Product Attention的函数组合而成的。<br>Scaled Dot-Product Attention的计算公式如下：</p><!--![](https://img-blog.csdnimg.cn/20200829000047826.png#pic_center)--><p>$$Attention(Q,K,V)&#x3D;softmax(\frac {QK^{T}}{\sqrt{d_k}})V$$</p><p>Q、K、V的维度均为<code>[batch_size, seq_len, emb_dim]</code></p><h3 id="为什么要除以-sqrt-d-k"><a href="#为什么要除以-sqrt-d-k" class="headerlink" title="为什么要除以$\sqrt{d_k}$"></a>为什么要除以$\sqrt{d_k}$</h3><p>首先计算q和k的点乘，然后除以 $\sqrt{d_k}$，经过softmax得到V上的权重分布，最后通过点乘计算V的加权值。这里$d_k$是K的维度，除以$\sqrt{d_k}$的原因是Q与K的转置相乘了，值会变大</p><p>首先我们看下原论文的解释：We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients . To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$</p><p>原文说，他们怀疑当key的维度过大的时候去做点乘值会变得很大，导致softmax函数的梯度异常的小，导致于无法快速更新参数。</p><p>我们也可以直观的来讲：</p><p>假设我们输入一个token长度为2的句子，key的维度为4，然后我们现在有query和key的在两个位置的点积，假设两个点积为<code>[4,9]</code>，我们softmax可以得到<code>[0.0067, 0.9933]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">4</span>,<span class="number">9</span>],dtype=torch.float64)</span><br><span class="line">a.softmax(-<span class="number">1</span>)</span><br><span class="line">output；tensor([<span class="number">0.0067</span>, <span class="number">0.9933</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>softmax后权重直接是99%了，如果我们除4后[1,2.25]，权重变成<code>[0.2227, 0.7773]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2.25</span>],dtype=torch.float64)</span><br><span class="line">a.softmax(-<span class="number">1</span>)</span><br><span class="line">output；tensor([<span class="number">0.2227</span>, <span class="number">0.7773</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>这样的话attention就不会只关注一个词汇，而是可以看到整句话的面貌分布。</p><h3 id="对Q、K、V的理解"><a href="#对Q、K、V的理解" class="headerlink" title="对Q、K、V的理解"></a>对Q、K、V的理解</h3><p>通俗的理解：</p><ul><li>Q：query要去查的</li><li>K： 等着被查的</li><li>V： 实际的特征信息</li></ul><p>在这里提两个问题：</p><p>问题1：Q,K,V是怎么来的？</p><p>答：输入的每个词经过Input Embedding后会变成一个向量，然后这个向量会分别经过一个矩阵变换得到Q,K,V。</p><p>问题2：Q,K,V分别的含义是什么?</p><p>答：Q代表Query，K代表Key，V代表Value。相对于上面的seq2seq+attention模型，这里做了一个attention概念上的拆分，Q和K去计算权重，V去和权重做加权平均。而在seq2seq+attention中相当于Q,K,V是一个向量。</p><h3 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h3><p>之所以需要mask，是因为在目标语言上，每一步是预测下一个词，所以在预测下一个词的时候不能让模型看到下一个词以及之后的信息，所以在处理目标语言的时候需要对attention做mask，attention本来是一个二维矩阵，即每个位置对每个位置的权重，做了mask后就相当于强制一半的值（二维矩阵的右上三角）为0。这就是mask的含义。</p><h3 id="全连接层的作用"><a href="#全连接层的作用" class="headerlink" title="全连接层的作用"></a>全连接层的作用</h3><p>全连接层不仅可以变换维度，更重要的是如果没有全连接层，他们模型只会有self-attention层出来的一些线性组合，表达能力有限，而全连接层可以自己学习复杂的特征表达，并且激活函数能提供非线性。</p><h2 id="Encoder和Decoder"><a href="#Encoder和Decoder" class="headerlink" title="Encoder和Decoder"></a>Encoder和Decoder</h2><p>问：Encoder和Decoder是怎么联系的呢？ </p><p>答：Decoder的每一个block比Encoder的block多一个Multi-Head Attention，这个的输入的K,V是Encoder这边的输出，由此，构建了Encoder和Decoder之间的信息传递。</p><p><img src= "/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-8a9556f27d87e89a54de402744d1fcbf_r.jpg"></p><h2 id="Transformer-的结构"><a href="#Transformer-的结构" class="headerlink" title="Transformer 的结构"></a>Transformer 的结构</h2><p>Transformer 本身是一个典型的 encoder-decoder 模型</p><h3 id="模型结构总览"><a href="#模型结构总览" class="headerlink" title="模型结构总览"></a>模型结构总览</h3><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/transformer_model.jpg"></p><h4 id="Encoder端"><a href="#Encoder端" class="headerlink" title="Encoder端"></a>Encoder端</h4><p>Encoder 端由 N(原论文中「N&#x3D;6」)个相同的大模块堆叠而成，其中每个大模块又由「两个子模块」构成，这两个子模块分别为多头 self-attention 模块，以及一个前馈神经网络模块；</p><blockquote><p>需要注意的是，Encoder 端每个大模块接收的输入是不一样的，第一个大模块(最底下的那个)接收的输入是输入序列的 embedding(embedding 可以通过 word2vec 预训练得来)，其余大模块接收的是其前一个大模块的输出，最后一个模块的输出作为整个 Encoder 端的输出。</p></blockquote><h4 id="Decoder-端"><a href="#Decoder-端" class="headerlink" title="Decoder 端"></a>Decoder 端</h4><p>Decoder 端同样由 N(原论文中「N&#x3D;6」)个相同的大模块堆叠而成，其中每个大模块则由「三个子模块」构成，这三个子模块分别为多头 self-attention 模块，「多头 Encoder-Decoder attention 交互模块」，以及一个前馈神经网络模块；</p><blockquote><ul><li>同样需要注意的是，Decoder端每个大模块接收的输入也是不一样的，其中第一个大模块(最底下的那个)训练时和测试时的接收的输入是不一样的，并且每次训练时接收的输入也可能是不一样的(也就是模型总览图示中的&quot;shifted right&quot;，后续会解释)，其余大模块接收的是同样是其前一个大模块的输出，最后一个模块的输出作为整个Decoder端的输出</li><li>对于第一个大模块，简而言之，其训练及测试时接收的输入为：<ul><li>训练的时候每次的输入为上次的输入加上输入序列向后移一位的 ground truth(例如每向后移一位就是一个新的单词，那么则加上其对应的 embedding)，特别地，当 decoder 的 time step 为 1 时(也就是第一次接收输入)，其输入为一个特殊的 token，可能是目标序列开始的 token(如)，也可能是源序列结尾的 token(如)，也可能是其它视任务而定的输入等等，不同源码中可能有微小的差异，其目标则是预测下一个位置的单词(token)是什么，对应到 time step 为 1 时，则是预测目标序列的第一个单词(token)是什么，以此类推；<ul><li>这里需要注意的是，在实际实现中可能不会这样每次动态的输入，而是一次性把目标序列的embedding通通输入第一个大模块中，然后在多头attention模块对序列进行mask即可</li></ul></li><li>而在测试的时候，是先生成第一个位置的输出，然后有了这个之后，第二次预测时，再将其加入输入序列，以此类推直至预测结束。</li></ul></li></ul></blockquote><h3 id="Encoder-端各个子模块"><a href="#Encoder-端各个子模块" class="headerlink" title="Encoder 端各个子模块"></a>Encoder 端各个子模块</h3><p>所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/encoder.png"></p><h4 id="多头-self-attention-模块"><a href="#多头-self-attention-模块" class="headerlink" title="多头 self-attention 模块"></a>多头 self-attention 模块</h4><p>在介绍Encoder模块之前，先介绍 self-attention 模块，图示如下：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/self_attention.png"></p><p>上述 attention 可以被描述为「将 query 和 key-value 键值对的一组集合映射到输出」，其中 query，keys，values 和输出都是向量，其中 query 和 keys 的维度均为$d_k$，values 的维度为$d_v$(论文中$d_k&#x3D;d_v&#x3D;d_{model}&#x2F;h&#x3D;64$)，输出被计算为 values 的加权和，其中分配给每个 value 的权重由 query 与对应 key 的相似性函数计算得来。这种 attention 的形式被称为“Scaled Dot-Product Attention”，对应到公式的形式为：</p><p>$$Attention(Q,K,V)&#x3D;softmax(\frac {QK^{T}}{\sqrt{d_k}})V$$</p><p>而多头 self-attention 模块，则是将$Q、K、V$通过参数矩阵映射后(给$Q、K、V$分别接一个全连接层)，然后再做 self-attention，将这个过程重复$h$(原论文中$h&#x3D;8$)次，最后再将所有的结果拼接起来，再送入一个全连接层即可，图示如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/multi_head_attention.png"><br>对应到公式的形式为：</p><p>$$MultiHead(Q,K,V)&#x3D;Concat(head_1,head_2,\ldots,head_h)W^O$$</p><p>$$where \ head_i &#x3D; Attention(QW_i^Q,KW_i^K,VW_i^V)$$</p><p>其中$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}，W_i^K \in \mathbb{R}^{d_{model} \times d_k}，W_i^V \in \mathbb{R}^{d_{model} \times d_v}，W_i^O \in \mathbb{R}^{hd_{v} \times d_{model}}，$</p><h4 id="前馈神经网络模块"><a href="#前馈神经网络模块" class="headerlink" title="前馈神经网络模块"></a>前馈神经网络模块</h4><p>前馈神经网络模块(即图示中的 Feed Forward)由两个线性变换组成，中间有一个 ReLU 激活函数，对应到公式的形式为：</p><p>$$FFN(x)&#x3D;max(0,xW_1 + b_1)W_2 + b_2$$</p><p>论文中前馈神经网络模块输入和输出的维度均为$d_{model}&#x3D;512$，其内层的维度$d_{ff}&#x3D;2048$</p><h3 id="Decoder-端各个子模块"><a href="#Decoder-端各个子模块" class="headerlink" title="Decoder 端各个子模块"></a>Decoder 端各个子模块</h3><h4 id="多头-self-attention-模块-1"><a href="#多头-self-attention-模块-1" class="headerlink" title="多头 self-attention 模块"></a>多头 self-attention 模块</h4><p>Decoder 端多头 self-attention 模块与 Encoder 端的一致，但是<strong>需要注意的是 Decoder 端的多头 self-attention 需要做 mask，因为它在预测时，是“看不到未来的序列的”，所以要将当前预测的单词(token)及其之后的单词(token)全部 mask 掉。</strong></p><h4 id="多头-Encoder-Decoder-attention-交互模块"><a href="#多头-Encoder-Decoder-attention-交互模块" class="headerlink" title="多头 Encoder-Decoder attention 交互模块"></a>多头 Encoder-Decoder attention 交互模块</h4><p>多头 Encoder-Decoder attention 交互模块的形式与多头 self-attention 模块一致，唯一不同的是其$Q,K,V$矩阵的来源，其$Q$矩阵来源于下面子模块的输出(对应到图中即为 masked 多头 self-attention 模块经过 Add &amp; Norm 后的输出)，而$K,V$矩阵则来源于整个 Encoder 端的输出，仔细想想其实可以发现，这里的交互模块就跟 seq2seq with attention 中的机制一样，目的就在于让 Decoder 端的单词(token)给予 Encoder 端对应的单词(token)“更多的关注(attention weight)”</p><h4 id="前馈神经网络模块-1"><a href="#前馈神经网络模块-1" class="headerlink" title="前馈神经网络模块"></a>前馈神经网络模块</h4><p>该部分与 Encoder 端的一致</p><h3 id="其他模块"><a href="#其他模块" class="headerlink" title="其他模块"></a>其他模块</h3><h4 id="Add-amp-Norm-模块"><a href="#Add-amp-Norm-模块" class="headerlink" title="Add &amp; Norm 模块"></a>Add &amp; Norm 模块</h4><p>Add &amp; Norm 模块接在 Encoder 端和 Decoder 端每个子模块的后面，其中 Add 表示残差连接，Norm 表示 LayerNorm，残差连接来源于论文Deep Residual Learning for Image Recognition[1]，LayerNorm 来源于论文Layer Normalization[2]，因此 Encoder 端和 Decoder 端每个子模块实际的输出为：$LayerNorm(x + Sublayer(x))$，其中$Sublayer(x)$为子模块的输出。</p><h4 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h4><p>Positional Encoding 添加到 Encoder 端和 Decoder 端最底部的输入 embedding。Positional Encoding 具有与 embedding 相同的维度$d_{model}$<br>具体做法是使用不同频率的正弦和余弦函数，公式如下：</p><ul><li>$PE(pos,2i)&#x3D;sin(pos&#x2F;10000^{2i&#x2F;d_{model}})$</li><li>$PE(pos,2i+1)&#x3D;cos(pos&#x2F;10000^{2i&#x2F;d_{model}})$</li></ul><p>其中$pos$为位置，$i$为维度，之所以选择这个函数，是因为任意位置$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数，这个主要是三角函数的特性：</p><p>$$sin(α  + \beta) &#x3D; sin(α)cos(\beta) + cos(α)sin(\beta)$$</p><p>$$cos(α  + \beta) &#x3D; cos(α)cos(\beta) - sin(α)sin(\beta)$$</p><p>需要注意的是，Transformer 中的 Positional Encoding 不是通过网络学习得来的，而是直接通过上述公式计算而来的，论文中也实验了利用网络学习 Positional Encoding，发现结果与上述基本一致，但是论文中选择了正弦和余弦函数版本，<strong>因为三角公式不受序列长度的限制，也就是可以对比所遇到序列的更长的序列进行表示。</strong></p><h2 id="Transformer相关问题"><a href="#Transformer相关问题" class="headerlink" title="Transformer相关问题"></a>Transformer相关问题</h2><h3 id="self-attention是什么？"><a href="#self-attention是什么？" class="headerlink" title="self-attention是什么？"></a>self-attention是什么？</h3><p>self-attention，也叫 intra-attention，是一种通过自身和自身相关联的 attention 机制，从而得到一个更好的 representation 来表达自身，self-attention 可以看成一般 attention 的一种特殊情况。在 self-attention 中，$Q&#x3D;K&#x3D;V$，序列中的每个单词(token)和该序列中其余单词(token)进行 attention 计算。self-attention 的特点在于<strong>无视词(token)之间的距离直接计算依赖关系，从而能够学习到序列的内部结构</strong>，实现起来也比较简单，值得注意的是，在后续一些论文中，self-attention 可以当成一个层和 RNN，CNN 等配合使用，并且成功应用到其他 NLP 任务。</p><p>下例子可以大概探知 self-attention 的效果：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/self_attention1.png"><br>从图中可以看出，self-attention 可以捕获同一个句子中单词之间的一些句法特征或者语义特征。</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/self_attention2.png"></p><p>很明显，引入 Self Attention 后会更容易捕获句子中长距离的相互依赖的特征，因为如果是 RNN 或者 LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p><p>但是 Self Attention 在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention 对于增加计算的并行性也有直接帮助作用。这是为何 Self Attention 逐渐被广泛使用的主要原因。</p><h3 id="self-attention-为什么要使用-Q、K、V，仅仅使用-Q、V-x2F-K、V-或者-V-为什么不行？"><a href="#self-attention-为什么要使用-Q、K、V，仅仅使用-Q、V-x2F-K、V-或者-V-为什么不行？" class="headerlink" title="self-attention 为什么要使用 Q、K、V，仅仅使用 Q、V&#x2F;K、V 或者 V 为什么不行？"></a>self-attention 为什么要使用 Q、K、V，仅仅使用 Q、V&#x2F;K、V 或者 V 为什么不行？</h3><p>self-attention 使用 Q、K、V，这样三个参数独立，模型的表达能力和灵活性显然会比只用 Q、V 或者只用 V 要好些，当然主流 attention 的做法还有很多种，比如说 seq2seq with attention 也就只有 hidden state 来做相似性的计算，处理不同的任务，attention 的做法有细微的不同，但是主体思想还是一致的。</p><blockquote><p>其实还有个小细节，因为 self-attention 的范围是包括自身的(masked self-attention 也是一样)，因此至少是要采用 Q、V 或者 K、V 的形式，而这样“询问式”的 attention 方式，个人感觉 Q、K、V 显然合理一些。</p></blockquote><h3 id="Transformer-为什么需要进行-Multi-head-Attention？"><a href="#Transformer-为什么需要进行-Multi-head-Attention？" class="headerlink" title="Transformer 为什么需要进行 Multi-head Attention？"></a>Transformer 为什么需要进行 Multi-head Attention？</h3><p>原论文中说到进行 Multi-head Attention 的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次 attention，多次 attention 综合的结果至少能够起到增强模型的作用，也可以类比 CNN 中同时使用多个卷积核的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征信息</strong>。</p><h3 id="Transformer-相比于-RNN-x2F-LSTM，有什么优势？"><a href="#Transformer-相比于-RNN-x2F-LSTM，有什么优势？" class="headerlink" title="Transformer 相比于 RNN&#x2F;LSTM，有什么优势？"></a>Transformer 相比于 RNN&#x2F;LSTM，有什么优势？</h3><ul><li><p>RNN 系列的模型，并行计算能力很差。RNN 系列的模型$T$时刻隐层状态的计算，依赖两个输入，一个是$T$时刻的句子输入单词$X_t$，另一个是$T-1$时刻的隐层状态的输出$S_{t-1}$，这是最能体现 RNN 本质特征的一点，RNN 的历史信息是通过这个信息传输渠道往后传输的。而 RNN 并行计算的问题就出在这里，因为时刻的计算依赖$T-1$时刻的隐层计算结果，而$T-1$时刻的计算依赖$T-2$时刻的隐层计算结果，如此下去就形成了所谓的序列依赖关系。</p></li><li><p>Transformer 的特征抽取能力比 RNN 系列的模型要好。但是值得注意的是，并不是说 Transformer 就能够完全替代 RNN 系列的模型了，任何模型都有其适用范围，同样的，RNN 系列模型在很多任务上还是首选，熟悉各种模型的内部原理，知其然且知其所以然，才能遇到新任务时，快速分析这时候该用什么样的模型，该怎么做好。</p></li></ul><h3 id="Transformer-是如何训练的？"><a href="#Transformer-是如何训练的？" class="headerlink" title="Transformer 是如何训练的？"></a>Transformer 是如何训练的？</h3><p>Transformer 训练过程与 seq2seq 类似，首先 Encoder 端得到输入的 encoding 表示，并将其输入到 Decoder 端做交互式 attention，之后在 Decoder 端接收其相应的输入(见 1 中有详细分析)，经过多头 self-attention 模块之后，结合 Encoder 端的输出，再经过 FFN，得到 Decoder 端的输出之后，最后经过一个线性全连接层，就可以通过 softmax 来预测下一个单词(token)，然后根据 softmax 多分类的损失函数，将 loss 反向传播即可，所以从整体上来说，Transformer 训练过程就相当于一个有监督的多分类问题。</p><blockquote><p>需要注意的是，<strong>Encoder 端可以并行计算，一次性将输入序列全部 encoding 出来，但 Decoder 端不是一次性把所有单词(token)预测出来的，而是像 seq2seq 一样一个接着一个预测出来的。</strong></p></blockquote><h3 id="Transformer-测试阶段如何进行测试呢？"><a href="#Transformer-测试阶段如何进行测试呢？" class="headerlink" title="Transformer 测试阶段如何进行测试呢？"></a>Transformer 测试阶段如何进行测试呢？</h3><p>对于测试阶段，其与训练阶段唯一不同的是 Decoder 端最底层的输入。</p><h3 id="为什么说-Transformer-可以代替-seq2seq？"><a href="#为什么说-Transformer-可以代替-seq2seq？" class="headerlink" title="为什么说 Transformer 可以代替 seq2seq？"></a>为什么说 Transformer 可以代替 seq2seq？</h3><p>这里用代替这个词略显不妥当，seq2seq 虽已老，但始终还是有其用武之地，seq2seq 最大的问题在于「将 Encoder 端的所有信息压缩到一个固定长度的向量中」，并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，而且这样一股脑的把该固定向量送入 Decoder 端，Decoder 端不能够关注到其想要关注的信息。</p><p>上述两点都是 seq2seq 模型的缺点，后续论文对这两点有所改进，如著名的Neural Machine Translation by Jointly Learning to Align and Translate[11]，虽然确确实实对 seq2seq 模型有了实质性的改进，但是由于主体模型仍然为 RNN(LSTM)系列的模型，因此模型的并行能力还是受限，而 transformer 不但对 seq2seq 模型这两点缺点有了实质性的改进(多头交互式 attention 模块)，而且还引入了 self-attention 模块，让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力(ACL 2018 会议上有论文对 Self-Attention 和 FFN 等模块都有实验分析，见论文：How Much Attention Do You Need?A Granular Analysis of Neural Machine Translation Architectures[12])，并且 Transformer 并行计算的能力是远远超过 seq2seq 系列的模型，因此我认为这是 transformer 优于 seq2seq 模型的地方。</p><h3 id="self-attention-公式中的归一化有什么作用？"><a href="#self-attention-公式中的归一化有什么作用？" class="headerlink" title="self-attention 公式中的归一化有什么作用？"></a>self-attention 公式中的归一化有什么作用？</h3><p>首先说明做归一化的原因，随着的增大，点积后的结果也随之增大，这样会将 softmax 函数推入梯度非常小的区域，使得收敛困难(可能出现梯度消失的情况)</p><p>为了说明点积变大的原因，假设$q$和$k$的分量是具有均值 0 和方差 1 的独立随机变量，那么它们的点积均值为 0，方差为$d_k$，因此为了抵消这种影响，我们将点积缩放，对于更详细的分析，参见<a href="https://www.zhihu.com/question/339723385">transformer 中的 attention 为什么 scaled?</a></p><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/108288834">Transformer: Attention的集大成者</a></p></li><li><p><a href="https://www.zhihu.com/question/339723385">transformer中的attention为什么scaled?</a></p></li><li><p><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p></li><li><p><a href="https://blog.csdn.net/weixin_42142630/article/details/114928214?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&utm_relevant_index=2">通俗理解Transformer中的Attention机制</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/367120088">关于bert中softmax前除以维度d的理解</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/189717114">Hugging Face的BERT源码框架图文详解</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650412561&idx=2&sn=ef7a88ca7acfb4d666d51f1c0a7cdb7f&chksm=becd904b89ba195d9c07e5aca6fcf483cce3aee0378049d4abb977d5ce2a98095efc963151a2&mpshare=1&scene=1&srcid=0209JnIuxMtyP2L5PZN3HEfw&sharer_sharetime=1644408100818&sharer_shareid=e3facbe5e17968b425891a08c9231ad1#rd">关于Transformer，面试官们都怎么问</a></p></li><li><p><a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">图解Transformer（完整版）</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src= &quot;/img/loading.gif&quot; data-lazy-src=&quot;https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/transformer.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;左边处理源语言，称之为Encoder，</summary>
      
    
    
    
    
    <category term="Transformer" scheme="https://www.songzj.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>crontab教程</title>
    <link href="https://www.songzj.com/posts/7cb382c9/"/>
    <id>https://www.songzj.com/posts/7cb382c9/</id>
    <published>2021-09-06T07:28:15.000Z</published>
    <updated>2021-09-06T07:28:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>大名鼎鼎的crontab就不用介绍了，直接上干货</p><h2 id="Crontab-命令"><a href="#Crontab-命令" class="headerlink" title="Crontab 命令"></a>Crontab 命令</h2><ul><li><strong>crontab -e</strong>: 编辑 crontab 文件, 如果文件不存在则新建</li><li><strong>crontab -l</strong>: 列出crontab文件的内容</li><li><strong>crontab -r</strong>: 删除crontab文件</li></ul><h2 id="Crontab-案例"><a href="#Crontab-案例" class="headerlink" title="Crontab 案例"></a>Crontab 案例</h2><p>每一分钟执行一次 &#x2F;bin&#x2F;ls：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*  *  *  *  *  /bin/ls</span><br></pre></td></tr></table></figure><p>在 12 月内, 每天的早上 6 点到 12 点，每隔 3 个小时 0 分钟执行一次 &#x2F;usr&#x2F;bin&#x2F;backup：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0  6-12/3  *  12  *  /usr/bin/backup</span><br></pre></td></tr></table></figure><p>周一到周五每天下午 5:00 寄一封信给 <a href="mailto:&#97;&#108;&#x65;&#120;&#64;&#100;&#111;&#109;&#97;&#x69;&#110;&#46;&#x6e;&#97;&#x6d;&#x65;">&#97;&#108;&#x65;&#120;&#64;&#100;&#111;&#109;&#97;&#x69;&#110;&#46;&#x6e;&#97;&#x6d;&#x65;</a>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0  17  *  *  1-5 mail -s &quot;hi&quot; alex@domain.name &lt;  /tmp/maildata</span><br></pre></td></tr></table></figure><p>每月每天的午夜 0 点 20 分, 2 点 20 分, 4 点 20 分....执行 echo &quot;haha&quot;：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20  0-23/2  *  *  * echo &quot;haha&quot;</span><br></pre></td></tr></table></figure><p>意思是每两个小时重启一次apache </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0  */2 * * * /sbin/service httpd restart</span><br></pre></td></tr></table></figure><p>意思是每天7：50开启ssh服务  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">50  7  *  *  *  /sbin/service sshd start</span><br></pre></td></tr></table></figure><p>意思是每天22：50关闭ssh服务  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">50  22  *  *  *  /sbin/service sshd stop</span><br></pre></td></tr></table></figure><p>每月1号和15号检查&#x2F;home 磁盘  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0  0  1,15  *  * fsck /home</span><br></pre></td></tr></table></figure><p>每小时的第一分执行  &#x2F;home&#x2F;bruce&#x2F;backup这个文件  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1  *  *  *  *  /home/bruce/backup</span><br></pre></td></tr></table></figure><p>每周一至周五3点钟，在目录&#x2F;home中，查找文件名为*.xxx的文件，并删除4天前的文件。  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">00  03  *  *  1-5 find /home &quot;*.xxx&quot;  -mtime +4  -exec rm &#123;&#125; \; </span><br></pre></td></tr></table></figure><p>意思是每月的1、11、21、31日是的6：30执行一次ls命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">30  6  */10  *  * ls</span><br></pre></td></tr></table></figure><h3 id="更多-crontab-案例"><a href="#更多-crontab-案例" class="headerlink" title="更多 crontab 案例"></a>更多 crontab 案例</h3><table><thead><tr><th align="center">min</th><th align="center">hour</th><th align="center">day&#x2F;month</th><th align="center">month</th><th align="center">day&#x2F;week</th><th align="center">Execution time</th></tr></thead><tbody><tr><td align="center">30</td><td align="center">0</td><td align="center">1</td><td align="center">1,6,12</td><td align="center">*</td><td align="center">— 00:30 Hrs on 1st of Jan, June &amp; Dec.</td></tr><tr><td align="center">0</td><td align="center">20</td><td align="center">*</td><td align="center">10</td><td align="center">1-5</td><td align="center">–8.00 PM every weekday (Mon-Fri) only in Oct.</td></tr><tr><td align="center">0</td><td align="center">0</td><td align="center">1,10,15</td><td align="center">*</td><td align="center">*</td><td align="center">— midnight on 1st ,10th &amp; 15th of month</td></tr><tr><td align="center">5,10</td><td align="center">0</td><td align="center">10</td><td align="center">*</td><td align="center">1</td><td align="center">— At 12.05,12.10 every Monday &amp; on 10th of every month</td></tr><tr><td align="center">...</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>注意：</strong>当程序在你所指定的时间执行后，系统会发一封邮件给当前的用户，显示该程序执行的内容，若是你不希望收到这样的邮件，请在每一行空一格之后加上 &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1 即可，如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20  03  *  *  *  bash  /etc/profile/test.sh &gt;  /dev/null  2&gt;&amp;1 </span><br></pre></td></tr></table></figure><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://www.adminschoice.com/crontab-quick-reference">Crontab – Quick Reference</a></li><li><a href="https://www.runoob.com/linux/linux-comm-crontab.html">RUNOOB crontab教程</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;大名鼎鼎的crontab就不用介绍了，直接上干货&lt;/p&gt;
&lt;h2 id=&quot;Crontab-命令&quot;&gt;&lt;a href=&quot;#Crontab-命令&quot; class=&quot;headerlink&quot; title=&quot;Crontab 命令&quot;&gt;&lt;/a&gt;Crontab 命令&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="教程" scheme="https://www.songzj.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="crontab" scheme="https://www.songzj.com/tags/crontab/"/>
    
    <category term="教程" scheme="https://www.songzj.com/tags/%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Markdown教程</title>
    <link href="https://www.songzj.com/posts/89757140/"/>
    <id>https://www.songzj.com/posts/89757140/</id>
    <published>2021-09-04T15:31:06.000Z</published>
    <updated>2021-09-05T01:15:39.000Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="首行缩进"><a href="#首行缩进" class="headerlink" title="首行缩进"></a>首行缩进</h2><p>一个汉字占两个空格大小，所以使用四个空格就可以达到首行缩进两个汉字的效果。有如下几种方法：<br>1.一个空格大小的表示：&amp;ensp;或&amp;#8194;，此时只要在相应需要缩进的段落前加上 4个 如上的标记即可，注意要带上分号。<br>2.两个空格的大小表示：&amp;emsp;或&amp;#8195;，同理，使用2个即可缩进2个汉字，推荐使用该方式。<br>3.不换行空格：&amp;nbsp;或&amp;#160;，使用4个&amp;#160;即可。</p><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$y_t = \tau_t + \zeta_t$$</span><br></pre></td></tr></table></figure><p>$$y_t &#x3D; \tau_t + \zeta_t$$</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\min_&#123;\\&#123; \tau_&#123;t&#125;\\&#125; &#125;\sum_&#123;t&#125;^&#123;T&#125;\zeta_&#123;t&#125;^&#123;2&#125;+\lambda\sum_&#123;t=1&#125;^&#123;T&#125;\left[\left(\tau_&#123;t&#125;-\tau_&#123;t-1&#125;\right)-\left(\tau_&#123;t-1&#125;-\tau_&#123;t-2&#125;\right)\right]^&#123;2&#125;$$</span><br></pre></td></tr></table></figure><p>$$\min_{\{ \tau_{t}\} }\sum_{t}^{T}\zeta_{t}^{2}+\lambda\sum_{t&#x3D;1}^{T}\left[\left(\tau_{t}-\tau_{t-1}\right)-\left(\tau_{t-1}-\tau_{t-2}\right)\right]^{2}$$</p><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># H1</span></span><br><span class="line"><span class="section">## H2</span></span><br><span class="line"><span class="section">### H3</span></span><br><span class="line"><span class="section">#### H4</span></span><br><span class="line"><span class="section">##### H5</span></span><br><span class="line"><span class="section">###### H6</span></span><br><span class="line"></span><br><span class="line">Alternatively, for H1 and H2, an underline-ish style:</span><br><span class="line"></span><br><span class="line"><span class="section">Alt-H1</span></span><br><span class="line"><span class="section">======</span></span><br><span class="line"></span><br><span class="line"><span class="section">Alt-H2</span></span><br><span class="line"><span class="section">------</span></span><br></pre></td></tr></table></figure><h1 id="H1"><a href="#H1" class="headerlink" title="H1"></a>H1</h1><h2 id="H2"><a href="#H2" class="headerlink" title="H2"></a>H2</h2><h3 id="H3"><a href="#H3" class="headerlink" title="H3"></a>H3</h3><h4 id="H4"><a href="#H4" class="headerlink" title="H4"></a>H4</h4><h5 id="H5"><a href="#H5" class="headerlink" title="H5"></a>H5</h5><h6 id="H6"><a href="#H6" class="headerlink" title="H6"></a>H6</h6><p>Alternatively, for H1 and H2, an underline-ish style:</p><h1 id="Alt-H1"><a href="#Alt-H1" class="headerlink" title="Alt-H1"></a>Alt-H1</h1><h2 id="Alt-H2"><a href="#Alt-H2" class="headerlink" title="Alt-H2"></a>Alt-H2</h2><h2 id="强调"><a href="#强调" class="headerlink" title="强调"></a>强调</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Emphasis, aka italics, with <span class="emphasis">*asterisks*</span> or <span class="emphasis">_underscores_</span>.</span><br><span class="line"></span><br><span class="line">Strong emphasis, aka bold, with <span class="strong">**asterisks**</span> or <span class="strong">__underscores__</span>.</span><br><span class="line"></span><br><span class="line">Combined emphasis with <span class="strong">**asterisks and <span class="emphasis">_underscores_</span>**</span>.</span><br><span class="line"></span><br><span class="line">Strikethrough uses two tildes. ~~Scratch this.~~</span><br></pre></td></tr></table></figure><p>Emphasis, aka italics, with <em>asterisks</em> or <em>underscores</em>.</p><p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p><p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p><p>Strikethrough uses two tildes. <del>Scratch this.</del></p><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> First ordered list item</span><br><span class="line"><span class="bullet">2.</span> Another item</span><br><span class="line"><span class="bullet">  *</span> Unordered sub-list.</span><br><span class="line"><span class="bullet">1.</span> Actual numbers don&#x27;t matter, just that it&#x27;s a number</span><br><span class="line"><span class="bullet">  1.</span> Ordered sub-list</span><br><span class="line"><span class="bullet">4.</span> And another item.</span><br><span class="line"></span><br><span class="line">   You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we&#x27;ll use three here to also align the raw Markdown).</span><br><span class="line"></span><br><span class="line">   To have a line break without a paragraph, you will need to use two trailing spaces.  </span><br><span class="line">   Note that this line is separate, but within the same paragraph.  </span><br><span class="line">   (This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</span><br><span class="line"></span><br><span class="line"><span class="bullet">*</span> Unordered list can use asterisks</span><br><span class="line"><span class="bullet">-</span> Or minuses</span><br><span class="line"><span class="bullet">+</span> Or pluses</span><br><span class="line"><span class="bullet">-</span> Paragraph In unordered list</span><br><span class="line"></span><br><span class="line">  For example like this.</span><br><span class="line"></span><br><span class="line">Common Paragraph with some text.</span><br><span class="line">And more text.</span><br></pre></td></tr></table></figure><ol><li>First ordered list item</li><li>Another item</li></ol><ul><li>Unordered sub-list.</li></ul><ol><li><p>Actual numbers don&#39;t matter, just that it&#39;s a number</p></li><li><p>Ordered sub-list</p></li><li><p>And another item.</p><p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we&#39;ll use three here to also align the raw Markdown).</p><p>To have a line break without a paragraph, you will need to use two trailing spaces.<br>Note that this line is separate, but within the same paragraph.<br>(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p></li></ol><ul><li>Unordered list can use asterisks</li></ul><ul><li>Or minuses</li></ul><ul><li>Or pluses</li></ul><ul><li><p>Paragraph In unordered list</p><p>For example like this.</p></li></ul><p>Common Paragraph with some text.<br>And more text.</p><h2 id="嵌入-HTML"><a href="#嵌入-HTML" class="headerlink" title="嵌入 HTML"></a>嵌入 HTML</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span>To reboot your computer, press <span class="language-xml"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span></span>ctrl<span class="language-xml"><span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span>+<span class="language-xml"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span></span>alt<span class="language-xml"><span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span>+<span class="language-xml"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span></span>del<span class="language-xml"><span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span>.<span class="language-xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>To reboot your computer, press <kbd>ctrl</kbd>+<kbd>alt</kbd>+<kbd>del</kbd>.</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">dl</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;dt&gt;Definition list&lt;/dt&gt;</span></span><br><span class="line"><span class="code">    &lt;dd&gt;Is something people use sometimes.&lt;/dd&gt;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">    &lt;dt&gt;Markdown in HTML&lt;/dt&gt;</span></span><br><span class="line"><span class="code">    &lt;dd&gt;Does *not* work **very** well. Use HTML &lt;em&gt;tags&lt;/em&gt;.&lt;/dd&gt;</span></span><br><span class="line"><span class="code">&lt;/dl&gt;</span></span><br></pre></td></tr></table></figure><dl>    <dt>Definition list</dt>    <dd>Is something people use sometimes.</dd><pre><code>&lt;dt&gt;Markdown in HTML&lt;/dt&gt;&lt;dd&gt;Does *not* work **very** well. Use HTML &lt;em&gt;tags&lt;/em&gt;.&lt;/dd&gt;</code></pre></dl><h2 id="表"><a href="#表" class="headerlink" title="表"></a>表</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|                |ASCII                          |HTML                         |</span><br><span class="line">|----------------|-------------------------------|-----------------------------|</span><br><span class="line">|Single backticks|<span class="code">`&#x27;Isn&#x27;t this fun?&#x27;`</span>            |&#x27;Isn&#x27;t this fun?&#x27;            |</span><br><span class="line">|Quotes          |<span class="code">`&quot;Isn&#x27;t this fun?&quot;`</span>            |&quot;Isn&#x27;t this fun?&quot;            |</span><br><span class="line">|Dashes          |<span class="code">`-- is en-dash, --- is em-dash`</span>|-- is en-dash, --- is em-dash|</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>ASCII</th><th>HTML</th></tr></thead><tbody><tr><td>Single backticks</td><td><code>&#39;Isn&#39;t this fun?&#39;</code></td><td>&#39;Isn&#39;t this fun?&#39;</td></tr><tr><td>Quotes</td><td><code>&quot;Isn&#39;t this fun?&quot;</code></td><td>&quot;Isn&#39;t this fun?&quot;</td></tr><tr><td>Dashes</td><td><code>-- is en-dash, --- is em-dash</code></td><td>-- is en-dash, --- is em-dash</td></tr></tbody></table><p>Colons can be used to align columns.</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| Tables        | Are           | Cool  |</span><br><span class="line">| ------------- |:-------------:| -----:|</span><br><span class="line">| col 3 is      | right-aligned |  |</span><br><span class="line">| col 2 is      | centered      |    |</span><br><span class="line">| zebra stripes | are neat      |   </span><br></pre></td></tr></table></figure><table><thead><tr><th>Tables</th><th align="center">Are</th><th align="right">Cool</th></tr></thead><tbody><tr><td>col 3 is</td><td align="center">right-aligned</td><td align="right"></td></tr><tr><td>col 2 is</td><td align="center">centered</td><td align="right"></td></tr><tr><td>zebra stripes</td><td align="center">are neat</td><td align="right"></td></tr></tbody></table><p>The outer pipes (|) are optional, and you don&#39;t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Markdown | Less | Pretty</span><br><span class="line">--- | --- | ---</span><br><span class="line"><span class="emphasis">*Still*</span> | <span class="code">`renders`</span> | <span class="strong">**nicely**</span></span><br><span class="line">1 | 2 | 3</span><br></pre></td></tr></table></figure><table><thead><tr><th>Markdown</th><th>Less</th><th>Pretty</th></tr></thead><tbody><tr><td><em>Still</em></td><td><code>renders</code></td><td><strong>nicely</strong></td></tr><tr><td>1</td><td>2</td><td>3</td></tr></tbody></table><blockquote><p>You can find more information about <strong>LaTeX</strong> mathematical expressions <a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference">here</a>.</p></blockquote><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><blockquote><p>Blockquotes are very handy in email to emulate reply text.<br>This line is part of the same quote.</p></blockquote><p>Quote break.</p><blockquote><p>This is a very long line that will still be quoted properly when it wraps. Oh boy let&#39;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p></blockquote><h2 id="水平线"><a href="#水平线" class="headerlink" title="水平线"></a>水平线</h2><p>Three or more...</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">Hyphens</span><br><span class="line"></span><br><span class="line"><span class="strong">**<span class="emphasis">*</span></span></span><br><span class="line"><span class="emphasis"><span class="strong"></span></span></span><br><span class="line"><span class="emphasis"><span class="strong">Asterisks</span></span></span><br><span class="line"><span class="emphasis"><span class="strong"></span></span></span><br><span class="line"><span class="emphasis"><span class="strong">___</span></span></span><br><span class="line"><span class="emphasis"><span class="strong"></span></span></span><br><span class="line"><span class="emphasis"><span class="strong">Underscores</span></span></span><br></pre></td></tr></table></figure><hr><p>Hyphens</p><hr><p>Asterisks</p><hr><p>Underscores</p><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Here&#x27;s a line for us to start with.</span><br><span class="line"></span><br><span class="line">This line is separated from the one above by two newlines, so it will be a <span class="emphasis">*separate paragraph*</span>.</span><br><span class="line"></span><br><span class="line">This line is also a separate paragraph, but...</span><br><span class="line">This line is only separated by a single newline, so it&#x27;s a separate line in the <span class="emphasis">*same paragraph*</span>.</span><br></pre></td></tr></table></figure><p>Here&#39;s a line for us to start with.</p><p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p><p>This line is also a separate paragraph, but...<br>This line is only separated by a single newline, so it&#39;s a separate line in the <em>same paragraph</em>.</p><hr><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">This is a regular paragraph.</span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">table</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;tr&gt;</span></span><br><span class="line"><span class="code">        &lt;td&gt;Foo&lt;/td&gt;</span></span><br><span class="line"><span class="code">    &lt;/tr&gt;</span></span><br><span class="line"><span class="code">&lt;/table&gt;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">This is another regular paragraph.</span><br></pre></td></tr></table></figure><p>This is a regular paragraph.</p><table>    <tr>        <td>Foo</td>    </tr></table><p>This is another regular paragraph.</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">I&#x27;m an inline-style link</span>](<span class="link">https://www.google.com</span>)</span><br><span class="line"></span><br><span class="line">[<span class="string">I&#x27;m an inline-style link with title</span>](<span class="link">https://www.google.com &quot;Google&#x27;s Homepage&quot;</span>)</span><br><span class="line"></span><br><span class="line">[<span class="string">I&#x27;m a reference-style link</span>][<span class="symbol">Arbitrary case-insensitive reference text</span>]</span><br><span class="line"></span><br><span class="line">[<span class="string">I&#x27;m a relative reference to a repository file</span>](<span class="link">../blob/master/LICENSE</span>)</span><br><span class="line"></span><br><span class="line">[<span class="string">You can use numbers for reference-style link definitions</span>][<span class="symbol">1</span>]</span><br><span class="line"></span><br><span class="line">Or leave it empty and use the [link text itself]</span><br><span class="line"></span><br><span class="line">Some text to show that the reference links can follow later.</span><br><span class="line"></span><br><span class="line">[<span class="symbol">arbitrary case-insensitive reference text</span>]: <span class="link">https://hexo.io</span></span><br><span class="line">[<span class="symbol">1</span>]: <span class="link">https://hexo.io/docs/</span></span><br><span class="line">[<span class="symbol">link text itself</span>]: <span class="link">https://hexo.io/api/</span></span><br></pre></td></tr></table></figure><p><a href="https://www.google.com/">I&#39;m an inline-style link</a></p><p><a href="https://www.google.com/" title="Google&#39;s Homepage">I&#39;m an inline-style link with title</a></p><p><a href="https://hexo.io/">I&#39;m a reference-style link</a></p><p><a href="../blob/master/LICENSE">I&#39;m a relative reference to a repository file</a></p><p><a href="https://hexo.io/docs/">You can use numbers for reference-style link definitions</a></p><p>Or leave it empty and use the <a href="https://hexo.io/api/">link text itself</a></p><p>Some text to show that the reference links can follow later.</p><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hover to see the title text:</span><br><span class="line"></span><br><span class="line">Inline-style:</span><br><span class="line"></span><br><span class="line">![<span class="string">alt text</span>](<span class="link">https://hexo.io/icon/favicon-196x196.png &quot;Logo Title Text 1&quot;</span>)</span><br><span class="line"></span><br><span class="line">Reference-style:</span><br><span class="line">![<span class="string">alt text</span>][<span class="symbol">logo</span>]</span><br><span class="line"></span><br><span class="line">[<span class="symbol">logo</span>]: <span class="link">https://hexo.io/icon/favicon-196x196.png &quot;Logo Title Text 2&quot;</span></span><br></pre></td></tr></table></figure><p>hover to see the title text:</p><p>Inline-style:</p><p><img src= "/img/loading.gif" data-lazy-src="https://hexo.io/icon/favicon-196x196.png" alt="alt text" title="Logo Title Text 1"></p><p>Reference-style:<br><img src= "/img/loading.gif" data-lazy-src="https://hexo.io/icon/favicon-196x196.png" alt="alt text" title="Logo Title Text 2"></p><h2 id="视频"><a href="#视频" class="headerlink" title="视频"></a>视频</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;https://www.youtube.com/watch?feature=player_embedded&amp;v=ARted4RniaU</span></span></span></span><br><span class="line"><span class="string"><span class="tag"><span class="language-xml">&quot;</span> <span class="attr">target</span>=<span class="string">&quot;_blank&quot;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://img.youtube.com/vi/ARted4RniaU/0.jpg&quot;</span></span></span></span><br><span class="line"><span class="tag"><span class="language-xml"><span class="attr">alt</span>=<span class="string">&quot;IMAGE ALT TEXT HERE&quot;</span> <span class="attr">width</span>=<span class="string">&quot;240&quot;</span> <span class="attr">height</span>=<span class="string">&quot;180&quot;</span> <span class="attr">border</span>=<span class="string">&quot;10&quot;</span> /&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">Pure markdown version:</span><br><span class="line"></span><br><span class="line">[<span class="string">![IMAGE ALT TEXT HERE</span>](<span class="link">https://img.youtube.com/vi/ARted4RniaU/0.jpg</span>)](<span class="link">https://www.youtube.com/watch?v=ARted4RniaU</span>)</span><br></pre></td></tr></table></figure><p><a href="https://www.youtube.com/watch?feature=player_embedded&v=ARted4RniaU" target="_blank"><img src= "/img/loading.gif" data-lazy-src="https://img.youtube.com/vi/ARted4RniaU/0.jpg"alt="IMAGE ALT TEXT HERE" width="240" height="180" border="10" /></a></p><p>Pure markdown version:</p><p><a href="https://www.youtube.com/watch?v=ARted4RniaU"><img src= "/img/loading.gif" data-lazy-src="https://img.youtube.com/vi/ARted4RniaU/0.jpg" alt="IMAGE ALT TEXT HERE"></a></p><h2 id="代码和语法高亮"><a href="#代码和语法高亮" class="headerlink" title="代码和语法高亮"></a>代码和语法高亮</h2><p>Inline <code>code</code> has <code>back-ticks around</code> it.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> s = <span class="string">&quot;JavaScript syntax highlighting&quot;</span>;</span><br><span class="line"><span class="title function_">alert</span>(s);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&quot;Python syntax highlighting&quot;</span></span><br><span class="line"><span class="built_in">print</span> s</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">No language indicated, so no syntax highlighting.</span><br><span class="line">But let&#x27;s throw in a &lt;b&gt;tag&lt;/b&gt;.</span><br></pre></td></tr></table></figure><span id="more"></span><p>This post is originated from <a href="https://gist.github.com/apackeer/4159268">here</a> and is used for testing markdown style. This post contains nearly every markdown usage. Make sure all the markdown elements below show up correctly.</p>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;首行缩进&quot;&gt;&lt;a href=&quot;#首行缩进&quot; class=&quot;headerlink&quot; title=&quot;首行缩进&quot;&gt;&lt;/a&gt;首行缩进&lt;/h2&gt;&lt;p&gt;一个汉字占两个空格大小，所以使用四个空格就可以达到首行缩进两个汉字的效果。有如下几种方法：&lt;br&gt;1.一个空格大小的表示：&amp;amp;ensp;或&amp;amp;#8194;，此时只要在相应需要缩进的段落前加上 4个 如上的标记即可，注意要带上分号。&lt;br&gt;2.两个空格的大小表示：&amp;amp;emsp;或&amp;amp;#8195;，同理，使用2个即可缩进2个汉字，推荐使用该方式。&lt;br&gt;3.不换行空格：&amp;amp;nbsp;或&amp;amp;#160;，使用4个&amp;amp;#160;即可。&lt;/p&gt;
&lt;h2 id=&quot;公式&quot;&gt;&lt;a href=&quot;#公式&quot; class=&quot;headerlink&quot; title=&quot;公式&quot;&gt;&lt;/a&gt;公式&lt;/h2&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$$y_t = \tau_t + \zeta_t$$&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;$$y_t &amp;#x3D; \tau_t + \zeta_t$$&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$$\min_&amp;#123;\\&amp;#123; \tau_&amp;#123;t&amp;#125;\\&amp;#125; &amp;#125;\sum_&amp;#123;t&amp;#125;^&amp;#123;T&amp;#125;\zeta_&amp;#123;t&amp;#125;^&amp;#123;2&amp;#125;+\lambda\sum_&amp;#123;t=1&amp;#125;^&amp;#123;T&amp;#125;\left[\left(\tau_&amp;#123;t&amp;#125;-\tau_&amp;#123;t-1&amp;#125;\right)-\left(\tau_&amp;#123;t-1&amp;#125;-\tau_&amp;#123;t-2&amp;#125;\right)\right]^&amp;#123;2&amp;#125;$$&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;$$\min_{\{ \tau_{t}\} }\sum_{t}^{T}\zeta_{t}^{2}+\lambda\sum_{t&amp;#x3D;1}^{T}\left[\left(\tau_{t}-\tau_{t-1}\right)-\left(\tau_{t-1}-\tau_{t-2}\right)\right]^{2}$$&lt;/p&gt;
&lt;h2 id=&quot;标题&quot;&gt;&lt;a href=&quot;#标题&quot; class=&quot;headerlink&quot; title=&quot;标题&quot;&gt;&lt;/a&gt;标题&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;# H1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;## H2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;### H3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;#### H4&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;##### H5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;###### H6&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Alternatively, for H1 and H2, an underline-ish style:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;Alt-H1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;======&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;Alt-H2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;section&quot;&gt;------&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h1 id=&quot;H1&quot;&gt;&lt;a href=&quot;#H1&quot; class=&quot;headerlink&quot; title=&quot;H1&quot;&gt;&lt;/a&gt;H1&lt;/h1&gt;&lt;h2 id=&quot;H2&quot;&gt;&lt;a href=&quot;#H2&quot; class=&quot;headerlink&quot; title=&quot;H2&quot;&gt;&lt;/a&gt;H2&lt;/h2&gt;&lt;h3 id=&quot;H3&quot;&gt;&lt;a href=&quot;#H3&quot; class=&quot;headerlink&quot; title=&quot;H3&quot;&gt;&lt;/a&gt;H3&lt;/h3&gt;&lt;h4 id=&quot;H4&quot;&gt;&lt;a href=&quot;#H4&quot; class=&quot;headerlink&quot; title=&quot;H4&quot;&gt;&lt;/a&gt;H4&lt;/h4&gt;&lt;h5 id=&quot;H5&quot;&gt;&lt;a href=&quot;#H5&quot; class=&quot;headerlink&quot; title=&quot;H5&quot;&gt;&lt;/a&gt;H5&lt;/h5&gt;&lt;h6 id=&quot;H6&quot;&gt;&lt;a href=&quot;#H6&quot; class=&quot;headerlink&quot; title=&quot;H6&quot;&gt;&lt;/a&gt;H6&lt;/h6&gt;&lt;p&gt;Alternatively, for H1 and H2, an underline-ish style:&lt;/p&gt;
&lt;h1 id=&quot;Alt-H1&quot;&gt;&lt;a href=&quot;#Alt-H1&quot; class=&quot;headerlink&quot; title=&quot;Alt-H1&quot;&gt;&lt;/a&gt;Alt-H1&lt;/h1&gt;&lt;h2 id=&quot;Alt-H2&quot;&gt;&lt;a href=&quot;#Alt-H2&quot; class=&quot;headerlink&quot; title=&quot;Alt-H2&quot;&gt;&lt;/a&gt;Alt-H2&lt;/h2&gt;&lt;h2 id=&quot;强调&quot;&gt;&lt;a href=&quot;#强调&quot; class=&quot;headerlink&quot; title=&quot;强调&quot;&gt;&lt;/a&gt;强调&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Emphasis, aka italics, with &lt;span class=&quot;emphasis&quot;&gt;*asterisks*&lt;/span&gt; or &lt;span class=&quot;emphasis&quot;&gt;_underscores_&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Strong emphasis, aka bold, with &lt;span class=&quot;strong&quot;&gt;**asterisks**&lt;/span&gt; or &lt;span class=&quot;strong&quot;&gt;__underscores__&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Combined emphasis with &lt;span class=&quot;strong&quot;&gt;**asterisks and &lt;span class=&quot;emphasis&quot;&gt;_underscores_&lt;/span&gt;**&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Strikethrough uses two tildes. ~~Scratch this.~~&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;Emphasis, aka italics, with &lt;em&gt;asterisks&lt;/em&gt; or &lt;em&gt;underscores&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Strong emphasis, aka bold, with &lt;strong&gt;asterisks&lt;/strong&gt; or &lt;strong&gt;underscores&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Combined emphasis with &lt;strong&gt;asterisks and &lt;em&gt;underscores&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Strikethrough uses two tildes. &lt;del&gt;Scratch this.&lt;/del&gt;&lt;/p&gt;
&lt;h2 id=&quot;列表&quot;&gt;&lt;a href=&quot;#列表&quot; class=&quot;headerlink&quot; title=&quot;列表&quot;&gt;&lt;/a&gt;列表&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;1.&lt;/span&gt; First ordered list item&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;2.&lt;/span&gt; Another item&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;  *&lt;/span&gt; Unordered sub-list.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;1.&lt;/span&gt; Actual numbers don&amp;#x27;t matter, just that it&amp;#x27;s a number&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;  1.&lt;/span&gt; Ordered sub-list&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;4.&lt;/span&gt; And another item.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we&amp;#x27;ll use three here to also align the raw Markdown).&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   To have a line break without a paragraph, you will need to use two trailing spaces.  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   Note that this line is separate, but within the same paragraph.  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   (This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;*&lt;/span&gt; Unordered list can use asterisks&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; Or minuses&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;+&lt;/span&gt; Or pluses&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; Paragraph In unordered list&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  For example like this.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Common Paragraph with some text.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;And more text.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;First ordered list item&lt;/li&gt;
&lt;li&gt;Another item&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Unordered sub-list.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Actual numbers don&amp;#39;t matter, just that it&amp;#39;s a number&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ordered sub-list&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And another item.&lt;/p&gt;
&lt;p&gt;You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we&amp;#39;ll use three here to also align the raw Markdown).&lt;/p&gt;
&lt;p&gt;To have a line break without a paragraph, you will need to use two trailing spaces.&lt;br&gt;Note that this line is separate, but within the same paragraph.&lt;br&gt;(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Unordered list can use asterisks&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Or minuses&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Or pluses&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Paragraph In unordered list&lt;/p&gt;
&lt;p&gt;For example like this.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Common Paragraph with some text.&lt;br&gt;And more text.&lt;/p&gt;
&lt;h2 id=&quot;嵌入-HTML&quot;&gt;&lt;a href=&quot;#嵌入-HTML&quot; class=&quot;headerlink&quot; title=&quot;嵌入 HTML&quot;&gt;&lt;/a&gt;嵌入 HTML&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;To reboot your computer, press &lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;kbd&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;ctrl&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;kbd&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;+&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;kbd&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;alt&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;kbd&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;+&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;kbd&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;del&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;kbd&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;.&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;To reboot your computer, press &lt;kbd&gt;ctrl&lt;/kbd&gt;+&lt;kbd&gt;alt&lt;/kbd&gt;+&lt;kbd&gt;del&lt;/kbd&gt;.&lt;/p&gt;


&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;dl&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;dt&amp;gt;Definition list&amp;lt;/dt&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;dd&amp;gt;Is something people use sometimes.&amp;lt;/dd&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;dt&amp;gt;Markdown in HTML&amp;lt;/dt&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;dd&amp;gt;Does *not* work **very** well. Use HTML &amp;lt;em&amp;gt;tags&amp;lt;/em&amp;gt;.&amp;lt;/dd&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;&amp;lt;/dl&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;


&lt;dl&gt;
    &lt;dt&gt;Definition list&lt;/dt&gt;
    &lt;dd&gt;Is something people use sometimes.&lt;/dd&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dt&amp;gt;Markdown in HTML&amp;lt;/dt&amp;gt;
&amp;lt;dd&amp;gt;Does *not* work **very** well. Use HTML &amp;lt;em&amp;gt;tags&amp;lt;/em&amp;gt;.&amp;lt;/dd&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/dl&gt;


&lt;h2 id=&quot;表&quot;&gt;&lt;a href=&quot;#表&quot; class=&quot;headerlink&quot; title=&quot;表&quot;&gt;&lt;/a&gt;表&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;|                |ASCII                          |HTML                         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|----------------|-------------------------------|-----------------------------|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|Single backticks|&lt;span class=&quot;code&quot;&gt;`&amp;#x27;Isn&amp;#x27;t this fun?&amp;#x27;`&lt;/span&gt;            |&amp;#x27;Isn&amp;#x27;t this fun?&amp;#x27;            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|Quotes          |&lt;span class=&quot;code&quot;&gt;`&amp;quot;Isn&amp;#x27;t this fun?&amp;quot;`&lt;/span&gt;            |&amp;quot;Isn&amp;#x27;t this fun?&amp;quot;            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|Dashes          |&lt;span class=&quot;code&quot;&gt;`-- is en-dash, --- is em-dash`&lt;/span&gt;|-- is en-dash, --- is em-dash|&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;ASCII&lt;/th&gt;
&lt;th&gt;HTML&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Single backticks&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;#39;Isn&amp;#39;t this fun?&amp;#39;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&amp;#39;Isn&amp;#39;t this fun?&amp;#39;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Quotes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;Isn&amp;#39;t this fun?&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&amp;quot;Isn&amp;#39;t this fun?&amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dashes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-- is en-dash, --- is em-dash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-- is en-dash, --- is em-dash&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Colons can be used to align columns.&lt;/p&gt;
&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;| Tables        | Are           | Cool  |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;| ------------- |:-------------:| -----:|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;| col 3 is      | right-aligned |  |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;| col 2 is      | centered      |    |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;| zebra stripes | are neat      |   &lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tables&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Are&lt;/th&gt;
&lt;th align=&quot;right&quot;&gt;Cool&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;col 3 is&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;right-aligned&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;col 2 is&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;centered&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;zebra stripes&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;are neat&lt;/td&gt;
&lt;td align=&quot;right&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;The outer pipes (|) are optional, and you don&amp;#39;t need to make the raw Markdown line up prettily. You can also use inline Markdown.&lt;/p&gt;
&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Markdown | Less | Pretty&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;--- | --- | ---&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;emphasis&quot;&gt;*Still*&lt;/span&gt; | &lt;span class=&quot;code&quot;&gt;`renders`&lt;/span&gt; | &lt;span class=&quot;strong&quot;&gt;**nicely**&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;1 | 2 | 3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Markdown&lt;/th&gt;
&lt;th&gt;Less&lt;/th&gt;
&lt;th&gt;Pretty&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Still&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;renders&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;nicely&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;You can find more information about &lt;strong&gt;LaTeX&lt;/strong&gt; mathematical expressions &lt;a href=&quot;https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Blockquotes are very handy in email to emulate reply text.&lt;br&gt;This line is part of the same quote.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Quote break.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a very long line that will still be quoted properly when it wraps. Oh boy let&amp;#39;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can &lt;em&gt;put&lt;/em&gt; &lt;strong&gt;Markdown&lt;/strong&gt; into a blockquote.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;水平线&quot;&gt;&lt;a href=&quot;#水平线&quot; class=&quot;headerlink&quot; title=&quot;水平线&quot;&gt;&lt;/a&gt;水平线&lt;/h2&gt;&lt;p&gt;Three or more...&lt;/p&gt;
&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;---&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Hyphens&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;strong&quot;&gt;**&lt;span class=&quot;emphasis&quot;&gt;*&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;emphasis&quot;&gt;&lt;span class=&quot;strong&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;emphasis&quot;&gt;&lt;span class=&quot;strong&quot;&gt;Asterisks&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;emphasis&quot;&gt;&lt;span class=&quot;strong&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;emphasis&quot;&gt;&lt;span class=&quot;strong&quot;&gt;___&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;emphasis&quot;&gt;&lt;span class=&quot;strong&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;emphasis&quot;&gt;&lt;span class=&quot;strong&quot;&gt;Underscores&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;Hyphens&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Asterisks&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Underscores&lt;/p&gt;
&lt;h2 id=&quot;分割线&quot;&gt;&lt;a href=&quot;#分割线&quot; class=&quot;headerlink&quot; title=&quot;分割线&quot;&gt;&lt;/a&gt;分割线&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Here&amp;#x27;s a line for us to start with.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;This line is separated from the one above by two newlines, so it will be a &lt;span class=&quot;emphasis&quot;&gt;*separate paragraph*&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;This line is also a separate paragraph, but...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;This line is only separated by a single newline, so it&amp;#x27;s a separate line in the &lt;span class=&quot;emphasis&quot;&gt;*same paragraph*&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;


&lt;p&gt;Here&amp;#39;s a line for us to start with.&lt;/p&gt;
&lt;p&gt;This line is separated from the one above by two newlines, so it will be a &lt;em&gt;separate paragraph&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This line is also a separate paragraph, but...&lt;br&gt;This line is only separated by a single newline, so it&amp;#39;s a separate line in the &lt;em&gt;same paragraph&lt;/em&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;This is a regular paragraph.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;table&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;tr&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;        &amp;lt;td&amp;gt;Foo&amp;lt;/td&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;/tr&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;&amp;lt;/table&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;This is another regular paragraph.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;This is a regular paragraph.&lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;td&gt;Foo&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This is another regular paragraph.&lt;/p&gt;
&lt;h2 id=&quot;链接&quot;&gt;&lt;a href=&quot;#链接&quot; class=&quot;headerlink&quot; title=&quot;链接&quot;&gt;&lt;/a&gt;链接&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;string&quot;&gt;I&amp;#x27;m an inline-style link&lt;/span&gt;](&lt;span class=&quot;link&quot;&gt;https://www.google.com&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;string&quot;&gt;I&amp;#x27;m an inline-style link with title&lt;/span&gt;](&lt;span class=&quot;link&quot;&gt;https://www.google.com &amp;quot;Google&amp;#x27;s Homepage&amp;quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;string&quot;&gt;I&amp;#x27;m a reference-style link&lt;/span&gt;][&lt;span class=&quot;symbol&quot;&gt;Arbitrary case-insensitive reference text&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;string&quot;&gt;I&amp;#x27;m a relative reference to a repository file&lt;/span&gt;](&lt;span class=&quot;link&quot;&gt;../blob/master/LICENSE&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;string&quot;&gt;You can use numbers for reference-style link definitions&lt;/span&gt;][&lt;span class=&quot;symbol&quot;&gt;1&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Or leave it empty and use the [link text itself]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Some text to show that the reference links can follow later.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;symbol&quot;&gt;arbitrary case-insensitive reference text&lt;/span&gt;]: &lt;span class=&quot;link&quot;&gt;https://hexo.io&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;symbol&quot;&gt;1&lt;/span&gt;]: &lt;span class=&quot;link&quot;&gt;https://hexo.io/docs/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;symbol&quot;&gt;link text itself&lt;/span&gt;]: &lt;span class=&quot;link&quot;&gt;https://hexo.io/api/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;https://www.google.com/&quot;&gt;I&amp;#39;m an inline-style link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.google.com/&quot; title=&quot;Google&amp;#39;s Homepage&quot;&gt;I&amp;#39;m an inline-style link with title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hexo.io/&quot;&gt;I&amp;#39;m a reference-style link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;../blob/master/LICENSE&quot;&gt;I&amp;#39;m a relative reference to a repository file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hexo.io/docs/&quot;&gt;You can use numbers for reference-style link definitions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or leave it empty and use the &lt;a href=&quot;https://hexo.io/api/&quot;&gt;link text itself&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Some text to show that the reference links can follow later.&lt;/p&gt;
&lt;h2 id=&quot;图片&quot;&gt;&lt;a href=&quot;#图片&quot; class=&quot;headerlink&quot; title=&quot;图片&quot;&gt;&lt;/a&gt;图片&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;hover to see the title text:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Inline-style:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;![&lt;span class=&quot;string&quot;&gt;alt text&lt;/span&gt;](&lt;span class=&quot;link&quot;&gt;https://hexo.io/icon/favicon-196x196.png &amp;quot;Logo Title Text 1&amp;quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Reference-style:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;![&lt;span class=&quot;string&quot;&gt;alt text&lt;/span&gt;][&lt;span class=&quot;symbol&quot;&gt;logo&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;symbol&quot;&gt;logo&lt;/span&gt;]: &lt;span class=&quot;link&quot;&gt;https://hexo.io/icon/favicon-196x196.png &amp;quot;Logo Title Text 2&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;hover to see the title text:&lt;/p&gt;
&lt;p&gt;Inline-style:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://hexo.io/icon/favicon-196x196.png&quot; alt=&quot;alt text&quot; title=&quot;Logo Title Text 1&quot;&gt;&lt;/p&gt;
&lt;p&gt;Reference-style:&lt;br&gt;&lt;img src=&quot;https://hexo.io/icon/favicon-196x196.png&quot; alt=&quot;alt text&quot; title=&quot;Logo Title Text 2&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;视频&quot;&gt;&lt;a href=&quot;#视频&quot; class=&quot;headerlink&quot; title=&quot;视频&quot;&gt;&lt;/a&gt;视频&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;href&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;https://www.youtube.com/watch?feature=player_embedded&amp;amp;v=ARted4RniaU&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&lt;span class=&quot;language-xml&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;target&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;_blank&amp;quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;src&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;https://img.youtube.com/vi/ARted4RniaU/0.jpg&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;attr&quot;&gt;alt&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;IMAGE ALT TEXT HERE&amp;quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;width&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;240&amp;quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;height&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;180&amp;quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;border&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;10&amp;quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;language-xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;a&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Pure markdown version:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;string&quot;&gt;![IMAGE ALT TEXT HERE&lt;/span&gt;](&lt;span class=&quot;link&quot;&gt;https://img.youtube.com/vi/ARted4RniaU/0.jpg&lt;/span&gt;)](&lt;span class=&quot;link&quot;&gt;https://www.youtube.com/watch?v=ARted4RniaU&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?feature=player_embedded&amp;v=ARted4RniaU
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/ARted4RniaU/0.jpg&quot;
alt=&quot;IMAGE ALT TEXT HERE&quot; width=&quot;240&quot; height=&quot;180&quot; border=&quot;10&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pure markdown version:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ARted4RniaU&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/ARted4RniaU/0.jpg&quot; alt=&quot;IMAGE ALT TEXT HERE&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;代码和语法高亮&quot;&gt;&lt;a href=&quot;#代码和语法高亮&quot; class=&quot;headerlink&quot; title=&quot;代码和语法高亮&quot;&gt;&lt;/a&gt;代码和语法高亮&lt;/h2&gt;&lt;p&gt;Inline &lt;code&gt;code&lt;/code&gt; has &lt;code&gt;back-ticks around&lt;/code&gt; it.&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;var&lt;/span&gt; s = &lt;span class=&quot;string&quot;&gt;&amp;quot;JavaScript syntax highlighting&amp;quot;&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;title function_&quot;&gt;alert&lt;/span&gt;(s);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;s = &lt;span class=&quot;string&quot;&gt;&amp;quot;Python syntax highlighting&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt; s&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;No language indicated, so no syntax highlighting.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;But let&amp;#x27;s throw in a &amp;lt;b&amp;gt;tag&amp;lt;/b&amp;gt;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="Markdown教程" scheme="https://www.songzj.com/categories/Markdown%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="教程" scheme="https://www.songzj.com/tags/%E6%95%99%E7%A8%8B/"/>
    
    <category term="Markdown" scheme="https://www.songzj.com/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>文本相似度</title>
    <link href="https://www.songzj.com/posts/409cacbb/"/>
    <id>https://www.songzj.com/posts/409cacbb/</id>
    <published>2021-09-01T01:32:59.000Z</published>
    <updated>2021-09-01T01:32:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Jaccard相似度"><a href="#Jaccard相似度" class="headerlink" title="Jaccard相似度"></a>Jaccard相似度</h2><p>Jaccard相似度的定义很简单，两个句子词汇的交集size除以两个句子词汇的并集size。举个例子来说：</p><ul><li>句子1： AI is our friend and it has been friendly.</li><li>句子2： AI and humans have always been friendly.</li></ul><p>为了计算Jaccard相似度，我们首先使用英文nlp中常用的技术<code>lemmatization</code>，用词根替换那些具有相同词根的词汇。在上面的例子中，friend和friendly具有相同的词根，have和has具有相同的词根。我们可以画出两个句子词汇的交集与并集情况，如图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/example.png"></p><p>对于上面两个句子，其Jaccard相似度为$$5&#x2F;(5+3+2)&#x3D;0.5$$，即两个句子词汇的交集5个词汇，并集10个词汇</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">jaccard_sim</span>(<span class="params">str1, str2</span>): </span><br><span class="line">    a = <span class="built_in">set</span>(str1.split()) </span><br><span class="line">    b = <span class="built_in">set</span>(str2.split())</span><br><span class="line">    c = a.intersection(b)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(<span class="built_in">len</span>(c)) / (<span class="built_in">len</span>(a) + <span class="built_in">len</span>(b) - <span class="built_in">len</span>(c))</span><br></pre></td></tr></table></figure><p>值得注意的是，句子1中包含了两个friend，但这并不影响我们计算相似度，但这会影响cosine相似度。先让我们回忆一下cosine相似度的定义，公式如下<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/cosine.png"></p><h2 id="Cosine相似度"><a href="#Cosine相似度" class="headerlink" title="Cosine相似度"></a>Cosine相似度</h2><p>cosine相似度是通过计算两个向量之间的夹角，来评价两个向量的相似度。</p><p>既然cosine相似度是使用向量计算的，我们就要先将句子文本转换为相应的向量。将句子转换为向量的方式有很多，最简单的一种就是使用bag of words计算的TF(term frequency)和TF-IDF（term frenquency-inverse document frequency）。哪一钟转换方法更好呢？实际上，两个方法各有各的应用场景。当我们要大概估计文本相似度时，使用TF就可以了。当我们使用文本相似度进行检索的类似场景时（如搜索引擎中的query relevence的计算），此时TF-IDF更好一些。</p><p>当然，我们也可以使用word2vec或者使用自定义的词向量来讲句子转换成向量。这里简单介绍一下tf-idf和word embedding的异同： </p><ul><li><ol><li>tf&#x2F;tf-idf为每一个词汇计算得到一个数字，而word embedding将词汇表示成向量</li></ol></li><li><ol start="2"><li>tf&#x2F;tf-idf在文本分类的任务中表现更好一些，而word embedding的方法更适用于来判断上下文的语义信息（这可能是由word embedding的计算方法决定的）</li></ol></li></ul><p>对于如何计算cosine similarity，我们还是试用上面的例子，计算cosine similarity的过程，分为以下几个步骤：</p><ol><li><p>使用bag of words的方式计算term frequency，下图展示了word frequency的统计结果。<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/cosine1.png" alt="Term Frequency after lemmatization of the two sentences"></p></li><li><p>term frequency的问题在于，较长的句子里的词汇term frequency会更高一些。为了解决这个问题，我们可以使用归一化的方法（Normlization，如L2-norm）来去掉句子长度的影响。操作如下：首先对各个词汇的frequency平方求和，然后再开方。如果使用L2-norm，那么句子1的值为3.3166，而句子2的值为2.6458。用每一个词的term frquency除以这些norm的值，就可以得到如下结果：<br><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/cosine2.png" alt="Normalization of term frequencies using L2 Norms"></p></li><li><p>上一步中，我们将句子向量的模归一化为1，就可以受用点乘的方法计算得到cosine相似度： $$Cosine Similarity &#x3D; (0.3020.378) + (0.6030.378) + (0.3020.378) + (0.3020.378) + (0.302*0.378) &#x3D; 0.684$$</p></li></ol><p>所以两个句子的cosine相似度为0.684，而Jaccard相似度的结果是0.5。计算cosine相似度的python代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_cosine_sim</span>(<span class="params">*strs</span>): </span><br><span class="line">    vectors = [t <span class="keyword">for</span> t <span class="keyword">in</span> get_vectors(*strs)]</span><br><span class="line">    <span class="keyword">return</span> cosine_similarity(vectors)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_vectors</span>(<span class="params">*strs</span>):</span><br><span class="line">    text = [t <span class="keyword">for</span> t <span class="keyword">in</span> strs]</span><br><span class="line">    vectorizer = CountVectorizer(text)</span><br><span class="line">    vectorizer.fit(text)</span><br><span class="line">    <span class="keyword">return</span> vectorizer.transform(text).toarray()</span><br></pre></td></tr></table></figure><p>总结一下，Jaccard和cosine相似度的区别是什么呢？应该有以下几点：</p><ul><li><p>Jaccard使用的是集合操作，句子的向量长度由两个句子中unique的词汇数目决定，而cosine相似度使用的向量大小由词向量的维度决定。</p></li><li><p>上面的结论意味着什么呢？假设friend这个词汇在句子1中重复了非常多次，cosine相似度会发生变化，而Jaccard相似度的值不会变。让我们做个简单的计算，若句子1中的friend一词重复50次，cosine相似度会降为0.4，而Jaccard相似度保持0.5不变。</p></li><li><p>基于上述的结论，Jaccard相似度适用于什么场景呢？假设某个业务场景的文本包含了很多重复性的词汇，而这些重复是否与我们想做的任务关系不大，那么在分析文本相似度时，使用Jaccard计算相似度即可，因为对于Jaccard相似度来说，重复不会产生影响；假设这种重复对我们想做的任务影响很大，那么就要使用cosine相似度。</p></li></ul><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50">Overview of Text Similarity Metrics in Python</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Jaccard相似度&quot;&gt;&lt;a href=&quot;#Jaccard相似度&quot; class=&quot;headerlink&quot; title=&quot;Jaccard相似度&quot;&gt;&lt;/a&gt;Jaccard相似度&lt;/h2&gt;&lt;p&gt;Jaccard相似度的定义很简单，两个句子词汇的交集size除以两个句子词汇</summary>
      
    
    
    
    <category term="相似度算法" scheme="https://www.songzj.com/categories/%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="算法" scheme="https://www.songzj.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="文本相似度" scheme="https://www.songzj.com/tags/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>Mac配置多个Github账号</title>
    <link href="https://www.songzj.com/posts/35811c74/"/>
    <id>https://www.songzj.com/posts/35811c74/</id>
    <published>2021-08-12T01:52:44.000Z</published>
    <updated>2021-08-13T01:52:44.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>用Github绑定域名后博客网站有时会挂掉，还是原生的稳定些，因为一个Github账号只能部署一个Github Pages，所以就想把博客部署在不同的Github。<br>但是电脑只能有一个Github作为默认账号，用非默认账号提交代码会出现<code>Permission denied</code>错误。<br>那要怎么实现多个Github同时存在呢？</p><h3 id="生成ssh-key并上传Github"><a href="#生成ssh-key并上传Github" class="headerlink" title="生成ssh key并上传Github"></a>生成<code>ssh key</code>并上传Github</h3><p>这个就太简单了，不想写，百度一下教程多的是，这里是<a href="https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#adding-your-ssh-key-to-the-ssh-agent">Github官方教程</a></p><h3 id="x2F-ssh-x2F-config文件配置"><a href="#x2F-ssh-x2F-config文件配置" class="headerlink" title="~&#x2F;.ssh&#x2F;config文件配置"></a>~&#x2F;.ssh&#x2F;config文件配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">主Github账号，默认即可</span></span><br><span class="line">Host github.com</span><br><span class="line">HostName github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">第二个Github账号的配置，需要加上自己的用户名</span></span><br><span class="line">Host iszj.github.com</span><br><span class="line">HostName github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa_iszj</span><br></pre></td></tr></table></figure><h3 id="测试是否配置成功"><a href="#测试是否配置成功" class="headerlink" title="测试是否配置成功"></a>测试是否配置成功</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">主账号测试命令</span></span><br><span class="line">ssh -T git@github.com</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">次账号测试命令</span></span><br><span class="line">ssh -T git@iszj.github.com</span><br></pre></td></tr></table></figure><p>看到下面的输出就配置成功了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi szj2ys! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure><p>如果这时去主账号的项目<code>git push</code>是可以上传的，但是在次账号的项目还是会还是会报错</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">identity_sign: private key /Users/songzhijun/.ssh/id_rsa contents do not match public</span><br><span class="line">git@github.com: Permission denied (publickey).</span><br><span class="line">fatal: Could not read from remote repository.</span><br><span class="line"></span><br><span class="line">Please make sure you have the correct access rights</span><br><span class="line">and the repository exists.</span><br></pre></td></tr></table></figure><p>可以看到，它还是去找的主账号的<code>ssh key</code>，所以我们就需要让<code>git</code>找到项目对应的账号，做法是这样</p><h3 id="查看项目采用的提交方式"><a href="#查看项目采用的提交方式" class="headerlink" title="查看项目采用的提交方式"></a>查看项目采用的提交方式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure><p>如果是<code>https</code>需要换成<code>ssh</code>的方式</p><h3 id="修改提交方式"><a href="#修改提交方式" class="headerlink" title="修改提交方式"></a>修改提交方式</h3><p>先删除原来的提交方式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br></pre></td></tr></table></figure><p>添加新的ssh提交方式，到github官网获取项目的ssh链接<br>主账号可以用这种方式，也可以不用<code>ssh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:xxx/test.git</span><br></pre></td></tr></table></figure><p>次账号需要在<code>@</code>后面加上你的用户名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@xxx.github.com:xxx/test.git</span><br></pre></td></tr></table></figure><p>再<code>git remote -v</code>看看提交方式就变过来了，也就是说我们提交的时候可以找到对应的<code>ssh key</code>了，但如果这是你<code>git push</code>绝逼不成功，因为<code>git remote rm origin</code>把我们的项目和远程的关联解除了，所以需要设置一下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push --set-upstream origin master</span><br></pre></td></tr></table></figure><p>两个Github就完美的融合在你的电脑了</p><h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>最后，再来解决一下我们的需求，把博客部署在多个<code>Github Pages</code><br>在<code>hexo</code>部署博客的时候配置一下<code>_config.yml</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo:</span><br><span class="line">    # 主账号Github</span><br><span class="line">    - https://github.com/szj2ys/szj2ys.github.io.git</span><br><span class="line">    # 次账号Github，必须用ssh方式</span><br><span class="line">    - git@iszj.github.com:iszj/iszj.github.io.git</span><br></pre></td></tr></table></figure><p>好啦，这下博客就会一次部署到多个Github，再也不怕网站打不开了，一句话舒爽顺滑，哈哈哈哈哈😂🤣✌️😃</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h3&gt;&lt;p&gt;用Github绑定域名后博客网站有时会挂掉，还是原生的稳定些，因为一个Github账号只能部署一个Github Pages，所以就想把博客部</summary>
      
    
    
    
    <category term="教程" scheme="https://www.songzj.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="教程" scheme="https://www.songzj.com/tags/%E6%95%99%E7%A8%8B/"/>
    
    <category term="Github" scheme="https://www.songzj.com/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Python项目发布到pypi</title>
    <link href="https://www.songzj.com/posts/6c3d97ca/"/>
    <id>https://www.songzj.com/posts/6c3d97ca/</id>
    <published>2021-08-01T07:03:15.000Z</published>
    <updated>2021-08-01T07:03:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先注册pypi，并生成token</p><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>在家目录下创建twine配置文件.pypirc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.pypirc</span><br></pre></td></tr></table></figure><p>填充如下内容，下面你只需要把password替换成你在pypi生成的token</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[distutils]</span><br><span class="line">index-servers =</span><br><span class="line">  pypi</span><br><span class="line"></span><br><span class="line">[pypi]</span><br><span class="line">repository=https://upload.pypi.org/legacy/</span><br><span class="line">username=__token__</span><br><span class="line">password=pypi-AgEIcHlwaS5vcmcCJDJlNmNjOTRlLTg4MDgtNDQ1YS04ODIxLTI1YTg4NmI5ZDE2YxACJXsicGVybWlzc2lbbnMiOiAidXNlciIsICJ26XJzdfsdfsfdsfdsfX0AAAYgVJykZg2EBQ_QsLIQ-_ntCGdnRkFSEm3Tz_8_pIZbEgA</span><br></pre></td></tr></table></figure><h2 id="打包上传文件"><a href="#打包上传文件" class="headerlink" title="打包上传文件"></a>打包上传文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python setup.py build</span><br><span class="line">twine upload dist/*</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;首先注册pypi，并生成token&lt;/p&gt;
&lt;h2 id=&quot;配置文件&quot;&gt;&lt;a href=&quot;#配置文件&quot; class=&quot;headerlink&quot; title=&quot;配置文件&quot;&gt;&lt;/a&gt;配置文件&lt;/h2&gt;&lt;p&gt;在家目录下创建twine配置文件.pypirc&lt;/p&gt;
&lt;figure c</summary>
      
    
    
    
    <category term="Python教程" scheme="https://www.songzj.com/categories/Python%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Python" scheme="https://www.songzj.com/tags/Python/"/>
    
    <category term="pypi" scheme="https://www.songzj.com/tags/pypi/"/>
    
    <category term="项目发布" scheme="https://www.songzj.com/tags/%E9%A1%B9%E7%9B%AE%E5%8F%91%E5%B8%83/"/>
    
  </entry>
  
  <entry>
    <title>Python程序用pyinstaller打包成app或exe文件</title>
    <link href="https://www.songzj.com/posts/40aa2654/"/>
    <id>https://www.songzj.com/posts/40aa2654/</id>
    <published>2021-08-01T06:54:30.000Z</published>
    <updated>2021-08-01T06:54:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>当你写好了Python程序，想把自己做的程序发布给用户使用的时候，你会面对这样的问题：</p><ul><li>不可能你的用户都是懂Python编程的人，你还要照顾那些普通用户</li><li>因为工具用了一些第三方库，不可能在每台电脑上对这些库逐一进行安装</li></ul><p>如果你的程序任何人只要双击就能用，把你写出来的bugs从你自己的电脑中解放出来供全人类使用，是不是很棒？<br><a href="https://pyinstaller.readthedocs.io/en/latest/usage.html">pyinstaller</a>就是这么个帮你解决问题的帮手，他会帮你把需要的环境打包好封装到一个独立的应用中，让别人无忧使用。</p><h2 id="准备材料"><a href="#准备材料" class="headerlink" title="准备材料"></a>准备材料</h2><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><p>我们准备画一只小猪的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> turtle <span class="keyword">as</span> tu</span><br><span class="line"></span><br><span class="line">tu.pensize(<span class="number">4</span>)  <span class="comment"># 设置画笔的大小</span></span><br><span class="line">tu.colormode(<span class="number">255</span>)  <span class="comment"># 设置GBK颜色范围为0-255</span></span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>), <span class="string">&quot;pink&quot;</span>)  <span class="comment"># 设置画笔颜色和填充颜色(pink)</span></span><br><span class="line">tu.setup(<span class="number">850</span>, <span class="number">500</span>)  <span class="comment"># 设置主窗口的大小为850*500</span></span><br><span class="line">tu.speed(<span class="number">10</span>)  <span class="comment"># 设置画笔速度为10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画鼻子部分</span></span><br><span class="line">tu.pu()  <span class="comment"># 提笔</span></span><br><span class="line">tu.goto(-<span class="number">100</span>, <span class="number">100</span>)  <span class="comment"># 画笔前往坐标(-100,100)</span></span><br><span class="line">tu.pd()  <span class="comment"># 下笔</span></span><br><span class="line">tu.seth(-<span class="number">30</span>)  <span class="comment"># 笔的角度为-30°</span></span><br><span class="line">tu.begin_fill()  <span class="comment"># 外形填充的开始标志</span></span><br><span class="line">a = <span class="number">0.4</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">120</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="number">0</span> &lt;= i &lt; <span class="number">30</span> <span class="keyword">or</span> <span class="number">60</span> &lt;= i &lt; <span class="number">90</span>:</span><br><span class="line">        a = a + <span class="number">0.08</span></span><br><span class="line">        tu.lt(<span class="number">3</span>)  <span class="comment"># 向左转3度</span></span><br><span class="line">        tu.fd(a)  <span class="comment"># 向前走a的步长</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        a = a - <span class="number">0.08</span></span><br><span class="line">        tu.lt(<span class="number">3</span>)</span><br><span class="line">        tu.fd(a)</span><br><span class="line">tu.end_fill()  <span class="comment"># 依据轮廓填充</span></span><br><span class="line">tu.pu()  <span class="comment"># 提笔</span></span><br><span class="line">tu.seth(<span class="number">90</span>)  <span class="comment"># 笔的角度为90度</span></span><br><span class="line">tu.fd(<span class="number">25</span>)  <span class="comment"># 向前移动25</span></span><br><span class="line">tu.seth(<span class="number">0</span>)  <span class="comment"># 转换画笔的角度为0</span></span><br><span class="line">tu.fd(<span class="number">10</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.pencolor(<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>)  <span class="comment"># 设置画笔颜色</span></span><br><span class="line">tu.seth(<span class="number">10</span>)</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.circle(<span class="number">5</span>)  <span class="comment"># 画一个半径为5的圆</span></span><br><span class="line">tu.color(<span class="number">160</span>, <span class="number">82</span>, <span class="number">45</span>)  <span class="comment"># 设置画笔和填充颜色</span></span><br><span class="line">tu.end_fill()</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">20</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.pencolor(<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>)</span><br><span class="line">tu.seth(<span class="number">10</span>)</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.circle(<span class="number">5</span>)</span><br><span class="line">tu.color(<span class="number">160</span>, <span class="number">82</span>, <span class="number">45</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画头部</span></span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>), <span class="string">&quot;pink&quot;</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">41</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">0</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.seth(<span class="number">180</span>)</span><br><span class="line">tu.circle(<span class="number">300</span>, -<span class="number">30</span>)  <span class="comment"># 顺时针画一个半径为300,圆心角为30°的园</span></span><br><span class="line">tu.circle(<span class="number">100</span>, -<span class="number">60</span>)</span><br><span class="line">tu.circle(<span class="number">80</span>, -<span class="number">100</span>)</span><br><span class="line">tu.circle(<span class="number">150</span>, -<span class="number">20</span>)</span><br><span class="line">tu.circle(<span class="number">60</span>, -<span class="number">95</span>)</span><br><span class="line">tu.seth(<span class="number">161</span>)</span><br><span class="line">tu.circle(-<span class="number">300</span>, <span class="number">15</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.goto(-<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">30</span>)</span><br><span class="line">a = <span class="number">0.4</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="number">0</span> &lt;= i &lt; <span class="number">30</span> <span class="keyword">or</span> <span class="number">60</span> &lt;= i &lt; <span class="number">90</span>:</span><br><span class="line">        a = a + <span class="number">0.08</span></span><br><span class="line">        tu.lt(<span class="number">3</span>)  <span class="comment"># 向左转3度</span></span><br><span class="line">        tu.fd(a)  <span class="comment"># 向前走a的步长</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        a = a - <span class="number">0.08</span></span><br><span class="line">        tu.lt(<span class="number">3</span>)</span><br><span class="line">        tu.fd(a)</span><br><span class="line">tu.end_fill()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画耳朵</span></span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>), <span class="string">&quot;pink&quot;</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">7</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">70</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.seth(<span class="number">100</span>)</span><br><span class="line">tu.circle(-<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line">tu.circle(-<span class="number">10</span>, <span class="number">120</span>)</span><br><span class="line">tu.circle(-<span class="number">50</span>, <span class="number">54</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">12</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">30</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.seth(<span class="number">100</span>)</span><br><span class="line">tu.circle(-<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line">tu.circle(-<span class="number">10</span>, <span class="number">120</span>)</span><br><span class="line">tu.circle(-<span class="number">50</span>, <span class="number">56</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画眼睛</span></span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>), <span class="string">&quot;white&quot;</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">20</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(-<span class="number">95</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.circle(<span class="number">15</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line">tu.color(<span class="string">&quot;black&quot;</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">12</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(-<span class="number">3</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.circle(<span class="number">3</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>), <span class="string">&quot;white&quot;</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">25</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">40</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.circle(<span class="number">15</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line">tu.color(<span class="string">&quot;black&quot;</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">12</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(-<span class="number">3</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.circle(<span class="number">3</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画腮</span></span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>))</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">95</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">65</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.circle(<span class="number">30</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画嘴</span></span><br><span class="line">tu.color(<span class="number">239</span>, <span class="number">69</span>, <span class="number">19</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">15</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(-<span class="number">100</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">80</span>)</span><br><span class="line">tu.circle(<span class="number">30</span>, <span class="number">40</span>)</span><br><span class="line">tu.circle(<span class="number">40</span>, <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画身体</span></span><br><span class="line">tu.color(<span class="string">&quot;red&quot;</span>, (<span class="number">255</span>, <span class="number">99</span>, <span class="number">71</span>))</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">20</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(-<span class="number">78</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.begin_fill()</span><br><span class="line">tu.seth(-<span class="number">130</span>)</span><br><span class="line">tu.circle(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">tu.circle(<span class="number">300</span>, <span class="number">30</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">230</span>)</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.circle(<span class="number">300</span>, <span class="number">30</span>)</span><br><span class="line">tu.circle(<span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>), (<span class="number">255</span>, <span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line">tu.seth(-<span class="number">135</span>)</span><br><span class="line">tu.circle(-<span class="number">80</span>, <span class="number">63</span>)</span><br><span class="line">tu.circle(-<span class="number">150</span>, <span class="number">24</span>)</span><br><span class="line">tu.end_fill()</span><br><span class="line"><span class="comment"># 手</span></span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>))</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">40</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(-<span class="number">27</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">160</span>)</span><br><span class="line">tu.circle(<span class="number">300</span>, <span class="number">15</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">15</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">0</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">10</span>)</span><br><span class="line">tu.circle(-<span class="number">20</span>, <span class="number">90</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">30</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">237</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">20</span>)</span><br><span class="line">tu.circle(-<span class="number">300</span>, <span class="number">15</span>)</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">20</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">0</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">170</span>)</span><br><span class="line">tu.circle(<span class="number">20</span>, <span class="number">90</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画脚</span></span><br><span class="line">tu.pensize(<span class="number">10</span>)</span><br><span class="line">tu.color((<span class="number">240</span>, <span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(-<span class="number">75</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(-<span class="number">180</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">40</span>)</span><br><span class="line">tu.seth(-<span class="number">180</span>)</span><br><span class="line">tu.color(<span class="string">&quot;black&quot;</span>)</span><br><span class="line">tu.pensize(<span class="number">15</span>)</span><br><span class="line">tu.fd(<span class="number">20</span>)</span><br><span class="line">tu.pensize(<span class="number">10</span>)</span><br><span class="line">tu.color((<span class="number">240</span>, <span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">40</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">90</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(-<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">40</span>)</span><br><span class="line">tu.seth(-<span class="number">180</span>)</span><br><span class="line">tu.color(<span class="string">&quot;black&quot;</span>)</span><br><span class="line">tu.pensize(<span class="number">15</span>)</span><br><span class="line">tu.fd(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画尾巴</span></span><br><span class="line">tu.pensize(<span class="number">4</span>)</span><br><span class="line">tu.color((<span class="number">255</span>, <span class="number">155</span>, <span class="number">192</span>))</span><br><span class="line">tu.pu()</span><br><span class="line">tu.seth(<span class="number">90</span>)</span><br><span class="line">tu.fd(<span class="number">70</span>)</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.fd(<span class="number">95</span>)</span><br><span class="line">tu.pd()</span><br><span class="line">tu.seth(<span class="number">0</span>)</span><br><span class="line">tu.circle(<span class="number">70</span>, <span class="number">20</span>)</span><br><span class="line">tu.circle(<span class="number">10</span>, <span class="number">330</span>)</span><br><span class="line">tu.circle(<span class="number">70</span>, <span class="number">30</span>)</span><br></pre></td></tr></table></figure><h3 id="图标图片"><a href="#图标图片" class="headerlink" title="图标图片"></a>图标图片</h3><p>打包后应用的图标</p><p><img src= "/img/loading.gif" data-lazy-src="https://gitee.com/szj2ys/Pictures/raw/master/program/pig.jpg" alt="图标图片"></p><h2 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller -i pig.jpg -Fw pig.py</span><br></pre></td></tr></table></figure><p>pyinstaller会根据你打包的平台打成对应的包，比如你在Windows执行的命令，pyinstaller就会打包成.exe文件，如果你在Mac打包，结果就是.app文件，文件会出现在工程文件目录下的 dist 文件夹中，大功告成！好好享受吧~</p><h2 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a><a href="https://pyinstaller.readthedocs.io/en/latest/spec-files.html">高级用法</a></h2><h3 id="添加外部数据"><a href="#添加外部数据" class="headerlink" title="添加外部数据"></a>添加外部数据</h3><p>如果你的代码读取了外部数据集，pyinstaller不会自动帮你打包，这是你需要在.spec文件a&#x3D;Analysis中配置datas：<br>例如:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">datas=[</span><br><span class="line">                 ( &#x27;data/*&#x27;, &#x27;data&#x27; ),</span><br><span class="line">                 ( &#x27;data/chromedrivers/*&#x27;, &#x27;data/chromedrivers&#x27; ),</span><br><span class="line">             ],</span><br></pre></td></tr></table></figure><blockquote><p>*表示所有文件<br>()中左边是源文件地址，可以写绝对地址，也可以写当前项目的相对地址；右边是打包后包里面的地址，建议和相对地址相同</p></blockquote><p>重点来了，改过之后，重新打包运行的命令就有所区别，之前是<code>pyinstaller -Fw xx.py</code>，现在我们需要运行命令<code>pyinstaller -Fw xx.spec</code>，否则pyinstaller会重新生成xx.spec文件把我们修改的数据覆盖</p><h3 id="常用参数解释"><a href="#常用参数解释" class="headerlink" title="常用参数解释"></a>常用参数解释</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller -F 文件.py # 生成单个可执行文件</span><br><span class="line">pyinstaller -w 文件.py # 去掉控制台窗口，对于执行文件没有多大的用处，一般用于GUI面板代码文件</span><br><span class="line">pyinstaller -i 图标路径 # 表示可执行文件的图标</span><br><span class="line">pyinstaller -c # 使用控制台无窗口</span><br><span class="line">pyinstaller -D # 生成一个文件夹包括依赖文件</span><br><span class="line">pyinstaller -K # 当包含tcl和tk也就是使用tkinter时加上-K参数</span><br></pre></td></tr></table></figure><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://blog.csdn.net/weixin_42052836/article/details/82315118?utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link">Pyinstaller 打包发布经验总结</a></li><li><a href="https://github.com/szj2ys/py2app">py2app</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;当你写好了Python程序，想把自己做的程序发布给用户使用的时候，你会面对这样的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不可能你的用户都是懂Python编程的人，你还要照顾那些普通用户&lt;/li&gt;
&lt;li&gt;因为工具用了一些第三方库，不可能在每台电脑上对这些库逐一进行安装&lt;/li&gt;
</summary>
      
    
    
    
    <category term="Python教程" scheme="https://www.songzj.com/categories/Python%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Python" scheme="https://www.songzj.com/tags/Python/"/>
    
    <category term="pyinstaller" scheme="https://www.songzj.com/tags/pyinstaller/"/>
    
  </entry>
  
  <entry>
    <title>Centos普通用户配置免密登陆</title>
    <link href="https://www.songzj.com/posts/871dc6a0/"/>
    <id>https://www.songzj.com/posts/871dc6a0/</id>
    <published>2021-08-01T06:51:16.000Z</published>
    <updated>2021-08-01T06:51:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>在Linux里当我们需要执行一条root权限的命令时，每次都要用sudo命令然后再确认密码，非常不方便。那么我们修改配置sudo免密。默认新建的用户不在sudo组，但可以编辑&#x2F;etc&#x2F;sudoers文件将普通用户加入sudo组。要注意的是修改该文件需要切换到root用户<br>使用命令 vi &#x2F;etc&#x2F;sudoers修改配置文件，将下列第三或第四行添加到文件中</p><ul><li>youuser ALL&#x3D;(ALL) ALL</li><li>%youuser ALL&#x3D;(ALL) ALL</li><li>youuser ALL&#x3D;(ALL) NOPASSWD: ALL</li><li>%youuser ALL&#x3D;(ALL) NOPASSWD: ALL</li></ul><blockquote><p>第一行:允许用户youuser执行sudo命令(需要输入密码).<br>第二行:允许用户组youuser里面的用户执行sudo命令(需要输入密码).<br>第三行:允许用户youuser执行sudo命令,并且在执行的时候不输入密码.<br>第四行:允许用户组youuser里面的用户执行sudo命令,并且在执行的时候不输入密码.</p></blockquote><p>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root       ALL=(ALL)       ALL</span><br><span class="line">%test      ALL=(ALL)       NOPASSWD: ALL</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>保存退出后，切换到普通用户，使用sudo命令再也不用输入密码确认了，是不是很方便:)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在Linux里当我们需要执行一条root权限的命令时，每次都要用sudo命令然后再确认密码，非常不方便。那么我们修改配置sudo免密。默认新建的用户不在sudo组，但可以编辑&amp;#x2F;etc&amp;#x2F;sudoers文件将普通用户加入sudo组。要注意的是修改该文件需要切</summary>
      
    
    
    
    <category term="linux" scheme="https://www.songzj.com/categories/linux/"/>
    
    
    <category term="教程" scheme="https://www.songzj.com/tags/%E6%95%99%E7%A8%8B/"/>
    
    <category term="shell" scheme="https://www.songzj.com/tags/shell/"/>
    
    <category term="linux" scheme="https://www.songzj.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Shell单引号和双引号区别</title>
    <link href="https://www.songzj.com/posts/c7921ac5/"/>
    <id>https://www.songzj.com/posts/c7921ac5/</id>
    <published>2021-08-01T06:47:26.000Z</published>
    <updated>2021-08-01T06:47:26.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim test.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">do_date=$1  </span><br><span class="line"></span><br><span class="line">echo &#x27;$do_date&#x27;  # 英文字母的单引号 $do_date</span><br><span class="line">echo &quot;$do_date&quot;  # 英文字母的双引号  2019-02-10</span><br><span class="line">echo &quot;&#x27;$do_date&#x27;&quot;  # 英文字母的双引号包着单引号 &#x27;2019-02-10&#x27;</span><br><span class="line">echo &#x27;&quot;$do_date&quot;&#x27;   # 英文字母的单引号包着双引号 &quot;$do_date&quot;</span><br><span class="line">echo `date`   # 数字1旁边的反引号 可以写一些命令会把结果返回</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.sh 2019-02-10</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">do_date</span></span><br><span class="line">2019-02-10</span><br><span class="line">&#x27;2019-02-10&#x27;</span><br><span class="line">&quot;$do_date&quot;</span><br><span class="line">2019年 05月 02日 星期四 21:02:08 CST</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span cla</summary>
      
    
    
    
    <category term="Shell" scheme="https://www.songzj.com/categories/Shell/"/>
    
    
    <category term="教程" scheme="https://www.songzj.com/tags/%E6%95%99%E7%A8%8B/"/>
    
    <category term="Shell" scheme="https://www.songzj.com/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>wget命令高级用法总结</title>
    <link href="https://www.songzj.com/posts/92b0ce0c/"/>
    <id>https://www.songzj.com/posts/92b0ce0c/</id>
    <published>2021-08-01T06:43:33.000Z</published>
    <updated>2021-08-01T06:43:33.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本格式"><a href="#基本格式" class="headerlink" title="基本格式"></a>基本格式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget  [参数]   [URL地址]</span><br></pre></td></tr></table></figure><h2 id="命令参数："><a href="#命令参数：" class="headerlink" title="命令参数："></a>命令参数：</h2><h4 id="启动参数："><a href="#启动参数：" class="headerlink" title="启动参数："></a>启动参数：</h4><p>-V, –version 显示wget的版本后退出</p><p>-h, –help 打印语法帮助</p><p>-b, –background 启动后转入后台执行</p><p>-e, –execute&#x3D;COMMAND 执行&#96;.wgetrc’格式的命令，wgetrc格式参见&#x2F;etc&#x2F;wgetrc或~&#x2F;.wgetrc</p><h4 id="记录和输入文件参数："><a href="#记录和输入文件参数：" class="headerlink" title="记录和输入文件参数："></a>记录和输入文件参数：</h4><p>-o, –output-file&#x3D;FILE 把记录写到FILE文件中</p><p>-a, –append-output&#x3D;FILE 把记录追加到FILE文件中</p><p>-d, –debug 打印调试输出</p><p>-q, –quiet 安静模式(没有输出)</p><p>-v, –verbose 冗长模式(这是缺省设置)</p><p>-nv, –non-verbose 关掉冗长模式，但不是安静模式</p><p>-i, –input-file&#x3D;FILE 下载在FILE文件中出现的URLs</p><p>-F, –force-html 把输入文件当作HTML格式文件对待</p><p>-B, –base&#x3D;URL 将URL作为在-F -i参数指定的文件中出现的相对链接的前缀</p><p>–sslcertfile&#x3D;FILE 可选客户端证书</p><p>–sslcertkey&#x3D;KEYFILE 可选客户端证书的KEYFILE</p><p>–egd-file&#x3D;FILE 指定EGD socket的文件名</p><h4 id="下载参数："><a href="#下载参数：" class="headerlink" title="下载参数："></a>下载参数：</h4><p>–bind-address&#x3D;ADDRESS 指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)</p><p>-t, –tries&#x3D;NUMBER 设定最大尝试链接次数(0 表示无限制).</p><p>-O –output-document&#x3D;FILE 把文档写到FILE文件中</p><p>-nc, –no-clobber 不要覆盖存在的文件或使用.#前缀</p><p>-c, –continue 接着下载没下载完的文件<br>–progress&#x3D;TYPE 设定进程条标记</p><p>-N, –timestamping 不要重新下载文件除非比本地文件新</p><p>-S, –server-response 打印服务器的回应</p><p>–spider 不下载任何东西</p><p>-T, –timeout&#x3D;SECONDS 设定响应超时的秒数</p><p>-w, –wait&#x3D;SECONDS 两次尝试之间间隔SECONDS秒</p><p>–waitretry&#x3D;SECONDS 在重新链接之间等待1…SECONDS秒</p><p>–random-wait 在下载之间等待0…2*WAIT秒</p><p>-Y, –proxy&#x3D;on&#x2F;off 打开或关闭代理</p><p>-Q, –quota&#x3D;NUMBER 设置下载的容量限制</p><p>–limit-rate&#x3D;RATE 限定下载输率</p><h4 id="目录参数："><a href="#目录参数：" class="headerlink" title="目录参数："></a>目录参数：</h4><p>-nd –no-directories 不创建目录</p><p>-x, –force-directories 强制创建目录</p><p>-nH, –no-host-directories 不创建主机目录</p><p>-P, –directory-prefix&#x3D;PREFIX 将文件保存到目录 PREFIX&#x2F;…</p><p>–cut-dirs&#x3D;NUMBER 忽略 NUMBER层远程目录</p><h4 id="HTTP-选项参数："><a href="#HTTP-选项参数：" class="headerlink" title="HTTP 选项参数："></a>HTTP 选项参数：</h4><p>–http-user&#x3D;USER 设定HTTP用户名为 USER.</p><p>–http-passwd&#x3D;PASS 设定http密码为 PASS</p><p>-C, –cache&#x3D;on&#x2F;off 允许&#x2F;不允许服务器端的数据缓存 (一般情况下允许)</p><p>-E, –html-extension 将所有text&#x2F;html文档以.html扩展名保存</p><p>–ignore-length 忽略 &#96;Content-Length’头域</p><p>–header&#x3D;STRING 在headers中插入字符串 STRING</p><p>–proxy-user&#x3D;USER 设定代理的用户名为 USER</p><p>–proxy-passwd&#x3D;PASS 设定代理的密码为 PASS</p><p>–referer&#x3D;URL 在HTTP请求中包含 &#96;Referer: URL’头</p><p>-s, –save-headers 保存HTTP头到文件</p><p>-U, –user-agent&#x3D;AGENT 设定代理的名称为 AGENT而不是 Wget&#x2F;VERSION</p><p>–no-http-keep-alive 关闭 HTTP活动链接 (永远链接)</p><p>–cookies&#x3D;off 不使用 cookies</p><p>–load-cookies&#x3D;FILE 在开始会话前从文件 FILE中加载cookie</p><p>–save-cookies&#x3D;FILE 在会话结束后将 cookies保存到 FILE文件中</p><h4 id="FTP-选项参数："><a href="#FTP-选项参数：" class="headerlink" title="FTP 选项参数："></a>FTP 选项参数：</h4><p>-nr, –dont-remove-listing 不移走 &#96;.listing’文件</p><p>-g, –glob&#x3D;on&#x2F;off 打开或关闭文件名的 globbing机制</p><p>–passive-ftp 使用被动传输模式 (缺省值).</p><p>–active-ftp 使用主动传输模式</p><p>–retr-symlinks 在递归的时候，将链接指向文件(而不是目录)</p><h4 id="递归下载参数："><a href="#递归下载参数：" class="headerlink" title="递归下载参数："></a>递归下载参数：</h4><p>-r, –recursive 递归下载－－慎用!</p><p>-l, –level&#x3D;NUMBER 最大递归深度 (inf 或 0 代表无穷)</p><p>–delete-after 在现在完毕后局部删除文件</p><p>-k, –convert-links 转换非相对链接为相对链接</p><p>-K, –backup-converted 在转换文件X之前，将之备份为 X.orig</p><p>-m, –mirror 等价于 -r -N -l inf -nr</p><p>-p, –page-requisites 下载显示HTML文件的所有图片</p><h4 id="递归下载中的包含和不包含-accept-x2F-reject-："><a href="#递归下载中的包含和不包含-accept-x2F-reject-：" class="headerlink" title="递归下载中的包含和不包含(accept&#x2F;reject)："></a>递归下载中的包含和不包含(accept&#x2F;reject)：</h4><p>-A, –accept&#x3D;LIST 分号分隔的被接受扩展名的列表</p><p>-R, –reject&#x3D;LIST 分号分隔的不被接受的扩展名的列表</p><p>-D, –domains&#x3D;LIST 分号分隔的被接受域的列表</p><p>–exclude-domains&#x3D;LIST 分号分隔的不被接受的域的列表</p><p>–follow-ftp 跟踪HTML文档中的FTP链接</p><p>–follow-tags&#x3D;LIST 分号分隔的被跟踪的HTML标签的列表</p><p>-G, –ignore-tags&#x3D;LIST 分号分隔的被忽略的HTML标签的列表</p><p>-H, –span-hosts 当递归时转到外部主机</p><p>-L, –relative 仅仅跟踪相对链接</p><p>-I, –include-directories&#x3D;LIST 允许目录的列表</p><p>-X, –exclude-directories&#x3D;LIST 不被包含目录的列表</p><p>-np, –no-parent 不要追溯到父目录</p><p>wget -S –spider url 不下载只显示过程</p><h2 id="使用实例："><a href="#使用实例：" class="headerlink" title="使用实例："></a>使用实例：</h2><h4 id="使用wget下载单个文件"><a href="#使用wget下载单个文件" class="headerlink" title="使用wget下载单个文件"></a>使用wget下载单个文件</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.minjieren.com/wordpress-3.1-zh_CN.zip</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。</p></blockquote><h4 id="使用wget-O下载并以不同的文件名保存"><a href="#使用wget-O下载并以不同的文件名保存" class="headerlink" title="使用wget -O下载并以不同的文件名保存"></a>使用wget -O下载并以不同的文件名保存</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -O wordpress.zip http://www.minjieren.com/download.aspx?id=1080</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>wget默认会以最后一个符合”&#x2F;”的后面的字符来命令，对于动态链接的下载通常文件名会不正确。<br>错误：下面的例子会下载一个文件并以名称download.aspx?id&#x3D;1080保存</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.minjieren.com/download?id=1</span><br></pre></td></tr></table></figure><blockquote><p>即使下载的文件是zip格式，它仍然以download.php?id&#x3D;1080命令。<br>正确：为了解决这个问题，我们可以使用参数-O来指定一个文件名：</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -O wordpress.zip http://www.minjieren.com/download.aspx?id=1080</span><br></pre></td></tr></table></figure><h4 id="使用wget-–limit-rate限速下载"><a href="#使用wget-–limit-rate限速下载" class="headerlink" title="使用wget –limit -rate限速下载"></a>使用wget –limit -rate限速下载</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --limit-rate=300k http://www.minjieren.com/wordpress-3.1-zh_CN.zip</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。</p></blockquote><h4 id="使用wget-c断点续传"><a href="#使用wget-c断点续传" class="headerlink" title="使用wget -c断点续传"></a>使用wget -c断点续传</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -c http://www.minjieren.com/wordpress-3.1-zh_CN.zip</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>使用wget -c重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c参数。</p></blockquote><h4 id="使用wget-b后台下载"><a href="#使用wget-b后台下载" class="headerlink" title="使用wget -b后台下载"></a>使用wget -b后台下载</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -b http://www.minjieren.com/wordpress-3.1-zh_CN.zip</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载。<br>wget -b <a href="http://www.minjieren.com/wordpress-3.1-zh_CN.zip">http://www.minjieren.com/wordpress-3.1-zh_CN.zip</a><br>你可以使用以下命令来察看下载进度：<br>tail -f wget-log</p></blockquote><h4 id="伪装代理名称下载"><a href="#伪装代理名称下载" class="headerlink" title="伪装代理名称下载"></a>伪装代理名称下载</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --user-agent=&quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16&quot; http://www.minjieren.com/wordpress-3.1-zh_CN.zip</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过–user-agent参数伪装。</p></blockquote><h4 id="使用wget-–tries增加重试次数"><a href="#使用wget-–tries增加重试次数" class="headerlink" title="使用wget –tries增加重试次数"></a>使用wget –tries增加重试次数</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --tries=40 URL</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用–tries增加重试次数。</p></blockquote><h4 id="使用wget-i下载多个文件"><a href="#使用wget-i下载多个文件" class="headerlink" title="使用wget -i下载多个文件"></a>使用wget -i下载多个文件</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -i filelist.txt</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>首先，保存一份下载链接文件<br>cat &gt; filelist.txt<br>url1<br>url2<br>url3<br>url4<br>接着使用这个文件和参数-i下载</p></blockquote><h4 id="使用wget-–mirror镜像网站"><a href="#使用wget-–mirror镜像网站" class="headerlink" title="使用wget –mirror镜像网站"></a>使用wget –mirror镜像网站</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --mirror -p --convert-links -P ./LOCAL URL</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>下载整个网站到本地。<br>–miror:开启镜像下载<br>-p:下载所有为了html页面显示正常的文件<br>–convert-links:下载后，转换成本地的链接<br>-P .&#x2F;LOCAL：保存所有文件和目录到本地指定目录</p></blockquote><h4 id="使用wget-–reject过滤指定格式下载"><a href="#使用wget-–reject过滤指定格式下载" class="headerlink" title="使用wget –reject过滤指定格式下载"></a>使用wget –reject过滤指定格式下载</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --reject=gif url</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>下载一个网站，但你不希望下载图片，可以使用以下命令。</p></blockquote><h4 id="使用wget-o把下载信息存入日志文件"><a href="#使用wget-o把下载信息存入日志文件" class="headerlink" title="使用wget -o把下载信息存入日志文件"></a>使用wget -o把下载信息存入日志文件</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -o download.log URL</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>不希望下载信息直接显示在终端而是在一个日志文件，可以使用</p></blockquote><h4 id="使用wget-Q限制总下载文件大小"><a href="#使用wget-Q限制总下载文件大小" class="headerlink" title="使用wget -Q限制总下载文件大小"></a>使用wget -Q限制总下载文件大小</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -Q5m -i filelist.txt</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。</p></blockquote><h4 id="使用wget-r-A下载指定格式文件"><a href="#使用wget-r-A下载指定格式文件" class="headerlink" title="使用wget -r -A下载指定格式文件"></a>使用wget -r -A下载指定格式文件</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -r -A.pdf url</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>可以在以下情况使用该功能：<br>下载一个网站的所有图片<br>下载一个网站的所有视频<br>下载一个网站的所有PDF文件</p></blockquote><h4 id="使用wget-FTP下载"><a href="#使用wget-FTP下载" class="headerlink" title="使用wget FTP下载"></a>使用wget FTP下载</h4><p>命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget ftp-url</span><br><span class="line"></span><br><span class="line">wget --ftp-user=USERNAME --ftp-password=PASSWORD url</span><br></pre></td></tr></table></figure><blockquote><p>说明：<br>可以使用wget来完成ftp链接的下载。<br>使用wget匿名ftp下载：<br>wget ftp-url<br>使用wget用户名和密码认证的ftp下载<br>wget --ftp-user&#x3D;USERNAME --ftp-password&#x3D;PASSWORD url</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本格式&quot;&gt;&lt;a href=&quot;#基本格式&quot; class=&quot;headerlink&quot; title=&quot;基本格式&quot;&gt;&lt;/a&gt;基本格式&lt;/h2&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;</summary>
      
    
    
    
    <category term="shell" scheme="https://www.songzj.com/categories/shell/"/>
    
    
    <category term="教程" scheme="https://www.songzj.com/tags/%E6%95%99%E7%A8%8B/"/>
    
    <category term="shell 命令" scheme="https://www.songzj.com/tags/shell-%E5%91%BD%E4%BB%A4/"/>
    
    <category term="wget" scheme="https://www.songzj.com/tags/wget/"/>
    
  </entry>
  
  <entry>
    <title>大数据集群操作脚本案例</title>
    <link href="https://www.songzj.com/posts/7f567768/"/>
    <id>https://www.songzj.com/posts/7f567768/</id>
    <published>2021-08-01T06:32:47.000Z</published>
    <updated>2021-08-01T06:32:47.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Zookeeper启停脚本"><a href="#Zookeeper启停脚本" class="headerlink" title="Zookeeper启停脚本"></a>Zookeeper启停脚本</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.4.10/bin/zkServer.sh start&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;status&quot;)&#123;</span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">ssh $i &quot;/opt/module/zookeeper-3.4.10/bin/zkServer.sh status&quot;</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="集群所有进程查看脚本"><a href="#集群所有进程查看脚本" class="headerlink" title="集群所有进程查看脚本"></a>集群所有进程查看脚本</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim xcall.sh</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">        echo --------- $i ----------</span><br><span class="line">        ssh $i &quot;$*&quot;</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 777 xcall.sh</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcall.sh jps</span><br></pre></td></tr></table></figure><h2 id="Flume启动停止脚本"><a href="#Flume启动停止脚本" class="headerlink" title="Flume启动停止脚本"></a>Flume启动停止脚本</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103</span><br><span class="line">        do</span><br><span class="line">                echo &quot; --------启动 $i 采集flume-------&quot;</span><br><span class="line">                ssh $i &quot;nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103</span><br><span class="line">        do</span><br><span class="line">                echo &quot; --------停止 $i 采集flume-------&quot;</span><br><span class="line">                ssh $i &quot;ps -ef | grep file-flume-kafka | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs kill&quot;</span><br><span class="line">        done</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><ul><li>说明1：nohup，该命令可以在你退出帐户&#x2F;关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。</li><li>说明2：&#x2F;dev&#x2F;null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。<br>标准输入0：从键盘获得输入 &#x2F;proc&#x2F;self&#x2F;fd&#x2F;0<br>标准输出1：输出到屏幕（即控制台） &#x2F;proc&#x2F;self&#x2F;fd&#x2F;1<br>错误输出2：输出到屏幕（即控制台） &#x2F;proc&#x2F;self&#x2F;fd&#x2F;2</li></ul></blockquote><h2 id="Hive加载数据脚本"><a href="#Hive加载数据脚本" class="headerlink" title="Hive加载数据脚本"></a>Hive加载数据脚本</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># 定义变量方便修改</span><br><span class="line">APP=gmall</span><br><span class="line">hive=/opt/module/hive/bin/hive</span><br><span class="line"></span><br><span class="line"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   do_date=$1</span><br><span class="line">else </span><br><span class="line">   do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">echo &quot;===日志日期为 $do_date===&quot;</span><br><span class="line">sql=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/gmall/log/topic_start/$do_date&#x27; into table &quot;$APP&quot;.ods_start_log partition(dt=&#x27;$do_date&#x27;);</span><br><span class="line"></span><br><span class="line">load data inpath &#x27;/origin_data/gmall/log/topic_event/$do_date&#x27; into table &quot;$APP&quot;.ods_event_log partition(dt=&#x27;$do_date&#x27;);</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">$hive -e &quot;$sql&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>[ -n 变量值 ] 判断变量的值，是否为空<br>-- 变量的值，非空，返回true<br>-- 变量的值，为空，返回false</p></blockquote>]]></content>
    
    
    <summary type="html">大数据集群操作脚本案例</summary>
    
    
    
    <category term="Shell" scheme="https://www.songzj.com/categories/Shell/"/>
    
    
    <category term="Shell" scheme="https://www.songzj.com/tags/Shell/"/>
    
    <category term="脚本" scheme="https://www.songzj.com/tags/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="大数据" scheme="https://www.songzj.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>git push时403，解决思路与方法</title>
    <link href="https://www.songzj.com/posts/198a4223a/"/>
    <id>https://www.songzj.com/posts/198a4223a/</id>
    <published>2021-08-01T06:19:06.000Z</published>
    <updated>2021-08-01T06:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>用命令行<code>git push</code>代码的时候突然出现了这样的报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">remote: Password authentication is temporarily disabled as part of a brownout. Please use a personal access token instead.</span><br><span class="line">remote: Please see https://github.blog/2020-07-30-token-authentication-requirements-for-api-and-git-operations/ for more information.</span><br><span class="line">fatal: unable to access &#x27;https://github.com/szj2ys/funlp.git/&#x27;: The requested URL returned error: 403</span><br></pre></td></tr></table></figure><p>反复试了多次，检查网络是正常的，奇怪的是前几分钟我还成功向远程push过代码，怎么突然就不行了呢？</p><p>哪里出问题，就从哪里找原因，报错信息通常会给我们指出错误原因和解决途径。那我们就看下报错信息，已经告诉我们从哪里<br>着手，就是这个<a href="https://github.blog/2020-07-30-token-authentication-requirements-for-api-and-git-operations/">网址</a><br><code>https://github.blog/2020-07-30-token-authentication-requirements-for-api-and-git-operations/</code></p><p>原来是因为安全考虑，从2021年中开始，GitHub对使用<code>REST API</code>进行身份验证时将不再支持帐户密码，并要求对GitHub上所有经过身份验证的<code>API</code>操作使用基于<code>Token</code>的身份验证（例如，个人访问、 OAuth 或 GitHub App)。<br>这样做的好处: </p><ul><li>Token是特定于 GitHub 的，可以根据每次使用或每个设备生成</li><li>可以在任何时候单独撤销，而不需要更新凭据</li><li>可以限制范围，只允许用例授权范围内的访问</li><li>防止密码泄露</li></ul><p>既然原因找到了，那要怎么解决呢？<br>既然是由密码验证转换到Token验证，那就首先要搞一个Token，GitHub给了生成Token的<a href="https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line">教程</a>。照着教程一步步来就可以了。<br>获取到Token后，就要把Token放到正确的地方，放在哪呢？我也不知道，但是GitHub给出了提示，你可以通过<a href="https://docs.github.com/en/github/getting-started-with-github/updating-credentials-from-the-macos-keychain">这篇教程</a>把密码替换成Token。<br>好了，问题到这基本上就解决了。但是GitHub提示如果不想每次都手动认证一遍，你就需要设置缓存，教程在<a href="https://docs.github.com/en/github/getting-started-with-github/caching-your-github-credentials-in-git">这里</a>。</p><p>最后，再<code>git push</code>推送远程试下，发现顺利提交了，完美~</p>]]></content>
    
    
    <summary type="html">GitHub用户验证新规导致push时403，解决思路与方法</summary>
    
    
    
    <category term="GitHub" scheme="https://www.songzj.com/categories/GitHub/"/>
    
    
    <category term="问题解决" scheme="https://www.songzj.com/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"/>
    
    <category term="Github" scheme="https://www.songzj.com/tags/Github/"/>
    
  </entry>
  
</feed>
