<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>在下小宋 - Never settle!</title><meta name="keywords" content="hexo,butterfly,demo,hexo-butterfly,hexo-theme-butterfly"><meta name="author" content="在下小宋"><meta name="copyright" content="在下小宋"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="从来没有真正的绝境，只有心灵的迷途"><meta property="og:type" content="website"><meta property="og:title" content="在下小宋"><meta property="og:url" content="https://www.songzj.com/"><meta property="og:site_name" content="在下小宋"><meta property="og:description" content="从来没有真正的绝境，只有心灵的迷途"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.songzj.com/img/avatar.png"><meta property="article:author" content="在下小宋"><meta property="article:tag" content="hexo,butterfly,demo,hexo-butterfly,hexo-theme-butterfly"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://www.songzj.com/img/avatar.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.songzj.com/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="9pANgEo9T-01B-QGkl4lWtWC7-ehwydH6uVvfqtfLR0"><meta name="msvalidate.01" content="567E47D75E8DCF128d2B9623AD914701E"><meta name="baidu-site-verification" content="code-r1zCiXqhIw"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload='this.media="all"'><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-1902923403139213",enable_page_level_ads:"true"})</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:400},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:150,languages:{author:"作者: 在下小宋",link:"链接: ",source:"来源: 在下小宋",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"mediumZoom",Snackbar:void 0,source:{justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js",css:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!0}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"在下小宋",isPost:!1,isHome:!0,isHighlightShrink:!1,isToc:!1,postUpdate:"2022-04-21 15:54:34"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const a=864e5*o,c={value:t,expiry:(new Date).getTime()+a};localStorage.setItem(e,JSON.stringify(c))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const a=document.createElement("script");a.src=e,a.async=!0,a.onerror=o,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=window.matchMedia("(prefers-color-scheme: dark)").matches,a=window.matchMedia("(prefers-color-scheme: light)").matches,c=window.matchMedia("(prefers-color-scheme: no-preference)").matches,n=!o&&!a&&!c;if(void 0===t){if(a)activateLightMode();else if(o)activateDarkMode();else if(c||n){const e=(new Date).getHours();e<=6||e>=18?activateDarkMode():activateLightMode()}window.matchMedia("(prefers-color-scheme: dark)").addListener((function(e){void 0===saveToLocal.get("theme")&&(e.matches?activateDarkMode():activateLightMode())}))}else"light"===t?activateLightMode():activateDarkMode();const i=saveToLocal.get("aside-status");void 0!==i&&("hide"===i?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><style>.card-announcement .social-button{margin:.6rem 0 0 0;text-align:center}.card-announcement .social-button a{display:block;margin:.2rem 0;background-color:var(--btn-bg);color:var(--btn-color);line-height:1.6rem;transition:all .3s;position:relative;z-index:1}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,300&display=swap" rel="stylesheet"><link rel="alternate" href="/atom.xml" title="在下小宋" type="application/atom+xml"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">30</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-compass"></i> <span>目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-address-book"></i> <span>链接</span></a></div><div class="menus_item"><a class="site-page" href="/navigate/"><i class="fa-fw fas fa-location-arrow"></i> <span>导航</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i> <span>关于我</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image:url(https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/wugengji/6fe9e1029245d68881f11de2b3c27d1ed31b249e.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">在下小宋</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-compass"></i> <span>目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-address-book"></i> <span>链接</span></a></div><div class="menus_item"><a class="site-page" href="/navigate/"><i class="fa-fw fas fa-location-arrow"></i> <span>导航</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i> <span>关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">在下小宋</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/f06b97c0/" title="Bert详解"><img class="post_bg" src="https://img1.baidu.com/it/u=137246240,1680214152&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Bert详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/f06b97c0/" title="Bert详解">Bert详解</a><div class="article-meta-wrap"><span class="article-meta"><i class="fas fa-thumbtack sticky"></i><span class="sticky">置顶</span><span class="article-meta-separator">|</span></span><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-01-24T12:50:06.000Z" title="更新于 2022-01-24 20:50:06">2022-01-24</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Bert/">Bert</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/f06b97c0/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">Bert含义BERT模型的全称是：BidirectionalEncoder Representations from Transformer。双向Transformer编码表达，其中双向指的是attention矩阵中，每个字都包含前后所有字的信息。 BERT模型的目标是利用大规模无标注语料训练、获得文本的包含丰富语义信息的Representation，即：文本的语义表示，然后将文本的语义表示在特定NLP任务中作微调，最终应用于该NLP任务。 模型结构Bert依然是依赖Transformer模型结构，我们知道GPT采用的是Transformer中的Decoder部分的模型结构，当前位置只能attend到之前的位置。而Bert中则没有这样的限制，因此它是用的Transformer的Encoder部分。 而Transformer是由一个一个的block组成的，其主要参数如下： L: 多少个blockH: 隐含状态尺寸，不同block上的隐含状态尺寸一般相等，这个尺寸单指多头注意力层的尺寸，有一个惯例就是在Transformer Block中全连接层的尺寸是多头注意力层的4倍。所以指定了H相当 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/c2ac91e1/" title="Transformer详解"><img class="post_bg" src="https://img1.baidu.com/it/u=2878985419,3213940956&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Transformer详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/c2ac91e1/" title="Transformer详解">Transformer详解</a><div class="article-meta-wrap"><span class="article-meta"><i class="fas fa-thumbtack sticky"></i><span class="sticky">置顶</span><span class="article-meta-separator">|</span></span><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-01-24T12:40:46.000Z" title="更新于 2022-01-24 20:40:46">2022-01-24</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/c2ac91e1/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">左边处理源语言，称之为Encoder，右边处理目标语言，被称为Decoder，分别由N个Block组成。然后每个block都有这么几个模块： Multi-Head Attention Masked Multi-Head Attention Add &amp; Norm Feed Forward Positional Encoding Linear 其中， Feed Forward和Linear是神经网络的基本操作全连接层，Add &amp; Norm以及延伸出来的一条侧边也是一个常见的神经网络结构残差连接 Attentionattention说白了就是权重计算和加权求和。图上的循环神经网络中的每一步都会输出一个向量，在预测目标语言到某一步时，用当前步的向量去和源语言中的每一步的向量去做内积，然后经过softmax得到归一化后的权重，再用权重去把源语言上的每一步的向量去做加权平均。然后做预测的时候也作为输入进入全连接层 Multi-Head Attention Multi-Head Attention是由多个Scaled Dot-Product Attention的函数组合而成的。 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/d9e0e633/" title="2022第十四周实盘"><img class="post_bg" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-6.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2022第十四周实盘"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/d9e0e633/" title="2022第十四周实盘">2022第十四周实盘</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-04-18T10:06:21.000Z" title="更新于 2022-04-18 18:06:21">2022-04-18</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%AE%9E%E7%9B%98/">实盘</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/d9e0e633/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">如图所示，这周市场比上周跌的更狠，A股平均跌幅3.9个点。目前持仓洲明科技、拉卡拉、润欣科技、蔚蓝生物、爱朋医疗、ST绿庭、华宇软件、高鸿股份、测绘股份。卖出了芒果超媒、维科科技，按照上周的分析，需要科学合理地配置每只股票的仓位，所以这周买入了拉卡拉、润欣科技、蔚蓝生物、爱朋医疗、ST绿庭等多只股票，当然这不是为买而买，买入的股票全都符合我的标准，都是具有小周期的股票且处在周期的底部。洲明科技的仓位有点多，暂时还没法减少到安全的水平线，希望后面幸运一点市场不要这么疯狂的杀跌了。这周做了正确的事情，把股票的配置合理化分散化了，还投资了几只很棒的股票，但是没有收到好的效果，市场的大环境就是这样，一边倒的下跌，投资者的努力仿佛就是螳臂当车一样无力。即使是这样，投资者依然应该有自己的信念和坚持。投资真的不能以短期的成败论输赢，投资本质上是个无限游戏，一时的得失根本不算什么，投资者真正要看重和计较的是在长期的投资活动中，什么样的策略是赚钱的，而这正是投资最难的部分，你可能无法从当下的市场获得恰当的反馈，没有一种公平的激励机制，我们的思维和行动就很容易混乱。所以投资者需要有战略思维，要用长远的眼 ...</div></div></div><div class="recent-post-item ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-fb+5w+4e-db+86" data-ad-client="ca-pub-1902923403139213" data-ad-slot="1538867630"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/80020160/" title="决策树详解"><img class="post_bg" src="https://img0.baidu.com/it/u=4130948941,3969879630&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="决策树详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/80020160/" title="决策树详解">决策树详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-01-09T10:09:49.000Z" title="更新于 2022-01-09 18:09:49">2022-01-09</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/80020160/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">决策树是一种以树状结构表示的分类和回归模型，从根节点开始，根据最优属性从上往下层层划分，最终输出叶子节点为分类结果值。 决策树代表对象属性和对象值之间的一种映射关系。它由节点（node）和有向边（directed edge）组成，其节点有两种类型：内节点（internal node）和叶节点（leaf node），内部节点表示一个特征或属性，叶节点表示一个类。根节点是决策树最开始的结点，内部结点是可以继续分类的结点。 决策树的学习本质上是从训练集中归纳出一组分类规则，得到与数据集矛盾较小的决策树，同时具有很好的泛化能力。决策树学习的损失函数通常是正则化的极大似然函数，通常采用启发式方法，近似求解这一最优化问题。决策树学习算法包含特征选择、决策树生成与决策树的剪枝。决策树表示的是一个条件概率分布，所以深浅不同的决策树对应着不同复杂程度的概率模型。决策树的生成对应着模型的局部选择（局部最优），决策树的剪枝对应着全局选择（全局最优）。决策树常用的算法有ID3，C4.5，CART，下面通过一个简单的例子来分别介绍这几种算法。 特征选择宗旨是在每一个决策点，选择能够使得样本的熵降低得最快，样本纯 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/ab903812/" title="逻辑回归详解"><img class="post_bg" src="https://img0.baidu.com/it/u=1261970995,1847114486&amp;fm=26" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="逻辑回归详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/ab903812/" title="逻辑回归详解">逻辑回归详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-02-02T06:20:03.000Z" title="更新于 2022-02-02 14:20:03">2022-02-02</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/ab903812/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">什么是逻辑回归？逻辑回归（Logistic Regression）主要解决二分类问题，用来表示某件事情发生的可能性。 比如： 一封邮件是垃圾邮件的肯能性（是、不是） 你购买一件商品的可能性（买、不买） 广告被点击的可能性（点、不点） 一句话总结，逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。 逻辑回归公式 Sigmoid函数 图像是Sigmoid函数之所以叫Sigmoid，是因为函数的图像很想一个字母S。这个函数是一个很有意思的函数，从图像上我们可以观察到一些直观的特性：函数的取值在0-1之间，且在0.5处为中心对称，并且越靠近x&#x3D;0的取值斜率越大。 逻辑回归的假设数据服从伯努利分布 伯努利分布：是一个离散型概率分布，若成功，则随机变量取值1；若失败，随机变量取值为0。成功概率记为p，失败为q &#x3D; 1-p。 在逻辑回归中，既然假设了数据分布服从伯努利分布，那就存在一个成功和失败，对应二分类问题就是正类和负类，那么就应该有一个样本为正类的概率，和样本为负类的概率。 正类的概率由sigmoid的函数计算 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/6bc1f905/" title="TextCNN详解"><img class="post_bg" src="https://img1.baidu.com/it/u=2268867597,1321565778&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="TextCNN详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/6bc1f905/" title="TextCNN详解">TextCNN详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-02-07T07:28:45.000Z" title="更新于 2022-02-07 15:28:45">2022-02-07</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/TextCNN/">TextCNN</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/6bc1f905/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">什么是卷积？最好理解的方式就是，一个小框在矩阵上滑动，并通过一定的计算来得到一个新的矩阵。看图吧，这样更好理解！ 卷积神经网络的核心思想是捕捉局部特征，对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。 TextCNN原理Yoon Kim在论文(2014 EMNLP) Convolutional Neural Networks for Sentence Classification提出TextCNN。将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性。 每一个单词的embedding固定，所以kernel size的宽度不变，只能改变高度。kernel的通道可以理解为用不同的词向量表示。 输入句子的长度不一样，但卷积核的个数一样。每个卷积核抽取单词的个数不一样，高度低的形成feature maps长度就较长，高度高的形成feature maps的长度就较短，但fe ...</div></div></div><div class="recent-post-item ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-fb+5w+4e-db+86" data-ad-client="ca-pub-1902923403139213" data-ad-slot="1538867630"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/b16b957b/" title="LSTM详解"><img class="post_bg" src="https://img1.baidu.com/it/u=3043404661,634103034&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="LSTM详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/b16b957b/" title="LSTM详解">LSTM详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-02-07T07:18:51.000Z" title="更新于 2022-02-07 15:18:51">2022-02-07</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LSTM/">LSTM</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/b16b957b/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">长期依赖(Long Term Dependencies)传统的RNN节点输出仅由权值，偏置以及激活函数决定（图3）。RNN是一个链式结构，每个时间片使用的是相同的参数。 在深度学习领域中（尤其是RNN），“长期依赖“问题是普遍存在的。长期依赖产生的原因是当神经网络的节点经过许多阶段的计算后，之前比较长的时间片的特征已经被覆盖，例如下面例子 123456eg1: The cat, which already ate a bunch of food, was full. | | | | | | | | | | | t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10eg2: The cats, which already ate a bunch of food, were full. | | | | | | | | | | | t0 t1 t2 t3 t4 t5 t ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/3197c121/" title="TinyBert详解"><img class="post_bg" src="https://img2.baidu.com/it/u=507580163,2809643925&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="TinyBert详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/3197c121/" title="TinyBert详解">TinyBert详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-01-30T03:28:39.000Z" title="更新于 2022-01-30 11:28:39">2022-01-30</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/TinyBert/">TinyBert</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/3197c121/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">BERT 等大模型性能强大，但很难部署到算力、内存有限的设备中。为此，来自华中科技大学、华为诺亚方舟实验室的研究者提出了 TinyBert，这是为基于 transformer 的模型专门设计的知识蒸馏（knowledge distillation，KD）方法。通过这种新的 KD 方法，大型 teacherBERT 模型中编码的大量知识可以很好地迁移到小型 student TinyBert模型中。模型大小还不到 BERT 的 1&#x2F;7，但速度是 BERT 的 9 倍还要多，而且性能没有出现明显下降。 TinyBert 的结构如下图：在TinyBert中，student 和 teacher 网络都是通过 Transformer 层构建的。此外，研究者还提出了一种专门用于 TinyBERT 的两段式学习框架，从而分别在预训练和针对特定任务的学习阶段执行 transformer 蒸馏。这一框架确保 TinyBert 可以获取 teacherBERT 的通用知识和针对特定任务的知识。 除了提出新的 transformer 蒸馏法之外，研究者还提出了一种专门用于 TinyBERT 的两段式 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/cf4456cd/" title="Normalization方法总结"><img class="post_bg" src="https://img2.baidu.com/it/u=1097103630,2955584474&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Normalization方法总结"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/cf4456cd/" title="Normalization方法总结">Normalization方法总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-01-29T07:26:03.000Z" title="更新于 2022-01-29 15:26:03">2022-01-29</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Normalization/">Normalization</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/cf4456cd/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">归一化（Normalization）描述： 将数据映射到指定的范围，如：把数据映射到0～1或-1~1的范围之内处理。 作用： 数据映射到指定的范围内进行处理，更加便捷快速。 把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。经过归一化后，将有量纲的数据集变成纯量，还可以达到简化计算的作用。 常见做法：Min-Max归一化 标准化（Normalization）注：在英文翻译中，归一化和标准化的翻译是一致的，而在实际使用中，我们需要根据实际的公式（或用途）去理解~ 数据标准化方法有多种，如：直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。不同的标准化方法，对系统的评价结果会产生不同的影响。其中，最常用的是Z-Score 标准化。 Z-Score 标准化其中，$\mu$为数据均值（mean），$\sigma$为标准差（std）。 描述：将原数据转换为符合均值为0，标准差为1的标准正态分布的新数据。 作用： 提升模型的收敛速度（加快梯度下降的求解速度） 提升模型的精度（消除量级和量纲的影响） 简化计算（与归一化的简化原 ...</div></div></div><div class="recent-post-item ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-fb+5w+4e-db+86" data-ad-client="ca-pub-1902923403139213" data-ad-slot="1538867630"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/378cfac4/" title="Markdown数学公式语法"><img class="post_bg" src="https://img1.baidu.com/it/u=3677495509,4089642876&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Markdown数学公式语法"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/378cfac4/" title="Markdown数学公式语法">Markdown数学公式语法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time datetime="2022-01-25T02:09:15.000Z" title="更新于 2022-01-25 10:09:15">2022-01-25</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E6%95%99%E7%A8%8B/">教程</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Markdown/">Markdown</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/">数学公式</a></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/posts/378cfac4/#post-comment"></a> <span class="article-meta-label">条评论</span></span></div><div class="content">行内与独行 行内公式：将公式插入到本行内，符号：$公式内容$，如：$xyz$ 独行公式：将公式插入到新的一行内，并且居中，符号：$$公式内容$$，如： $$xyz$$ 上标、下标与组合 上标符号，符号：^，如：$x^4$ 下标符号，符号：_，如：$x_1$ 组合符号，符号：&#123;&#125;，如：${16}{8}O{2+}{2}$ 汉字、字体与格式 汉字形式，符号：\mbox&#123;&#125;，如：$V_{\mbox{初始}}$ 字体控制，符号：\displaystyle，如：$\displaystyle \frac{x+y}{y+z}$ 下划线符号，符号：\underline，如：$\underline{x+y}$ 标签，符号\tag&#123;数字&#125;，如：$\tag{11}$ 上大括号，符号：\overbrace&#123;算式&#125;，如：$\overbrace{a+b+c+d}^{2.0}$ 下大括号，符号：\underbrace&#123;算式&#125;，如：$a+\underbrace{b+c}_{1.0}+d$ 上位符号，符号：\sta ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">在下小宋</div><div class="author-info__description">从来没有真正的绝境，只有心灵的迷途</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">30</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/szj2ys"><i class="fab fa-github"></i><span>GitHub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的小站，<a href="https://www.songzj.com" style="color:#49b1f5">在下小宋</a>。</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/d9e0e633/" title="2022第十四周实盘"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-6.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2022第十四周实盘"></a><div class="content"><a class="title" href="/posts/d9e0e633/" title="2022第十四周实盘">2022第十四周实盘</a><time datetime="2022-04-18T10:06:21.000Z" title="发表于 2022-04-18 18:06:21">2022-04-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/80020160/" title="决策树详解"><img src="https://img0.baidu.com/it/u=4130948941,3969879630&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="决策树详解"></a><div class="content"><a class="title" href="/posts/80020160/" title="决策树详解">决策树详解</a><time datetime="2022-02-09T10:09:49.000Z" title="发表于 2022-02-09 18:09:49">2022-02-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/ab903812/" title="逻辑回归详解"><img src="https://img0.baidu.com/it/u=1261970995,1847114486&amp;fm=26" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="逻辑回归详解"></a><div class="content"><a class="title" href="/posts/ab903812/" title="逻辑回归详解">逻辑回归详解</a><time datetime="2022-02-09T06:20:03.000Z" title="发表于 2022-02-09 14:20:03">2022-02-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/6bc1f905/" title="TextCNN详解"><img src="https://img1.baidu.com/it/u=2268867597,1321565778&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="TextCNN详解"></a><div class="content"><a class="title" href="/posts/6bc1f905/" title="TextCNN详解">TextCNN详解</a><time datetime="2022-02-07T07:28:45.000Z" title="发表于 2022-02-07 15:28:45">2022-02-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/b16b957b/" title="LSTM详解"><img src="https://img1.baidu.com/it/u=3043404661,634103034&amp;fm=253" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="LSTM详解"></a><div class="content"><a class="title" href="/posts/b16b957b/" title="LSTM详解">LSTM详解</a><time datetime="2022-02-07T07:18:51.000Z" title="发表于 2022-02-07 15:18:51">2022-02-07</time></div></div></div></div><div class="card-widget ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1902923403139213" data-ad-slot="8108145410" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="card-widget card-categories"><div class="item-headline"><i class="fas fa-folder-open"></i> <span>分类</span></div><ul class="card-category-list" id="aside-cat-list"><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/GitHub/"><span class="card-category-list-name">GitHub</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/Markdown%E6%95%99%E7%A8%8B/"><span class="card-category-list-name">Markdown教程</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/Python%E6%95%99%E7%A8%8B/"><span class="card-category-list-name">Python教程</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/Shell/"><span class="card-category-list-name">Shell</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/linux/"><span class="card-category-list-name">linux</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/shell/"><span class="card-category-list-name">shell</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/%E6%95%99%E7%A8%8B/"><span class="card-category-list-name">教程</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item"><a class="card-category-list-link" href="/categories/%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%95/"><span class="card-category-list-name">相似度算法</span><span class="card-category-list-count">1</span></a></li></ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size:1.15em;color:#8eb9c3">算法</a><a href="/tags/Normalization/" style="font-size:1.15em;color:#9faa46">Normalization</a><a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" style="font-size:1.15em;color:#58777e">决策树</a><a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size:1.15em;color:#bec71c">逻辑回归</a><a href="/tags/crontab/" style="font-size:1.15em;color:#792699">crontab</a><a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size:1.45em;color:#96bb75">教程</a><a href="/tags/shell/" style="font-size:1.15em;color:#6162b7">shell</a><a href="/tags/linux/" style="font-size:1.15em;color:#513d5b">linux</a><a href="/tags/shell-%E5%91%BD%E4%BB%A4/" style="font-size:1.15em;color:#ad0463">shell 命令</a><a href="/tags/wget/" style="font-size:1.15em;color:#782180">wget</a><a href="/tags/Bert/" style="font-size:1.15em;color:#35190a">Bert</a><a href="/tags/Shell/" style="font-size:1.3em;color:#9a610c">Shell</a><a href="/tags/%E8%84%9A%E6%9C%AC/" style="font-size:1.15em;color:#5abf42">脚本</a><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size:1.15em;color:#606410">大数据</a><a href="/tags/Albert/" style="font-size:1.15em;color:#3759b4">Albert</a><a href="/tags/Roberta/" style="font-size:1.15em;color:#bf056b">Roberta</a><a href="/tags/LSTM/" style="font-size:1.15em;color:#93b55c">LSTM</a><a href="/tags/TextCNN/" style="font-size:1.15em;color:#bc2e5d">TextCNN</a><a href="/tags/TinyBert/" style="font-size:1.15em;color:#722b98">TinyBert</a><a href="/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/" style="font-size:1.15em;color:#5e3c16">问题解决</a><a href="/tags/Github/" style="font-size:1.3em;color:#616d3d">Github</a><a href="/tags/Transformer/" style="font-size:1.15em;color:#631b94">Transformer</a><a href="/tags/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/" style="font-size:1.15em;color:#56136a">文本相似度</a><a href="/tags/Markdown/" style="font-size:1.3em;color:#2f9071">Markdown</a><a href="/tags/%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/" style="font-size:1.15em;color:#173cc6">数学公式</a><a href="/tags/%E5%AE%9E%E7%9B%98/" style="font-size:1.15em;color:#013f75">实盘</a><a href="/tags/Python/" style="font-size:1.3em;color:#2f784c">Python</a><a href="/tags/pyinstaller/" style="font-size:1.15em;color:#65b120">pyinstaller</a><a href="/tags/pypi/" style="font-size:1.15em;color:#360e22">pypi</a><a href="/tags/%E9%A1%B9%E7%9B%AE%E5%8F%91%E5%B8%83/" style="font-size:1.15em;color:#a3670c">项目发布</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/04/"><span class="card-archive-list-date">2022年04月</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/02/"><span class="card-archive-list-date">2022年02月</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/01/"><span class="card-archive-list-date">2022年01月</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/09/"><span class="card-archive-list-date">2021年09月</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/08/"><span class="card-archive-list-date">2021年08月</span><span class="card-archive-list-count">8</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">23</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">37.9k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastpushdate="2022-04-21T07:54:33.785Z"></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/wugengji/6fe9e1029245d68881f11de2b3c27d1ed31b249e.jpg)"><div id="footer-wrap"><div class="footer_custom_text"><p><a style="margin-inline:5px" target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"></a><a style="margin-inline:5px" target="_blank" href="https://butterfly.js.org/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"></a><a style="margin-inline:5px" target="_blank" href="https://www.jsdelivr.com/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用 Jsdelivr 为静态资源提供CDN加速" alt="Jsdelivr"></a><a style="margin-inline:5px" target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"></a><a style="margin-inline:5px" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@4.1.0/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@4.1.0/source/js/main.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@3.8.3/source/js/tw_cn.min.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@4.1.0/source/js/search/local-search.min.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>(()=>{const t=()=>{const t=document.querySelectorAll("#recent-posts .article-title");let e=[];return t.forEach(t=>{e.push(t.getAttribute("href"))}),e},e=()=>{const e=()=>{twikoo.getCommentsCount({envId:"https://twikoo-mfge-f6n1go8p6-szj2ys.vercel.app",region:"",urls:t(),includeReply:!1}).then((function(t){document.querySelectorAll("#recent-posts .twikoo-count").forEach((e,o)=>{e.innerText=t[o].count})})).catch((function(t){console.log(t)}))};"object"==typeof twikoo?e():getScript("https://cdn.jsdelivr.net/gh/zhheo/twikoo@dev/dist/twikoo.all.min.js").then(e)};window.pjax?e():window.addEventListener("load",e)})()</script><script>function subtitleType(){window.typed=new Typed("#subtitle",{strings:["一点浩然气，千里快哉风","留得青山在，不怕没柴烧","反听之谓聪，内视之谓明，自胜之谓强","德川家康家训：一、人之一生，如负重远行，不可急于求成。二、以受约束为常事，则不会心生不满。三、常思贫困，方无贪欲之念。四、忍耐乃长久无事之基石。五、愤怒是敌。六、只知胜而不知败，必害其身。七、常思己过，莫论人非。八、不及尚能补，过之无以救。","顺，不妄喜；逆，不惶馁；安，不奢逸；危，不惊惧；胸有惊雷而面如平湖者，可拜上将军也","祸不妄至，福不徒来","事必成然后举，身必安而后行","快于意者亏与行，甘于心者伤与性","大怒不怒，大喜不喜，可以养心。靡俗不交，恶党不入，可以立身。小利不争，小忿不发，可以和众。见善必行，闻过必改，可以畜德。","白圭之玷，尚可磨也；斯言之玷，不可为也","力田不如逢年，善仕不如遇合","知其不可奈何而安之若命，德之至也","智足以拒谏，言足以饰非","千金之裘，非一狐之腋也；台榭之榱，非一木之枝也；三代之际，非一代之智也","天之道，损有余而补不足；人之道，则不然，损不足以奉有余"],startDelay:300,typeSpeed:150,loop:!0,backSpeed:50})}"function"==typeof Typed?subtitleType():getScript("https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js").then(subtitleType)</script><script>(()=>{const e=document.querySelectorAll("#article-container .mermaid-wrap");if(e.length){window.runMermaid=()=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";Array.from(e).forEach((e,n)=>{const d=e.firstElementChild,r="mermaid-"+n,i="%%{init:{ 'theme':'"+t+"'}}%%\n"+d.textContent;mermaid.mermaidAPI.render(r,i,e=>{d.insertAdjacentHTML("afterend",e)})})};const t=()=>{window.loadMermaid?runMermaid():getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(runMermaid)};window.pjax?t():document.addEventListener("DOMContentLoaded",t)}})()</script></div><script data-ad-client="ca-pub-1902923403139213" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script defer id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>