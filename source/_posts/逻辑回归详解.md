---
title: 逻辑回归详解
tags:
  - 逻辑回归
keywords: 逻辑回归
cover: 'https://img0.baidu.com/it/u=1261970995,1847114486&fm=26'
comments: true
abbrlink: ab903812
date: 2022-02-09 14:20:03
updated: 2022-02-02 14:20:03
categories:
description:
top_img:
---


## 什么是逻辑回归？


逻辑回归（Logistic Regression）主要解决二分类问题，用来表示某件事情发生的可能性。



比如：

- 一封邮件是垃圾邮件的肯能性（是、不是）
- 你购买一件商品的可能性（买、不买）
- 广告被点击的可能性（点、不点）

一句话总结，逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

## 逻辑回归公式

![](https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/logistic.jpg)

Sigmoid函数
![](https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sigmoid.jpg)

图像是
![](https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sigmoidpic.jpg)
Sigmoid函数之所以叫Sigmoid，是因为函数的图像很想一个字母S。这个函数是一个很有意思的函数，从图像上我们可以观察到一些直观的特性：函数的取值在0-1之间，且在0.5处为中心对称，并且越靠近x=0的取值斜率越大。

## 逻辑回归的假设
### 数据服从伯努利分布
>伯努利分布：是一个离散型概率分布，若成功，则随机变量取值1；若失败，随机变量取值为0。成功概率记为p，失败为q = 1-p。
>

在逻辑回归中，既然假设了数据分布服从伯努利分布，那就存在一个成功和失败，对应二分类问题就是正类和负类，那么就应该有一个样本为正类的概率，和样本为负类的概率。

### 正类的概率由sigmoid的函数计算


## 逻辑回归的损失函数
都说逻辑回归的损失函数是它的极大似然函数，但是为啥呢？

> 极大似然估计：利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值（模型已定，参数未知）
> 

一般和平方损失函数（最小二乘法）拿来比较，因为线性回归用的就是平方损失函数，原因就是平方损失函数加上sigmoid的函数将会是一个非凸的函数，不易求解，会得到局部解，用对数似然函数得到高阶连续可导凸函数，可以得到最优解。

其次，是因为对数损失函数更新起来很快，因为只和x，y有关，和sigmoid本身的梯度无关。

## 逻辑回归的求解方法
一般都是用梯度下降法来求解，梯度下降又有随机梯度下降，批梯度下降，small batch 梯度下降三种方式：
- 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。

- 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。

- 小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。

## 逻辑回归的如何分类
设定一个阈值，判断正类概率是否大于该阈值。比如阈值是0.5，只用判断正类概率是否大于0.5即可。

## 逻辑回归的优缺点

**优点：**

- 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
- 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
- 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
- 资源占用小,尤其是内存。因为只需要存储各个维度的特征值。
- 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cut off，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

**缺点：**

- 当特征空间很大时，逻辑回归的性能不是很好，容易欠拟合；因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
- 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
- 不能很好地处理大量多类特征或变量；
- 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须**线性可分**；
- 对于非线性特征，需要进行转换；
- 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

## 逻辑回归 VS 线性回归

线性回归和逻辑回归是 2 种经典的算法。经常被拿来做比较，下面整理了一些两者的区别：
- 逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个Sigmoid函数，使样本映射到[0,1]之间的数值，从而来处理分类问题。
- 线性回归只能用于回归问题，逻辑回归虽然名字叫回归，但是更多用于分类问题（关于回归和分类的区别可以看看这篇文章《[一文看懂监督学习（基本概念+4步流程+9个典型算法）](https://easyai.tech/ai-definition/supervised-learning/)》）
- 逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。
-  线性回归要求因变量是连续性数值变量，而逻辑回归要求因变量是离散的变量
-  逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。
-  线性回归要求自变量和因变量呈线性关系，而逻辑回归不要求自变量和因变量呈线性关系
-  线性回归可以直观的表达自变量和因变量之间的关系，逻辑回归则无法表达变量之间的关系
-  逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量量关系进行拟合。

> 自变量：主动操作的变量，可以看做「因变量」的原因
> 因变量：因为「自变量」的变化而变化，可以看做「自变量」的结果。也是我们想要预测的结果。


## 在什么情况下将连续的特征离散化之后可以获得更好的效果？
在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：

- 离散特征的增加和减少都很容易，易于模型的快速迭代，容易扩展；

- 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

- 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。具体来说，离散化后可以进行特征交叉，由M+N个变量变为M*N个变量；

- 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。

## 逻辑回归在训练的过程当中，如果有很多的特征高度相关，或者说有一个特征重复了100遍，会造成怎样的影响？

先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

为什么我们还是会在训练的过程当中将高度相关的特征去掉？

- 去掉高度相关的特征会让模型的可解释性更好

- 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。


## REFERENCES
- [关于逻辑回归，面试官都怎么问](https://blog.csdn.net/Kaiyuan_sjtu/article/details/121759976)
- [关于逻辑回归，面试官们都怎么问](https://blog.csdn.net/fengdu78/article/details/105384574)
- [逻辑回归 - 理论篇](https://blog.csdn.net/pakko/article/details/37878837)
- [从线性回归到逻辑斯特回归](https://blog.csdn.net/zx10212029/article/details/49889319)

