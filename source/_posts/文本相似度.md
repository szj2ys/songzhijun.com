---
title: 文本相似度
tags:
  - 标签
categories: 相似度算法
keywords: ''
cover: 'https://img0.baidu.com/it/u=2895464536,1705824186&fm=26&fmt=auto&gp=0.jpg'
comments: true
abbrlink: 409cacbb
date: 2021-09-01 09:32:59
updated: 2021-09-01 09:32:59
description:
top_img:
---


## Jaccard相似度
Jaccard相似度的定义很简单，两个句子词汇的交集size除以两个句子词汇的并集size。举个例子来说：

- 句子1： AI is our friend and it has been friendly.
- 句子2： AI and humans have always been friendly.

为了计算Jaccard相似度，我们首先使用英文nlp中常用的技术`lemmatization`，用词根替换那些具有相同词根的词汇。在上面的例子中，friend和friendly具有相同的词根，have和has具有相同的词根。我们可以画出两个句子词汇的交集与并集情况，如图所示：

![](https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/example.png)

对于上面两个句子，其Jaccard相似度为$$5/(5+3+2)=0.5$$，即两个句子词汇的交集5个词汇，并集10个词汇

```python
def jaccard_sim(str1, str2): 
    a = set(str1.split()) 
    b = set(str2.split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
```
值得注意的是，句子1中包含了两个friend，但这并不影响我们计算相似度，但这会影响cosine相似度。先让我们回忆一下cosine相似度的定义，公式如下
![](https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/cosine.png)

## Cosine相似度
cosine相似度是通过计算两个向量之间的夹角，来评价两个向量的相似度。

既然cosine相似度是使用向量计算的，我们就要先将句子文本转换为相应的向量。将句子转换为向量的方式有很多，最简单的一种就是使用bag of words计算的TF(term frequency)和TF-IDF（term frenquency-inverse document frequency）。哪一钟转换方法更好呢？实际上，两个方法各有各的应用场景。当我们要大概估计文本相似度时，使用TF就可以了。当我们使用文本相似度进行检索的类似场景时（如搜索引擎中的query relevence的计算），此时TF-IDF更好一些。

当然，我们也可以使用word2vec或者使用自定义的词向量来讲句子转换成向量。这里简单介绍一下tf-idf和word embedding的异同： 
- 1. tf/tf-idf为每一个词汇计算得到一个数字，而word embedding将词汇表示成向量 
- 2. tf/tf-idf在文本分类的任务中表现更好一些，而word embedding的方法更适用于来判断上下文的语义信息（这可能是由word embedding的计算方法决定的）

对于如何计算cosine similarity，我们还是试用上面的例子，计算cosine similarity的过程，分为以下几个步骤：

1. 使用bag of words的方式计算term frequency，下图展示了word frequency的统计结果。
![Term Frequency after lemmatization of the two sentences](https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/cosine1.png)

2. term frequency的问题在于，较长的句子里的词汇term frequency会更高一些。为了解决这个问题，我们可以使用归一化的方法（Normlization，如L2-norm）来去掉句子长度的影响。操作如下：首先对各个词汇的frequency平方求和，然后再开方。如果使用L2-norm，那么句子1的值为3.3166，而句子2的值为2.6458。用每一个词的term frquency除以这些norm的值，就可以得到如下结果：
![Normalization of term frequencies using L2 Norms](https://cdn.jsdelivr.net/gh/szj2ys/cdn/resources/sims/cosine2.png)

1. 上一步中，我们将句子向量的模归一化为1，就可以受用点乘的方法计算得到cosine相似度： $$Cosine Similarity = (0.3020.378) + (0.6030.378) + (0.3020.378) + (0.3020.378) + (0.302*0.378) = 0.684$$

所以两个句子的cosine相似度为0.684，而Jaccard相似度的结果是0.5。计算cosine相似度的python代码如下

```python
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
def get_cosine_sim(*strs): 
    vectors = [t for t in get_vectors(*strs)]
    return cosine_similarity(vectors)
    
def get_vectors(*strs):
    text = [t for t in strs]
    vectorizer = CountVectorizer(text)
    vectorizer.fit(text)
    return vectorizer.transform(text).toarray()
```

总结一下，Jaccard和cosine相似度的区别是什么呢？应该有以下几点：

- Jaccard使用的是集合操作，句子的向量长度由两个句子中unique的词汇数目决定，而cosine相似度使用的向量大小由词向量的维度决定。

- 上面的结论意味着什么呢？假设friend这个词汇在句子1中重复了非常多次，cosine相似度会发生变化，而Jaccard相似度的值不会变。让我们做个简单的计算，若句子1中的friend一词重复50次，cosine相似度会降为0.4，而Jaccard相似度保持0.5不变。

- 基于上述的结论，Jaccard相似度适用于什么场景呢？假设某个业务场景的文本包含了很多重复性的词汇，而这些重复是否与我们想做的任务关系不大，那么在分析文本相似度时，使用Jaccard计算相似度即可，因为对于Jaccard相似度来说，重复不会产生影响；假设这种重复对我们想做的任务影响很大，那么就要使用cosine相似度。



## REFERENCES

- [Overview of Text Similarity Metrics in Python](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50)